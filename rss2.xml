<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>기술블로그</title>
    <link>https://tk-one.github.io/</link>
    
    <atom:link href="https://tk-one.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sat, 17 Jun 2023 07:48:02 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Hadoop 파일기반 자료구조(SequenceFile, MapFile)</title>
      <link>https://tk-one.github.io/2022/06/19/hadoop-file-based-data-structure/</link>
      <guid>https://tk-one.github.io/2022/06/19/hadoop-file-based-data-structure/</guid>
      <pubDate>Sun, 19 Jun 2022 13:29:02 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;이번 글에서는 Hadoop에서 지원하는 파일저장시 지원하는 자료구조 2가지를 알아본다.&lt;br&gt;파일에 데이터를 저장시 그냥 blob 전체를 한개의 파일에 그대로 몽땅 저장하는 방식을 생각해보자. 그러면 파일의 내용은 block 크기로 잘라져 논리적</description>
        
      
      
      
      <content:encoded><![CDATA[<p>이번 글에서는 Hadoop에서 지원하는 파일저장시 지원하는 자료구조 2가지를 알아본다.<br>파일에 데이터를 저장시 그냥 blob 전체를 한개의 파일에 그대로 몽땅 저장하는 방식을 생각해보자. 그러면 파일의 내용은 block 크기로 잘라져 논리적으로 연속된 block으로 보일 것이다. 다만 이런 방식이면 경우에 따라 확장성에 좋지 않을 수 있다.<br>따라서 다양한 상황을 위해 Hadoop은 여러 파일 기반의 자료구조를 지원한다. 즉 스토리지에 저장시에 고려하는 자료구조이다.<br>여기서 볼 자료구조는 <strong>SequenceFile</strong>과 <strong>MapFile</strong>이다.  </p><br/><h2 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h2><p>SequenceFile은 로그파일과 잘맞는다. 로그파일은 record 한개당 하나의 행이다. 이런 로그파일을 그냥 binary로 저장하는것은 이후에 이 로그파일을 기반으로 무언가 작업을 할 예정이라면 확장성이 없는 방식이다.<br>Hadoop에서 제공하는 SequenceFile은 이런 형식의 데이터를 저장하기에 적절하다. SequenceFile은 binary key-value 쌍에 대한 storage-level의 자료구조를 제공한다.<br>예를들어 로그의 포맷이라면 key를 Hadoop의 <code>LongWritable</code>로 표현되는 timestamp로, value를 로그의 내용으로 저장할 수 있겠다.  </p><h4 id="SequenceFile-format"><a href="#SequenceFile-format" class="headerlink" title="SequenceFile format"></a>SequenceFile format</h4><p>SequenceFile의 포맷을 조금 더 자세히 보자. SequenceFile의 대략적인 내부구조는 다음과 같다.  </p><p align="center">    <img alt="SequenceFile format" style="max-width: 700px;" src="/images/hadoop/sequencefile.png"/></p><p>맨앞에는 header가 오고 그 뒤에는 하나이상의 record들이 위치한다. header에는 key, value 클래스의 이름, 압축이 되어있다면 압축관련정보 그리고 sync marker등이 존재한다.<br>SequenceFile에서는 <strong>sync point</strong>라는 것이 존재한다. SequenceFile에 write 할때 몇개의 record 단위마다 sync point를 marking 해놓는다. 이 sync point는 반드시 record 경계에 맞추어진다.<br>이 sync point는 reader가 record 경계를 잃어버렸을때 record 경계를 다시 동기화 하는데에 사용할 수 있다. 예를들어 파일의 위치가 record 경계에 있지 않으면 reader의 읽기 메서드가 예외를 반환한다. 이런 경우에 <code>SequenceFile.Reader</code>의 <code>sync(long position)</code> 메서드를 호출하면 position 이후의 바로 다음 sync point로 read point를 이동시켜준다. 그러므로 그 다음부터는 읽기를 다시 정상적으로 수행할 수 있다.<br>위 그림에서도 몇개의 record 단위마다 sync point가 저장되어있는 것을 확인할 수 있다.<br>각 SequenceFile은 랜덤하게 생성된 sync marker가 있고 이 값이 header에 존재한다. 그리고 이 sync marker가 위에서 본 sync point 마다 값이 저장되어 있는것이다.  </p><p>이제 record는 내부구조가 어떤지 살펴보자.<br>만약 record가 압축되어있지 않다면 각 record 들은 위의 그림과 같이 byte 단위의 record length, key length, key, value로 구성된다.<br>record가 압축이 되어있는 경우는 압축하지 않은 경우와 거의 동일하지만 value가 header에 명시된 codec으로 압축된 binary 인 점만 다르다.  </p><p>여러개의 record을 모아 block을 구성하고 이 block을 압축하는 방식도 있다. 여러개의 record로 구성된 block 자체를 압축하므로 record 단위 압축보다 압축도가 높고 가까이 있는 record 간에 유사성이 높으므로 효율도 더 좋을 수 있어 record 단위의 압축보다는 block 단위 압축방식이 더 선호된다.<br>record들은 <code>io.seqfile.compress.blocksize</code> 설정값에 정의된 크기에 이를때까지 하나의 block에 계속 추가된다. 이 설정의 기본값은 1MB이다.<br>Block compression 방식의 내부구조는 다음과 같다.  </p><p align="center">    <img alt="SequenceFile Block compression" style="max-width: 700px;" src="/images/hadoop/sequencefile-block-compression.png"/></p><p>이 SequenceFile은 대용량 dataset에 적합하다. 효율적으로 대용량 dataset을 처리하는데 효율적으로 설계되었으며, 각 SequenceFile 의 부분들을 병렬로 처리하도록 설계할 수도 있다. 또 파일의 순차접근에 강하며 파일을 저장할때 그냥 binary format으로 저장하기 때문에 매우 심플하고 다른언어에서도 이를 쉽게 다룰 수 있다.  </p><p>다만, <code>Parquet</code> 이나 <code>ORC</code> 같은 포맷에 비해서는 공간효율적이지는 못하다. 그리고 random access 를 위해 설게된 것은 아니기에, random access 가 자주 사용되는 애플리케이션에서는 SequenceFile이 비효율적 일 수 있다. 그리고 schema 변화를 제한적으로만 지원하기 때문에 데이터 구조를 변경하는 것이 힘들다.  </p><br/><h2 id="MapFile"><a href="#MapFile" class="headerlink" title="MapFile"></a>MapFile</h2><p>MapFile은 key를 기준으로 정렬이 되어있는 SequenceFile이다. 앞에서 본 SequenceFile은 record들이 정렬되어있을 필요는 없었다. 다만 MapFile은 반드시 record 들이 key 기준으로 정렬되어있어야 한다. 그리고 MapFile은 index를 통해 key로 record를 빠르게 검색할 수 있다.<br>MapFile을 생성하게 되면 directory가 생성되고 이 내부에 data 파일과 index 파일이 각각 존재한다. 여기서의 data 파일과 index 파일 모두 SequenceFile이다. data 파일은 key로 <strong>정렬된 record</strong>들로 구성되어 있으며 index 파일도 내부에 key들의 fragment들을 포함하고 있는 SequenceFile이다.<br>index는 기본적으로 data record의 128번째마다 key를 저장해둔다.  </p><p>MapFile에서 key 검색을 위해서는 index를 메모리에 올려 index를 기준으로 data record의 위치를 찾는다.<br>MapFile은 SequenceFile과 조금 다르게 파일에 쓸때에는 반드시 key로 정렬된 순서로 write해야한다. 그렇지 않으면 예외가 발생한다.  </p><br/><h2 id="Other-File-format"><a href="#Other-File-format" class="headerlink" title="Other File format"></a>Other File format</h2><p>Hadoop은 SequenceFile과 MapFile 말고도 다른 새로운 파일포맷도 많이 제공한다. SequenceFile, MapFile은 모두 row 기반의 파일포맷이지만 이 방식 말고도 column 기반의 파일포맷도 존재한다.<br>column 기반의 파일포맷에서는 각 row를 컬럼기준으로 나누어 저장한다. 예를들어 다음 그림처럼 첫번째 column이 먼저 저장되고, 그 다음 column이 저장되는 방식이다.  </p><p align="center">    <img alt="Column based file format" style="max-width: 700px;" src="/images/hadoop/column-based-file-format.png"/></p><p>만약 이 data들이 table의 데이터고 table에서 특정 column만 쿼리하는 과정을 생각해보자. 기존의 row 기반의 파일포맷은 관련있는 row를 모두 읽어 메모리로 읽어들인 후에 이들을 역직렬화하여 column을 뽑아내야한다.<br>하지만 column based 파일포맷은 직접 필요한 소수의 column만 읽어들일 수 있는 장점이 있다.  </p><br/><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.amazon.com/Hadoop-Perfect-Guide-Korean-White/dp/8968484597">https://www.amazon.com/Hadoop-Perfect-Guide-Korean-White/dp/8968484597</a></li></ul><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/Hadoop/">Hadoop</category>
      
      
      <category domain="https://tk-one.github.io/tags/hadoop/">hadoop</category>
      
      
      <comments>https://tk-one.github.io/2022/06/19/hadoop-file-based-data-structure/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>HBase 기초</title>
      <link>https://tk-one.github.io/2022/06/18/hbase/</link>
      <guid>https://tk-one.github.io/2022/06/18/hbase/</guid>
      <pubDate>Sat, 18 Jun 2022 13:29:02 GMT</pubDate>
      
        
        
      <description>&lt;br/&gt;

&lt;h2 id=&quot;HBase-Data-Model&quot;&gt;&lt;a href=&quot;#HBase-Data-Model&quot; class=&quot;headerlink&quot; title=&quot;HBase Data Model&quot;&gt;&lt;/a&gt;HBase Data Model&lt;/h2&gt;&lt;p&gt;HBase의 </description>
        
      
      
      
      <content:encoded><![CDATA[<br/><h2 id="HBase-Data-Model"><a href="#HBase-Data-Model" class="headerlink" title="HBase Data Model"></a>HBase Data Model</h2><p>HBase의 데이터 모델부터 보도록 하자.  </p><h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>HBase는 데이터들을 table안에 구성한다. table 이름은 String으로 파일시스템 path로 사용하는데 문제없도록 구성한다.  </p><h4 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h4><p>Table 안에서 데이터는 table의 row에 따라 저장된다. Row는 rowkey에 의해 유일하게 식별되고 rowkey는 다른 data type을 가지지 않고 <code>byte[]</code>로 구성된다. 한개의 Row는 한개 혹은 여러개의 record로 구성된다.  </p><h4 id="Column-Family"><a href="#Column-Family" class="headerlink" title="Column Family"></a>Column Family</h4><p>HBase에서의 column들은 <strong>Column Family</strong> 라고 부르는 일종의 그룹에 속한다.<br>HBase의 테이블은 최소한 한 개 이상의 column family를 가져야 한다.<br>Row의 record들은 column family로 그룹화된다. Column Family는 중요한데 HBase에서 데이터 저장에 있어 물리적인 특성과 관련이 깊다. 각 column family 마다 storage 관련 속성(caching 여부, 압축여부 등)을 지정할 수 있다.  </p><h4 id="Column-Qualifier"><a href="#Column-Qualifier" class="headerlink" title="Column Qualifier"></a>Column Qualifier</h4><p>column family 안의 데이터들은 column qualifier를 통해 표현된다. 만약 column family가 <code>info</code> 라면 column qualifier는 <code>info:name</code> 나 <code>info:email</code> 등이 될 수 있겠다. column qualifier는 미리 정의되어있을 필요가 없으며 각 row들마다 일관성없이 다른 column qualifier들을 가질 수 있다.<br>Column Qualifier은 그냥 column 이라고도 불리고 qual 이라고도 불린다.  </p><h4 id="Cell"><a href="#Cell" class="headerlink" title="Cell"></a>Cell</h4><p>Cell은 rowkey, column family, column qualifier의 조합이다. 이 조합이 cell을 식별한다. 이 cell에는 value로 데이터가 저장되어있고 version을 의미하는 timestamp도 포함한다.  </p><h4 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h4><p>Cell 내부의 value들은 version 별로 저장이 된다. version은 long 타입의 timestamp이며 이 값을 따로 지정하지 않으면 현재 timestamp 값으로 설정된다. HBase에서 각 column family 마다 cell 마다 유지되는 version 개수를 설정할 수 있으며 기본값은 <code>3</code>이다.    </p><br/>위의 6가지 개념에 대해 이해한다면 앞으로 HBase를 이해하는데 크게 도움이 된다. 다음 그림을 보자.  <p align="center">    <img alt="HBase Data Model Example" src="/images/hadoop/hbase-schema.png"/></p><p>각 row는 한개 이상의 cell들로 구성되어있고 각 row는 rowkey를 기준으로 정렬되어있다.<br>각 cell에는 여러개의 version이 저장되어 있는 것도 확인할 수 있다.  </p><br/><h4 id="Cell-Coordinate"><a href="#Cell-Coordinate" class="headerlink" title="Cell Coordinate"></a>Cell Coordinate</h4><p>HBase에서 cell 값은 <strong>coordinate</strong>에 의해 접근된다. 말 그대로 좌표라는 의미이다. coordinate는 rowkey, column family, column qualifier 순서의 조합이다.<br>논리적인 그림으로 생각했을때에 결국 HBase는 coordinate를 key로 각 coordinate의 데이터를 value로 가진 key-value 저장소로 생각할 수 있다.<br>HBase에서 데이터를 얻기위해 <code>Get</code> 요청을 할때 coordinate 정보 전체를 제공하지 않아도 된다. 만약 rowkey, column family, column qualifier로 요청한다면 version 별 map을 결과로 얻을 수 있다.  </p><h4 id="Cell-Key"><a href="#Cell-Key" class="headerlink" title="Cell Key"></a>Cell Key</h4><p>HBase에서 cell key는 rowkey, column family, column qualifier, version 의 조합이다. 밑에서 보게되겠지만 HBase는 HFile 이라는 형식으로 데이터를 저장하는데 HBase의 cell key는 이 HFile의 key 이고 이 cell key로 정렬이 되어있다.  </p><br/><p>HBase는 엄격한 데이터 규칙이 없는 semi-structured 데이터들을 위해 설계되었다. 이런 semi-structured 논리 모델로 데이터들은 각 데이터 컴포넌트들 간의 느슨한 연결을 가지도록 하고 이런 구조로 물리적으로 scale을 쉽게 해준다.<br>애초에 HBase는 scale을 염두에 두고 설계되었고 이런 결정이 물리모델에 영향을 끼치고 있다. 다만 이런 물리적 모델 특성으로 RDBMS에서 제공하는 multirow transaction을 지원하지 못한다. 밑에서 HBase의 논리적 모델과 물리적 모델을 살펴보자.  </p><h4 id="Logical-Model"><a href="#Logical-Model" class="headerlink" title="Logical Model"></a>Logical Model</h4><p>HBase가 논리적으로는 어떤 모델을 가지고 있는지 이해하면 HBase를 쉽게 이해할 수 있다.<br>HBase는 맵들의 정렬된 맵(sorted map of maps)이라고 바라볼 수 있다. 먼저 다음 그림을 보며 이해해보자.  </p><p align="center">    <img alt="HBase map 들의 sorted map" src="/images/hadoop/hbase-sorted-map.png"/></p><p>이처럼 논리적으로 데이터를 map 구조로 표현할 수 있다. 이는 “TheRealMT” 라는 rowkey의 데이터를 가지고 온 내용이다.<br>Map을 자세히 보면 map의 가장 안쪽에서는 cell의 version이 key이고 저장된 데이터가 value이다. 그 한단계 위에서는 column qualifier가 key이고 cell이 value이다. 결국 이를 자바로 표현하면 이와 같을 것이다.<br><code>Map&lt;RowKey, Map&lt;ColumnFamily, Map&lt;ColumnQualifier, Map&lt;Version, Data&gt;&gt;&gt;&gt;</code></p><p>또하나 주목할 점은 sorted map이라는 것은 key로 정렬되어있다는 것이다. 위 예제에 password는 2가지 version이 존재하는데 항상 새로운 version이 더 앞에오도록 정렬되어있다. HBase는 내림차순으로 version timestamp을 정렬한다. 그러므로 최근 version에 대한 빠른 접근을 가능하게 한다. version이 아닌 다른 key들은 모두 오름차순으로 정렬되어있는 것을 확인할 수 있다. 이런 특징은 schema 설계시 매우 중요하다.  </p><br/><h4 id="Physical-Model"><a href="#Physical-Model" class="headerlink" title="Physical Model"></a>Physical Model</h4><p>HBase는 HFile에 key-value 형식으로 저장이 된다. 위에서 보았던 “TheRealMT” 라는 rowkey를 가진 데이터는 다음과 같이 HFile에 저장된다.  </p><p align="center">    <img src="/images/hadoop/hbase-physical-hfile.png"/></p><p>이처럼 row 1개는 HFile 안에서 여러개의 record로 이루어져 있다. 또한 사용되지 않거나 null인 record가 없다. HBase는 데이터가 없을시에는 아예 아무것도 저장하지 않는다.<br>또 한가지 주목할 점은 HFile은 column family 별로 따로 생성된다. 하지만 같은 column family를 가진 single row도 동일한 HFile에 같이 존재하지 않을 수 있다. 이미 존재하는 rowkey에 새로운 column qualifier로 데이터를 넣는과정을 예시로 들 수 있겠다. 그러므로 single row 전체를 다 읽으려면 모든 HFile을 다 확인해야한다.<br>각 column family 별로 별도의 HFile을 사용하므로 HBase는 read 수행시 요청된 column family에 해당하는 HFile들만 읽으면 된다. 이런 물리적인 특성들은 storage를 더 효율적으로 사용하고 빠른 읽기를 가능하게 한다.  </p><p align="center">    <img src="/images/hadoop/hbase-hfile-per-cf.png"/></p><p>위 그림처럼 새로운 column family인 “activity”를 추가했다고 해보자. 이는 더 많은 HFile을 만들어내고 기존의 “info” column family와는 격리되어있고 전혀 다른 HFile을 만들어 내고있다. activity column family 내의 데이터가 커져도 info column family 성능에는 영향을 주지 않는다.  </p><br/><h2 id="Data-in-HBase"><a href="#Data-in-HBase" class="headerlink" title="Data in HBase"></a>Data in HBase</h2><p>HBase 테이블의 모든 row는 <strong>rowkey</strong>라는 유일한 식별자를 가지고 있다. 이는 테이블 내에서 유일한 값이다.<br>HBase에 저장된 모든 데이터들은 byte array 형태의 raw data로 저장된다. 자바 클라이언트 라이브러리에서는 이를 위해 <code>Bytes</code> 클래스를 제공해 다양한 형태의 데이터를 byte array로 바꿀 수 있다.  </p><p>위에서 보았듯이 cell은 [rowkey, column family, column qualifier] 좌표로 결정된다. 밑의 예제를 한번 보자.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Put</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Put</span>(Bytes.toBytes(<span class="hljs-string">&quot;TK-one&quot;</span>);<br>p.add(Bytes.toBytes(<span class="hljs-string">&quot;info&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;name&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;Jin Hyuk&quot;</span>));<br>p.add(Bytes.toBytes(<span class="hljs-string">&quot;info&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;email&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;email@email.com&quot;</span>));<br>p.add(Bytes.toBytes(<span class="hljs-string">&quot;info&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;password&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;pwd&quot;</span>));<br></code></pre></td></tr></table></figure><p>여기서 <code>Put</code> 객체를 만들었는데 이는 새로운 data를 저장할때나 기존에 존재하는 row를 수정할때 사용한다.<br>여기서는 info라는 column family에 속한 name, email, password라는 column에 값을 설정했다. 이름을 저장하고 있는 cell의 coordinates는 [TK-one, info, name] 이다. </p><br/><h2 id="HBase-write"><a href="#HBase-write" class="headerlink" title="HBase write"></a>HBase write</h2><p>새로운 row를 만들때나 기존 row를 수정할때나 내부 프로세스는 동일하다.<br>HBase는 command를 받으면 변경사항을 저장하고 만약 저장에 실패했으면 예외를 발생시킨다. 변경사항을 저장할때 기본적으로 두곳에 변경사항을 저장한다.<br>첫번째는 <strong>WAL</strong>(<strong>Write Ahead Log</strong>)에 저장한다. 이는 <strong>HLog</strong>라고도 불린다. 두번째는 <strong>MemStore</strong>에 저장한다.<br>HBase는 기본적으로 data의 내구성을 위해 두곳에 모두 저장한다. 두곳에 모두 저장해야 write가 완료된다.  </p><p align="center">    <img alt="write in HBase" style="max-width: 500px;" src="/images/hadoop/hbase-write.png"/></p><p>Memstore는 HBase에서 disk에 write하기 전에 HBase 메모리에 데이터를 모아놓은 buffer이다. 나중에 Memstore가 가득차게되면 <strong>HFile</strong>이라는 형태로 disk에 flush된다. 이미 존재하는 HFile에 append하는게 아니라 매 flush 마다 새로운 파일을 만든다. 여기서의 HFile은 HBase에서 사용하는 storage용 format이라고 생각하면 좋다.<br>HFile은 1개의 column family에 속해있다. 즉 하나의 HFile은 여러개의 column family로 이루어진 데이터를 가질 수 없다. Column family 당 1 개의 Memstore를 가지고 각 Memstore들은 가득차면 HFile로 flush 된다. 이 HFile도 HDFS에 저장된다.  </p><p align="center">    <img alt="Memstore per column family" style="max-width: 300px;" src="/images/hadoop/hbase-memstore.png"/></p><p>HBase에도 장애가 발생할 수 있다. 만약 서버가 다운되어 in-memory data를 모두 잃었을때는 아직 flush되지 않은 Memstore의 내용은 모두 유실될 것이다. HBase는 write 시에 WAL(Write Ahead Log)에 data를 write 하기때문에 이 WAL를 다시 replay 함으로서 MemStore 내용을 복구할 수 있다. HBase는 모든 변경사항을 WAL에 쓴다. 그리고 HBase는 이 WAL을 HDFS에 쓴다.<br>HBase의 Region 서버가 다운되어도 WAL을 HDFS에 썼다면 replica 3개중 아무곳에서 데이터를 제공받아 recover 할수있다. 만약 WAL이 Region 서버의 local disk에만 제공된다면 data loss가 발생할 수 있으므로 HDFS에 write 한다.<br>이 말고도 HDFS는 HBase Region 서버들에게 단일 namespace 파일시스템으로서 역할을 하기때문에 모든 Region 서버들은 다른 Region 서버들이 쓴 데이터들을 모두 볼 수 있다. 그러므로 Region 서버 장애시 다른 Region 서버에서 손쉽게 해당 WAL을 읽어 복구가 가능하다. 그러므로 Region 서버의 설계자체를 조금 더 간단하게 할 수 있는 장점이 있다.  </p><p>WAL에 recording이 성공해야 write operation 이 성공했다고 간주한다.<br>WAL은 HBase 서버당 한개씩 존재하고 그 서버의 모든 table들이 이를 공유한다.  </p><br/><h2 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h2><p>HBase는 random access를 지원한다. HDFS 위에서 어떻게 이를 가능하게 할까? 이를 위해서는 <strong>HFile</strong>를 이해해야한다.<br>먼저 HFile을 알아보기 전에 Hadoop의 파일기반 자료구조인 SequenceFile과 MapFile에 대한 개념이 있어야한다. 해당 내용은 <a href="/2022/06/19/hadoop-file-based-data-structure/">Hadoop 파일기반 자료구조(SequenceFile, MapFile)</a>에 정리해놓았다.  </p><p>HBase 버전 0.20 이전까지는 데이터를 저장하는데 Hadoop의 <strong>MapFile</strong>을 사용했다. MapFile은 SequenceFile의 확장판으로 data 파일과 index 파일을 포함하고 있는 디렉토리이다. MapFile의 데이터는 key를 기준으로 정렬된 key-value 데이터이고 매 구간의 key를 index에  offset과 함께 저장해놓음으로서 index scan만 함으로서 fast lookup을 가능하게 한다.  </p><h6 id="HBase-with-MapFile"><a href="#HBase-with-MapFile" class="headerlink" title="HBase with MapFile"></a>HBase with MapFile</h6><p>초기버전의 HBase는 MapFile을 사용했는데 key로는 rowkey, column family, column qualifier, timestamp, type으로 구성했다. value로는 row 내용이 들어갈 것이다.  </p><p align="center">    <img alt="HBase with MapFile" style="max-width: 600px;" src="/images/hadoop/hbase-with-mapfile.png"/></p>  <p>여기서의 type은 해당 row가 삭제되었는지를 나타내는 flag인데 이는 밑에서 자세히 알아볼 것이다.<br>위와같이 MapFile을 구성하면 만약 row를 수정하면 어떻게 다음 조회에 수정된 내용을 반환할 수 있을까? 같은 row중 더 큰 timestamp를 가진 row를 반환할 수 있겠다.<br>MapFile에서의 data 파일은 반드시 key로 정렬이 되어있어야 한다. 하지만 HBase에 write하는 데이터는 정렬된 순서로 도착하지 않는다. 이를 해결하기 위해 위에서 보았듯이 HBase는 write command시 MemStore에 데이터를 저장하고 있다가 가득차면 flush 한다. MemStore는 <code>ConcurrentSkipListMap</code>과 동일하므로 이미 key로 정렬되어있다. 이를 MapFile로 flush하고 해당 MapFile은 더이상 수정하지 않는다. 그러므로 데이터를 찾을때에는 모든 MapFile을 대상으로 검색해야한다. 이는 성능에 좋지않으므로 성능을 개선하는 방법을 밑에서 자세히 알아볼 것이다.  </p><h6 id="HFile-version-1"><a href="#HFile-version-1" class="headerlink" title="HFile version 1"></a>HFile version 1</h6><p>HBase 0.20 버전부터 HBase는 MapFile을 사용하지 않고 직접 구현한 MapFile과 비슷한 <strong>HFile</strong> 이라는 파일기반 자료구조를 사용한다.<br>HFile은 MapFile과 유사하지만 index를 다른 파일로 분리하지않고 같은파일에서 관리하도록 하며 여러 metadata를 담을 수 있다.<br>HFile 내부에는 data block이 여러개가 존재한다. 여러개의 연속된 data block들이 존재하고 index도 같이 존재한다. 이 data block에는 실제 key-value 데이터를 담고있다. 각 data block의 첫번째 key가 index에 기록된다. data block은 기본설정으로 64KB의 크기를 가진다.<br>각 HFile의 data block에는 HBase cell 들이 KeyValue 형태로 연속해서 저장되어 있다.<br>아래는 HFile version 1의 구조이다.  </p><p align="center">    <img alt="HFile version 1" style="max-width: 700px;" src="/images/hadoop/hfile-v1.png"/></p><p>Block index에는 각 entry마다 block의 size, key 정보 등이 들어있다. HFile에는 위와같이 metadata를 담는 block인 Meta Block과 File Info 가 존재한다. version 1에서는 이 Meta Block을 Bloom Filter 정보를 담는데에 활용하였다. data scan시 해당 data가 이 HFile에 있는지 bloom filter를 활용하면 빠르게 판단할 수 있는데 bloom filter는 false positive가 발생한다. 그래서 HFile이 너무 오래되었는지 확인하기위한 Max SequenceId, Timerage 등을 File Info에 저장해 false positive를 한번 더 필터링한다.  </p><h6 id="HFile-version-2"><a href="#HFile-version-2" class="headerlink" title="HFile version 2"></a>HFile version 2</h6><p>HBase 0.92 버전에서는 많은 데이터가 저장될 때 성능개선을 위해 HFile 형식이 조금 변경되었다.<br>위의 HFile version 1에서는 데이터를 읽기 위해서는 해당 HFile의 전체 데이터 정보를 담고있는 단일 index와 bloom filter를 메모리에 모두 올려놓아야 했다. 이를 개선하기 위해 HFile version 2 에서는 bloom filter를 block 별로 두고, multi-level index를 사용하도록 개선했다.  </p><p align="center">    <img alt="HFile version 2" style="max-width: 700px;" src="/images/hadoop/hfile-v2.png"/></p><p>HFile version 2 에서는 bloom filter block과 index block을 data block과 나란히 배치한다.<br>bloom filter block과 index block 모두 random read을 최적화하기위한 용도로 사용된다. index block은 index data로 빠르게 검색할 수 있도록 하며, bloom filter block은 해당 data가 있는지 없는지를 빠르게 필터링하는데에 사용된다.  </p><p>Index block 에는 3가지가 있다. Root Index block, Intermediate Index block, Leaf Index block 이다.<br>Root Index block은 HFile을 읽을때 바로 memory로 올린다. Root Index는 각 entry가 Intermediate Index block을 가리킨다. 그리고 Intermediate Index의 각 entry는 Leaf index block을 가리킨다. 마지막으로 Leaf index block은 실제 data block 을 가리킨다. 이는 b+tree와 매우 유사한 구조이다.  </p><p align="center">    <img alt="HBase multi-level index" style="max-width: 700px;" src="/images/hadoop/hbase-multi-level-index.png"/></p><p>각 index entry에서의 key를 구성하는 것은 크게 두가지 방식이 있다. <strong>Rowkey-based index</strong> 와 <strong>Column-based index</strong> 이다.<br>Rowkey-based index 는 HBase 의 built-in 인덱스 방식으로 rowkey 를 기반으로 특정 row를 빠르게 찾도록 도와준다. rowkey-based index 는 key로 rowkey를 포함한다.  </p><p>Column-based index 는 HBase 의 secondary index 구현에 사용된다. 이는 특정 column qualifier 값의 질의를 빠르게 매칭하는데 사용한다. column-based index 는 index table 이라고 불리는 HBase 의 또다른 테이블에 저장된다. index table 구조는 HBase 의 테이블과 유사하며 동일한 rowkey 와 column 을 사용한다. 하지만 value 는 기존의 테이블의 부분만 가지고있다.<br>따라서 특정 쿼리에서 column-based index 를 사용해야한다고 판단할때는 index table 을 보고 일치하는 row를 먼저 골라낸다. 그 다음 해당 row를 실제 main table 에서 찾는다.<br>따라서 특정 column qualifier value를 조회할때에는 column-based index 가 조회가 필요한 row 만 필터링해줄 수 있으므로 성능향상을 가져올 수 있다.  </p><h4 id="HFile-block"><a href="#HFile-block" class="headerlink" title="HFile block"></a>HFile block</h4><p>HBase 에서 조회를 할때 조회해야하는 row가 포함되어있는 HFile block을 찾는다. 이는 index 에서 rowkey로 검색하여 찾을 수 있다.<br>그리고 그 HFile block을 rowkey를 활용해 찾으면 이 block 전체를 메모리에 올린다. 이들은 정렬되어있는 key-value 쌍이므로 binary search 로 원하는 row 조회 및 특정 column qualifier 조회를 수행할 수 있다.  </p><p>Data block에도 header를 포함한다.  </p><p align="center">    <img alt="HFile data header" style="max-width: 500px;" src="/images/hadoop/hfile-block-header.png"/></p><p>header에는 Block Type을 포함하여 해당 block이 data block인지 Index 인지 다른 내용인지를 구별하도록 한다. 또한 이전 block의 offset도 저장하여 빠른 backward seek을 가능하도록 한다.   </p><p>HBase는 이처럼 데이터를 HFile이라는 큰 파일에 저장한다. 보통 HFile은 몇백 MB 부터 시작해 GB 단위로 커져간다.  </p><br/><h2 id="HBase-read"><a href="#HBase-read" class="headerlink" title="HBase read"></a>HBase read</h2><p>HBase의 read는 쉽다. 먼저 <code>Get</code> command instance를 통해 읽고싶은 cell을 지정하고 table에 보내면 된다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Get</span> <span class="hljs-variable">g</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Get</span>(Bytes.toBytes(<span class="hljs-string">&quot;TK-one&quot;</span>));<br><span class="hljs-type">Result</span> <span class="hljs-variable">r</span> <span class="hljs-operator">=</span> usersTable.get(g);<br></code></pre></td></tr></table></figure><p>table은 <code>Result</code> 객체를 반환하는데 이 객체는 해당 row의 모든 column family의 모든 column 들을 포함한다. 다만 이는 우리가 필요한 데이터보다 더 많을수 있으므로 구체적으로 얻고싶은 column만 명시를 할수도 있다.   </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Get</span> <span class="hljs-variable">g</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Get</span>(Bytes.toBytes(<span class="hljs-string">&quot;Tk-one&quot;</span>);<br>g.addColumn(Bytes.toBytes(<span class="hljs-string">&quot;info&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;name&quot;</span>));<br><span class="hljs-type">Result</span> <span class="hljs-variable">r</span> <span class="hljs-operator">=</span> usersTable.get(g);<br></code></pre></td></tr></table></figure><p>위에서는 <code>addColumn()</code> 메서드를 통해 원하는 column을 명시했지만 <code>addFamily()</code> 메서드를 활용하면 해당 column family의 전체 column을 가져올 수도 있다.  </p><p>HBase는 대부분의 읽기를 millisecond 단위로 제공한다. 보통의 일반적인 방법과 같이 HBase도 빠른 data access를 위해 data를 정렬된 상태로 유지하고 memory에 많이 올려놓는다. 그리고 위에서 설명하였듯이 write는 MemStore에 저장되지만 MemStore는 HFile로 flush되므로 read command를 처리하기 위해서는 HFile과 MemStore에서 적절하게 데이터를 잘 찾아야한다.  </p><p>HBase는 <strong>BlockCache</strong>라는 LRU Cache를 내부적으로 사용한다. 이 BlockCache는 JVM heap에 MemStore옆에 위치한다. BlockCache는 HFile에서 자주 접근되는 data들을 캐싱해서 in-memory hit를 하고 disk read를 줄이기 위한 목적이다. 각 column family마다 BlockCache를 가지고있다.<br>HBase를 최적의 성능을 내도록 하기위해서는 BlockCache를 이해하는게 중요하다.  </p><br/><h4 id="BlockCache"><a href="#BlockCache" class="headerlink" title="BlockCache"></a>BlockCache</h4><p>BlockCache의 Block은 HBase에서 disk에서 한번에 읽는 데이터 단위이다. 위의 HFile 에서의 data block이 이것이다.<br>HBase가 데이터를 읽기위해 HFile을 뒤져야할때는 HFile의 index 를 보고 binary search를 통해 key를 포함하고 있는 block의 위치를 알아낸 뒤 그 block(64KB)를 HDFS로 부터 읽어낸다. block size는 column family 별로 다르게 설정될 수 있으며 기본값은 64KB이다.<br>만약 Application이 HBase에서 random lookup이 많다면 block size를 작게하는게 도움이 될 수 있다. 반면 block size가 작아지면 block의 개수가 많아지므로 index는 조금 더 커질 것이다. sequential lookup이 많다면 block size를 반대로 크게 하는게 도움이 된다.  </p><p>BlockCache는 기본적으로 enable 된다. 즉 모든 read operation은 그에 연관된 block을 BlockCache에 올릴것이다. BlockCache는 내부적으로 block들의 종류에 따라 evict 정책을 다르게 가져간다.<br>예를들어 <code>hbase:meta</code> table 내용은 최대한 BlockCache에서 evict 되지 않도록한다.<br>HFile의 index 들도 BlockCache에 올라가는데 자주 사용되지 않는 index들은 evict된다. HFile index들은 multi-layered index로 HBase에서 data를 찾을때 빠르게 찾을 수 있도록 도와준다. 이 말고도 BloomFilter도 활성화되어있다면 BlockCache에 올린다.<br>기본적인 key-value data 들도 당연히 BlockCache에 올라간다.<br>같은 데이터를 여러번 접근하는 패턴은 BlockCache의 이득을 최대로 볼 수 있다.  </p><p>HBase에서 row를 읽을때에는 먼저 MemStore를 확인한다. 그 다음 BlockCache를 확인하고 해당 row가 BlockCache에 올라와있는지 확인한다. 최근에 row가 접근된 적이 있다면 BlockCache에 존재할 확률이 높다. 만약 BlockCache에도 찾지못한다면 그때 관련된 HFile들을 확인한다. 이때에는 disk read가 발생한다. 완전한 row를 찾기위해서는 모든 HFile을 뒤져야한다.  </p><br/><h2 id="HBase-delete"><a href="#HBase-delete" class="headerlink" title="HBase delete"></a>HBase delete</h2><p>Delete는 HBase의 데이터를 저장하는 방식과 비슷하게 작동한다. 먼저 Delete를 하려면 <code>Delete</code> command의 인스턴스를 생성해야한다.<br>다음은 rowkey를 명시하여 해당 row를 삭제하는 코드이다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Delete</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Delete</span>(Bytes.toBytes(<span class="hljs-string">&quot;TK-one&quot;</span>));<br>userTables.delete(d);<br></code></pre></td></tr></table></figure><p>row 자체를 전부 삭제하는게 아닌 특정 coordinate를 명시해서 해당 cell만 삭제할 수도 있다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Delete</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Delete</span>(Bytes.toBytes(<span class="hljs-string">&quot;TK-one&quot;</span>));<br>d.deleteColumns(Bytes.toBytes(<span class="hljs-string">&quot;info&quot;</span>), Bytes.toBytes(<span class="hljs-string">&quot;name&quot;</span>));<br>userTables.delete(d);<br></code></pre></td></tr></table></figure><p>실제 삭제는 어떻게 진행될까?<br>실제로 <code>Delete</code> command는 해당 값을 바로 삭제하지 않는다. 대신에 해당 record가 삭제되었다는 마킹만 한다.  이는 HFile을 생각해보면 당연한 설계이다. HFile은 immutable하다. 그러므로 애초에 record를 수정하거나 삭제할 수가 없다. 그러므로 해당 record가 삭제되었다는 <strong>새로운 record를 write</strong>한다. 이를 <strong>tombstone</strong> 이라고 한다. 그래서 보통 HBase 에서 삭제한다고 하면 tombstone marking을 했다고 표현한다.<br>이 tombstone mark는 <code>Get</code>이나 <code>Scan</code>을 할때 해당 record가 결과에 포함되지 않도록 보장한다. 실제 삭제가 되었어야 하는 original 데이터는 계속해서 HFile에 남아있을 수 밖에 없는데, 이런 데이터들은 밑에서 볼 <strong>major compaction</strong> 단계에서 제거된다.  </p><br/><h2 id="HBase-Compaction"><a href="#HBase-Compaction" class="headerlink" title="HBase Compaction"></a>HBase Compaction</h2><p>Memstore가 어느정도 크기에 도달하거나 Region 서버가 Memstore에 너무 많은 메모리를 쓰고있다고 판단되면 Memstore는 flush를 하여 새로운 HFile을 만들어낸다고 했다.<br>매 flush마다 새로운 HFile이 생성되므로, 우리는 <code>Get</code>과 <code>Scan</code>을 수행할때 key를 찾기위해 해당 요청과 관련있는 모든 HFile을 다 뒤져봐야하므로 이는 성능이 좋지않다. 즉 HFile의 개수를 제한하는 것이 성능에 매우 중요한 부분을 차지한다. 이를 극복하기위해 HBase는 HFile의 개수가 특정개수를 넘어갈때 <strong>Compaction</strong>을 진행하여 여러개의 HFile을 하나의 큰 HFile로 병합한다.  </p><p>HBase의 compaction에는 2가지 종류가 있다. <strong>Minor Compaction</strong>과 <strong>Major Compaction</strong>이다.  </p><br/><h4 id="Minor-Compaction"><a href="#Minor-Compaction" class="headerlink" title="Minor Compaction"></a>Minor Compaction</h4><p>minor compaction은 간단하다. 작은 HFile 여러개를 하나의 큰 HFile로 합친다. minor compaction 과정은 HBase에 성능저하를 최소한으로 있도록 설계되었기 때문에 minor compaction 대상이되는 HFile 개수는 상한선이 있다. 이 값은 설정값으로 조정이 가능하다.<br>minor compaction은 작은 HFile 들로부터 record를 읽어 이들을 정렬하고 큰 HFile에 새로 write한다. 과정은 다음 그림과 같다.  </p><p align="center">    <img alt="minor compaction in HBase" style="max-width: 600px;" src="/images/hadoop/major-compaction.png"/></p><br/><h4 id="Major-Compaction"><a href="#Major-Compaction" class="headerlink" title="Major Compaction"></a>Major Compaction</h4><p>major compaction은 column family의 모든 HFile를 대상으로 수행되는 compaction이다. major compaction이 완료되면 해당 column family의 모든 HFile들은 하나의 HFile로 병합된다. 이 major compaction은 비용이 비싸므로 자주 일어나지 않는다. 다만 minor compaction은 빈번하게 일어난다.<br>major compaction 단계에서는 tombstone marker로 표시해둔 record를 완전히 삭제한다. 또한 tombstone marker record 자체도 같이 삭제한다.  </p><p>왜 minor compaction은 이와같은 deleted mark record를 삭제하지 못할까?<br>실제로 삭제대상의 record가 있는 HFile과 tombstone marker record가 있는 HFile은 다를 수 있고 삭제대상의 record가 어느 HFile에 있는지 모르기 때문이다. minor compaction은 작은 몇개의 HFile을 대상으로만 진행된다. 그러므로 major compaction에서 이를 담당한다.  </p><h4 id="HBase-data-locality"><a href="#HBase-data-locality" class="headerlink" title="HBase data locality"></a>HBase data locality</h4><p>HBase는 HDFS로부터 HFile을 읽을때 어느 node에서 읽을까?<br>이를 위해서는 먼저 HBase와 HDFS가 같은 cluster에 있는지 확인해야한다. 만약 같은 cluster에 없다면 HFile은 항상 HBase의 Region server와 다른 노드에 있으므로 network 비용이 발생한다.<br>만약 같은 cluster에 존재한다면 Region Server가 HDFS에 HFile을 쓸 때 HDFS는 가능하면 그 파일을 쓰는 datanode에 replica가 저장될 수 있도록 해준다. 따라서 Region Server에서 HFile 접근시 local disk에 접근하므로 data locality를 보장할 수 있다.<br>만약 Region Server가 문제가 생겨 새로 서버가 시작되었다고 하더라도 처음에는 다른 HFile을 읽기위해 다른 HDFS datanode로 부터 파일을 읽어오겠지만, 충분한 시간이 지난다면 major compaction이 발생하고 결국 HFile을 새로 다시 쓰기때문에 이 부분에서 다시한번 data locality를 보장할 수 있다.  </p><br/><h2 id="HBase-분산모드"><a href="#HBase-분산모드" class="headerlink" title="HBase 분산모드"></a>HBase 분산모드</h2><p>HBase의 테이블은 row와 column으로 구성되어 있고 수십억개의 row와 수백만 개의 column으로 확장이 가능하다. 각 테이블은 petabyte 단위까지도 증가할 수 있다. 다만 단일머신에서는 이를 서비스하기 힘들다. 어떻게 이를 가능하게 할까?  </p><p>HBase는 table을 작은 단위로 분할시키고 이를 여러 서버에 나누어 서비스한다. 이 작은 단위를 <strong>Region</strong> 이라고 부른다. 이 Region을 서비스하는 서버를 <strong>RegionServer</strong> 라고 한다.<br>일반적으로는 RegionServer 들은 HDFS datanode와 같은 물리적인 서버에 위치해 있다. 꼭 같은 물리서버에 위치해야할 필요는 없지만 locality를 얻기위해 그리고 성능최적화를 위해서는 같은 물리적 서버에 위치하도록 하는게 좋다. RegionServer 들은 HDFS의 입장에서는 HDFS를 사용하는 클라이언트중 하나이다.<br>HMaster라 불리는 process가 region을 할당하고 분배하는 역할을 수행하고 각 RegionServer는 일반적으로는 여러개의 Region을 서비스한다.<br>Region은 table을 rowkey를 기준으로 적절하게 범위를 나누어 할당된다. 다음 그림으로 Region의 분리를 볼 수 있다.  </p><p align="center">    <img alt="table to region" style="max-width: 700px;" src="/images/hadoop/hbase-region-example.png"/></p><p>Region이 너무 커지거나 Region이 나누어져야 하는 특정조건을 만족하면 RegionServer는 Region을 다시 작은 크기로 쪼갠다.  </p><p>client 가 특정 row에 접근하고자 할때는 어느 Region에 있고 이를 어떤 RegionServer가 호스팅하고 있는지 어떻게 알 수 있을까?<br>이 정보는 <strong>.META.</strong> 라는 HBase 내의 table이 도움을 준다. 실제 table 이름은 <code>hbase:meta</code> 이며 HBase의 모든 region 정보를 가지고 있다. 그리고 zookeeper가 <code>hbase:meta</code> table의 위치를 저장한다. .META. 테이블은 1개의 region으로만 사용하고 있다.  </p><p>따라서 client는 특정 row에 접근할때 zookeeper로부터 <code>hbase:meta</code> region을 서비스하는 RegionServer를 알아내고, 이 .META. table 정보를 들고있는 RegionServer에 해당 rowkey를 어떤 region과 RegionServer 에서 제공하고 있는지 질의한다.<br>그에 대한 RegionServer 정보를 받으면 그 RegionServer로 해당 row의 읽기나 쓰기작업을 진행한다.  </p><br/><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.quora.com/Why-does-HBase-keep-WAL-on-HDFS-instead-of-local-disks">https://www.quora.com/Why-does-HBase-keep-WAL-on-HDFS-instead-of-local-disks</a></li><li><a href="https://blog.cloudera.com/apache-hbase-i-o-hfile/">https://blog.cloudera.com/apache-hbase-i-o-hfile/</a></li><li><a href="https://nag-9-s.gitbook.io/hbase/hbase-architecture/region-servers/hfile">https://nag-9-s.gitbook.io/hbase/hbase-architecture/region-servers/hfile</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/Hadoop/">Hadoop</category>
      
      
      <category domain="https://tk-one.github.io/tags/hadoop/">hadoop</category>
      
      <category domain="https://tk-one.github.io/tags/hbase/">hbase</category>
      
      
      <comments>https://tk-one.github.io/2022/06/18/hbase/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Hadoop HDFS란?</title>
      <link>https://tk-one.github.io/2022/03/23/hadoop-hdfs/</link>
      <guid>https://tk-one.github.io/2022/03/23/hadoop-hdfs/</guid>
      <pubDate>Wed, 23 Mar 2022 13:42:24 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;현 회사에서 Hadoop을 적극적으로 사용하고 있고 저장소로 HBase를 사용하며 MR(MapReduce)을 다루는 경우가 많기 때문에 Hadoop 관련 내용을 정리하면 좋겠다고 생각했다.&lt;br&gt;이 글을 읽기 전에 &lt;a href=&quot;/2020/09</description>
        
      
      
      
      <content:encoded><![CDATA[<p>현 회사에서 Hadoop을 적극적으로 사용하고 있고 저장소로 HBase를 사용하며 MR(MapReduce)을 다루는 경우가 많기 때문에 Hadoop 관련 내용을 정리하면 좋겠다고 생각했다.<br>이 글을 읽기 전에 <a href="/2020/09/05/file-system-hard-disk/">파일시스템 1편 - 하드디스크</a> 를 먼저 읽으면 도움이 될 수 있습니다.  </p><p>먼저 Hadoop distributed file system을 알아보자.  </p><p align="center">    <img style="max-width: 700px;" src="/images/hadoop/hadoop.png"/></p><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>HDFS는 Hadoop Distributed File System 의 약어다.<br>HDFS는 하둡의 대표적인 분산파일시스템이다. 그렇다고 하둡에서 꼭 HDFS를 사용해야하는 것은 아니다. 하둡은 범용 파일시스템을 추구한다. 하둡은 파일시스템의 추상화개념을 가지고있고 HDFS는 그 구현체 중 하나일 뿐이다.<br>데이터가 단일 디스크의 저장용량을 초과하면 이 데이터들을 쪼개서 여러개의 머신에 저장해야한다. 이렇게 여러개의 머신으로 파일을 저장하고 서로 네트워크로 묶으면 여러머신의 스토리지를 관리할 수 있는데 이를 분산파일시스템이라고 한다.  </p><p>HDFS는 여러개의 머신으로 구성된 클러스터에서 실행되고 대용량 파일을 다룰 수 있도록 설계된 파일시스템이다.<br>HDFS는 petabyte단위의 파일을 다룰 수 있고 하드웨어는 항상 장애가 날 수 있는데 노드장애가 발생하더라도 대형 클러스터에서 문제없이 실행되도록 설계되었다.  </p><p>HDFS는 기본적으로 빠른 응답시간을 요구하는 애플리케이션에는 맞지않다. 설계자체가 높은 데이터 처리량을 제공을 목표로 하기 때문이다. 빠른 응답시간을 원하면 HBase가 선택지가 될 수 있겠다.<br>그리고 HDFS는 파일을 생성하거나 파일 끝에 append하는 것은 가능하지만 파일 중간에 내용을 update하는 것은 불가능하다. 한번의 쓰기작업 그리고 여러번 읽는 방식이 가장 효율적인 방식이도록 설계되었기 때문에 데이터를 수정하려면 현재 데이터를 삭제후 수정한 데이터를 새로 생성해야 한다.  </p><br/><h2 id="HDFS-Block"><a href="#HDFS-Block" class="headerlink" title="HDFS Block"></a>HDFS Block</h2><p>여기서 말하는 block은 흔히 파일시스템에서 말하는 block을 말한다.<br>보통 우리가 아는 단일디스크에서의 파일시스템에서 사용하는 block는 4KB이고, 디스크 자체는 기본적으로 512KB의 sector size를 가진다.<br>이와같이 HDFS도 block 개념이 있는데 HDFS block은 size가 굉장히 크다.<br>HDFS block size는 기본적으로 128MB이며 보통은 이것보다는 큰 block size를 사용한다. HDFS block이 큰 이유는 기본적으로  탐색비용(disk에서의 seek time + rotational delay)을 줄이기 위함이다. (SSD에서도 sequential read가 random read 보다 빠르다)<br>Disk seek 비용은 상당히 비싸다. Hadoop은 전체 dataset을 탐색하도록 설계되었기 때문에 큰 block size로 sequential read를 통해 성능을 크게 개선할 수 있다.<br>예를들어 position time(seek time + rotational delay)가 10ms 이고, disk 전송률이 100MB&#x2F;s 일때 position time을 전송시간의 1%로 만들고 싶다면 block size를 100MB로 잡으면된다.(position time 10ms + 100MB 전송에 1초가 걸리므로)  </p><p>또 만약에 block size가 작으면 파일에 대한 block의 개수자체도 많아질텐데 뒤에 보면 알겠지만 HDFS는 파일시스템 metadata를 메모리에서 관리한다. 그러면 모든 block에 대해 metadata를 들고 있어야 하는데 block 개수자체가 많아지므로 metadata가 너무 커지는 문제가 있다. 결국 block이 너무 많으면 오버헤드가 발생하고 네트워크 트래픽이 증가하는 문제가 발생할 수 있다.<br>HDFS에서의 파일은 우리가 단일디스크 파일시스템에서 파일을 저장할 때처럼 block size별로 chunk되어 저장된다.<br>다만 HDFS는 데이터가 block size보다 작을경우, 해당 block(128MB)을 모두 차지하지는 않는다(기본적인 단일디스크 파일시스템에서는 데이터가 4KB 보다 작아도 4KB block을 모두 차지한다). 그러므로 파일크기가 block size보다 작다고 공간이 버려지지는 않는다.</p><p>HDFS에도 block 개념이 있기때문에 여러가지 이점이 있는데 먼저 단일디스크에 있는 파일시스템에 다 담지 못하는 크기의 파일도 여러 block으로 나누어 여러 디스크에 저장할 수 있다. 그리고 block 단위의 추상화가 들어가면 스토리지의 subsystem을 단순화할 수 있고 metadata 관리가 편하다. 또 block은 fault tolerance와 availability를 제공하기 위한 replication을 구현하는데에 적합하다. block 마다 replicaion factor 개수만큼 여러머신에 중복저장하고 특정 노드에서 block을 읽을 수 없다면 다른 노드를 사용하게 할 수있다.</p><br/><h2 id="Namenode-And-Datanode"><a href="#Namenode-And-Datanode" class="headerlink" title="Namenode And Datanode"></a>Namenode And Datanode</h2><p>HDFS cluster는 master인 네임노드와 worker인 데이터노드로 구성되어 있다.<br>네임노드는 파일시스템 트리, 그리고 그 트리에 포함된 모든 파일, 디렉터리에 대한 metadata를 유지한다. 파일시스템에서 inode를 네임노드에서 관리하고 있다고 이해하면 좋겠다. 그리고 이 내용은 namespace image와 edit log라는 두 종류의 파일로 local disk에 영구히 저장된다.<br>네임노드는 모든 HDFS block이 어느 데이터노드에 있는지 모두 알고있다. 하지만 이 정보는 local disk에 영속 저장하지는 않고 시스템 시작시 데이터노드로 부터 받아서 재구성한다. 따라서 네임노드를 다른 것으로 교체할 때 전체 데이터노드에서 충분한 block report를 받아 안전모드를 벗어날때까지 요청을 처리할 수 없는데 이 시간이 30분이상 걸리기도 한다.  </p><p>사용자가 직접 네임노드, 데이터노드와 통신해야 하는것은 아니고 이 모든 것을 HDFS client가 대신해서 이들에게 접근한다.<br>데이터노드는 HDFS client나 네임노드의 요청이 있을때 block을 저장하고 탐색하며, 주기적으로 저장하고 있는 block list들을 네임노드에 보고한다.<br>또 네임노드는 모든 파일과 각 block의 참조정보를 다 메모리에 들고있다. 그래서 대형 cluster에서는 메모리가 걸림돌이 되는데 이부분은 HBase Federation을 적용하여 해결할 수 있다. 이는 네임노드를 여러개두고 각 네임노드가 특정 namespace를 담당하도록 하는 것이다. 예를들어 어떤 네임노드는 <code>/user</code>을 관리하고 어떤 네임노드는 <code>/bar</code>를 관리할 수 있다. 그러면 특정 네임노드가 장애가 나도 다른 namespace의 가용성에는 영향을 주지 않는 장점도 있다.<br>HDFS는 네임노드가 정말 중요한데 네임노드가 장애면 시스템의 어떤 파일도 찾을 수 없다. 모든 block 정보를 이용해 네임노드가 파일을 재구성하기 때문이다. 그래서 네임노드의 장애극복은 필수적이며, Hadoop은 이를위해 2.x 부터 HDFS HA를 지원한다.    </p><p>HA는 보조 네임노드를 운영한다. 네임노드를 Active-StandBy 구조로 한 쌍으로 구성하여 Active 네임노드에 장애가 발생할 경우 StandBy 네임노드가 이를 이어받는 방식이다.</p><br/><h2 id="File-read"><a href="#File-read" class="headerlink" title="File read"></a>File read</h2><p>클라이언트가 HDFS의 파일을 읽을때 내부적으로 어떤 일이 일어나는지 살펴보자.  </p><p align="center">    <img alt="HDFS File read" src="/images/hadoop/hdfs-file-read.png"/></p>  <p>먼저 클라이언트는 hadoop에서 제공하는 <code>FileSystem</code> 객체의 <code>open()</code>을 호출하여 원하는 파일을 열어야한다. 그러면 해당 파일의 첫번째 block이 어디있는지 파악하기 위해 RPC로 네임노드를 호출한다. 그러면 네임노드는 block 별로 해당 block의 복제본을 가진 데이터노드의 주소를 반환한다. 이때 데이터노드의 순서는 cluster의 network topology에 따라 클라이언트에 가장 가까운순으로 정렬되어 반환된다. 예를들어 클라이언트 자체가 데이터노드고 해당 block의 복제본을 본인이 가지고 있으면 첫번째 데이터노드는 로컬이 될 것이다.  </p><p>block 위치정보를 반환받으면 이 정보를 기반으로 데이터를 읽을 수 있도록 <code>FSDataInputStream</code>을 반환한다. 이는 스트림으로 클라이언트는 read 메서드를 호출하면된다. 그때 내부적으로 첫번째 데이터노드와 연결해 데이터를 전송받는다. 만약 block의 끝에 도달했으면 다음 block의 데이터노드와 연결해 데이터를 전송받는다. 이 과정은 내부적으로 일어나며 클라이언트는 스트림을 읽는것처럼 보인다.  </p><p>만약 데이터노드와의 통신에 문제가 생기면 해당 block을 저장하고 있는 다음 데이터노드와 연결을 시도하고, 문제가 생긴 데이터노드는 네임노드에 report한다.<br>전반적인 읽기는 클라이언트가 데이터노드에 직접 접근하여 데이터를 읽어오고, 네임노드가 각 block의 데이터노드를 적절하게 알려준다. 이런 과정을 통해 File 읽기에 대한 트래픽은 모든 데이터노드에 고르게 분산된다. 네임노드는 모든 클라이언트의 요청을 처리해야하지만 메타데이터를 메모리에 저장하여 memory read로 끝나기때문에 많은 클라이언트의 요청을 동시에 처리할 수 있다.  </p><br/><h2 id="File-write"><a href="#File-write" class="headerlink" title="File write"></a>File write</h2><p>클라이언트가 HDFS의 파일을 쓰게될때 어떤 일이 일어나는지 살펴보자.  </p><p align="center">    <img alt="HDFS File write" src="/images/hadoop/hdfs-file-write.png"/></p>  <p>클라이언트는 <code>DistributedFileSystem</code>의 <code>create()</code> 메서드를 호출하여 파일을 생성한다. 그러면 네임노드에 RPC를 보내는데 이때는 파일생성권한이 적절하게 있는지 동일파일이 존재하는지 등 검사를 진행한다. 검사가 통과되면 새로운 파일의 레코드를 만들어 저장하고 반환한다. 이때 block 정보는 반환하지 않는다.<br>클라이언트는 file read와 마찬가지로 <code>FSDataOutputStream</code>을 반환받고 이를 스트림으로 write할 수 있다. 다만 쓸때는 read 과정과 조금 다르다.  </p><p>클라이언트가 파일에 데이터를 쓸때에는 각 데이터를 패킷으로 나누어 분리하고 클라이언트의 내부 queue인 data queue라고 불리는 queue에 해당 패킷들을 쌓는다. 그러면 <code>DataStreamer</code>가 이 패킷들을 처리한다. 먼저 block을 어느 데이터노드에 써야하는지 모르므로, 네임노드로부터 복제본을 저장할 데이터노드 목록을 요청하고 반환받는다.<br>다만 이 데이터노드들은 pipeline을 형성하는데 replication factor가 3이라면 세개의 노드가 pipeline에 속한다.<br><code>DataStreamer</code>는 첫번째 데이터노드에 먼저 패킷들을 전송한다. 첫번째 데이터노드는 이를 저장하고 나서 이를 다음 pipeline의 데이터노드로 보낸다. 이어서 두번째 데이터노드는 다시 패킷을 저장하고 다음 pipeline의 데이터노드로 저장한다. 그림을 보면 이와같은 내용을 설명하고 있다.  </p><p>클라이언트는 내부 패킷을 저장하는 ack queue를 들고있어 각 데이터노드로 부터 ack 응답을 전부 받으면 해당 패킷이 queue에서 삭제된다.<br>만약 데이터노드의 쓰기에 문제가 있다면 ack를 받지못한다. 그러면 장애복구작업이 시작되는데 ack queue에 있는 패킷들이 다시 data queue에 들어가서 재시도된다.  </p><p>즉 file write는 비동기적으로 pipeline을 통해 데이터노드에 써주게되는데 <code>dfs.namenode.replication.min</code>에 설정된 개수의 데이터노드에만 block 저장이 성공하면 write은 성공한 것으로 반환된다. 기본값은 1 이다. 그리고 <code>dfs.replication</code> 값인 replication factor에 도달할때까지 클러스터에 걸쳐 복제가 비동기적으로 수행된다. replication factor의 기본값은 3이다.  </p><p>클라이언트가 file write을 다했을때는 <code>close()</code> 메서드를 호출한다. 그러면 클라이언트는 데이터노드 pipeline으로 남아있는 모든 패킷을 flush하고 ack를 기다린다. 그 이후에는 네임노드에 file write이 완료되었음을 알린다. <code>dfs.namenode.replication.min</code> 만큼의 block이 복제가 완료되었으면 성공을 반환한다.  </p><br/><h4 id="복제본-배치"><a href="#복제본-배치" class="headerlink" title="복제본 배치"></a>복제본 배치</h4><p>네임노드는 복제본을 저장할 데이터노드를 어떻게 선택할까?<br>노드간의 쓰기 대역폭을 줄이기위해 단일노드에 모두 복제본을 배치하면 복제의 의미가 없다. 해당 노드가 장애시 data loss가 일어난다.  </p><p>하둡에서는 첫번째 복제본은 클라이언트와 같은 노드에 배치한다. 그런데 클라이언트가 cluster 내부의 노드가 아니라면 무작위로 노드를 선택한다. 이 과정에서 노드들의 파일개수나 해당 노드의 자원상황을 고려한다.<br>두번째 복제본은 첫번째 복제본을 저장한 노드와 <strong>다른 랙</strong>에서 노드를 무작위로 선택한다. 세번째 복제본은 두번째 복제본이 저장되는 랙과 동일한 랙에서 다른 노드를 선택한다. 그 이상의 replication factor를 가졌다면 다음 노드들은 무작위로 선택한다.  </p><p>하둡은 block을 두개의 랙에 저장함으로서 신뢰성을 가지고, 쓰기 대역폭은 하나 혹은 두개의 네트워크 스위치만 통하도록 설계되었다. 읽기 성능을 위해서도 두개의 랙중 가까운 것을 선택하도록 한다. 그리고 전반적인 cluster의 block 분산의 균형을 적절하게 맞춘다.  </p><p align="center">    <img alt="HDFS block copy 배치" style="max-width: 400px;" src="/images/hadoop/hdfs-block-copy.png"/></p><br/><h2 id="일관성-모델"><a href="#일관성-모델" class="headerlink" title="일관성 모델"></a>일관성 모델</h2><p>일관성 모델은 coherence model 로 부른다. 이는 파일에 대한 read, write에 대해 visibility 가 어떻게 되는지 설명한다.<br>HDFS에서 파일을 생성하면 HDFS namespace에서 파일의 존재를 확인할 수 있다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Path</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;p&quot;</span>);<br>fs.create(p);<br>assertThat(fs.exists(p), is(<span class="hljs-literal">true</span>));<br></code></pre></td></tr></table></figure><p>하지만 파일을 write 할 때, 스트림을 flush 했다고 해서 해당 파일의 내용일 읽을 수 있음을 보장하지는 않는다.<br>다음 예제처럼 stream을 flush하고 읽었을때 파일의 내용이 비어있을 수 있다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Path</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;p&quot;</span>);<br><span class="hljs-type">OutputStream</span> <span class="hljs-variable">out</span> <span class="hljs-operator">=</span> fs.create(p);<br>out.write(<span class="hljs-string">&quot;content&quot;</span>.getBytes(<span class="hljs-string">&quot;UTF-8&quot;</span>));<br>out.flush();<br>assertThat(fs.getFileStatus(p).getLen(), is(<span class="hljs-number">0L</span>));<br></code></pre></td></tr></table></figure><p>일단 file의 데이터가 한 개의 block 넘게 기록이 되면, 그 file의 첫번째 block은 reader들이 볼 수 있다.<br>다만, 현재 쓰여지고 있는 block은 다른 reader 들에게 보이지 않을 수 있다.  </p><p>HDFS는 모든 buffer들이 데이터노드들에 강제로 flush 할 수 있는 <code>hflush()</code> 라는 메서드를 제공한다.<br><code>hflush()</code>가 성공하면, HDFS는 write pipeline에 속한 모든 데이터노드들에 file write에 대한 요청이 도달했음을 보장해준다. 그러므로 모든 다른 reader들이 이를 읽을 수 있다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Path</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;p&quot;</span>);<br><span class="hljs-type">OutputStream</span> <span class="hljs-variable">out</span> <span class="hljs-operator">=</span> fs.create(p);<br>out.write(<span class="hljs-string">&quot;content&quot;</span>.getBytes(<span class="hljs-string">&quot;UTF-8&quot;</span>));<br>out.hflush();<br>assertThat(fs.getFileStatus(p).getLen(), is((<span class="hljs-type">long</span>) <span class="hljs-string">&quot;content&quot;</span>.length()));<br></code></pre></td></tr></table></figure><p>한가지 주의해야 할 점은 <code>hflush()</code>는 데이터노드가 해당 파일을 disk에 썼음을 보장하지 않는다. 오직 데이터노드의 메모리에 썼음을 보장한다. 따라서 <code>hflush()</code>를 호출했음에도 data center 장애가 일어나면 데이터 유실이 일어날 수 있다.<br>데이터노드에서의 disk write도 보장하고 싶다면 <code>hsync()</code>를 호출해야 한다.  </p><p><code>hsync()</code>는 POSIX의 <code>fsync()</code>와 같다. 파일시스템 편에서의 buffer cache를 flush 하기 위한 <code>fsync()</code>를 기억하라.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Path</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;p&quot;</span>);<br><span class="hljs-type">OutputStream</span> <span class="hljs-variable">out</span> <span class="hljs-operator">=</span> fs.create(p);<br>out.write(<span class="hljs-string">&quot;content&quot;</span>.getBytes(<span class="hljs-string">&quot;UTF-8&quot;</span>));<br>out.flush();<br>out.getFD().sync(); <span class="hljs-comment">// disk에 sync를 보장한다.</span><br>assertThat(fs.getFileStatus(p).getLen(), is((<span class="hljs-type">long</span>) <span class="hljs-string">&quot;content&quot;</span>.length()));<br></code></pre></td></tr></table></figure><p>HDFS에서의 <code>close()</code> 메서드는 내부적으로 <code>hflush()</code>를 호출한다.  </p><p>이런 HDFS의 일관성 모델을 보고 애플리케이션의 디자인을 어떻게 해야할지 결정해야한다.<br><code>hflush()</code>나 <code>hsync()</code> 없이는 block의 저장을 보장할 수 없다. 하지만 이에도 tradeoff가 있다. <code>hflush()</code>는 비싼작업은 아니지만 그래도 오버헤드가 존재하고 <code>hsync()</code>는 비용이 더 비싸다. 애플리케이션에서 적절하게 일관성 모델을 정하고 그에 맞게 HDFS의 <code>hflush</code>와 <code>hsync</code>를 활용하여 성능과 데이터 신뢰성을 잘 결정해야 한다.  </p><br/><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.amazon.com/Hadoop-Perfect-Guide-Korean-White/dp/8968484597">https://www.amazon.com/Hadoop-Perfect-Guide-Korean-White/dp/8968484597</a></li></ul><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/Hadoop/">Hadoop</category>
      
      
      <category domain="https://tk-one.github.io/tags/hadoop/">hadoop</category>
      
      <category domain="https://tk-one.github.io/tags/hdfs/">hdfs</category>
      
      
      <comments>https://tk-one.github.io/2022/03/23/hadoop-hdfs/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>코로나 확진후기</title>
      <link>https://tk-one.github.io/2022/03/07/coronavirus-confirmed/</link>
      <guid>https://tk-one.github.io/2022/03/07/coronavirus-confirmed/</guid>
      <pubDate>Mon, 07 Mar 2022 13:21:13 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;결론부터 말하자면 2022년 3월 3일 코로나19 양성판정을 받았다.&lt;br&gt;계속 언제, 어디서부터 감염되었는지 생각해봤는데 아무리 생각해도 잘 모르겠다.&lt;br&gt;간 곳이라고는 집앞에 있는 스터디카페밖에 없었고 그 스터디카페는 정말 큼지막해서 사람간</description>
        
      
      
      
      <content:encoded><![CDATA[<p>결론부터 말하자면 2022년 3월 3일 코로나19 양성판정을 받았다.<br>계속 언제, 어디서부터 감염되었는지 생각해봤는데 아무리 생각해도 잘 모르겠다.<br>간 곳이라고는 집앞에 있는 스터디카페밖에 없었고 그 스터디카페는 정말 큼지막해서 사람간의 동선도 거의 겹치지 않는다.  </p><p>확진받은 날인 3월 3일 아침에 일어났을때 평소보다 목이 조금 잠겨있었다.<br>이거 코로나인가..? 라고 생각은 들지 않았을만큼 아주 미세한 차이였고 그래도 뭔가 찝찝해서 아침에 집에 있는키트를 사용하여 진단해본 결과 <strong>음성</strong>이 나왔다.<br>그래서 내가 확진되었다는 생각은 전혀 하지 못했지만, 오후에 갑자기 엄마가 확진판정을 받았다고 했다.<br>그 말을 듣자마자 나도 바로 PCR검사를 받으러 갔고 결과는 다음과 같다.</p><p><img src="/images/covid19-confirmed.jpg" alt="covid19 message"></p><p>흑흑흑..<br>처음에 카톡으로 알림이 왔을때 나한테 잘못온줄 알았다.<br><del>아니 내가 걸린다고?</del><br>심지어 당일 아침에 자가진단키트로는 음성이 나왔어서 더 예상을 못했던 것 같다.  </p><p>기가막히게 확진을 받은 다음날 새벽부터 오한이왔고 심한 목감기처럼 통증이 심해졌다.<br>회사에도 현 상황을 공유해드리고 아픈건 맞지만 어차피 재택근무이고 업무하는데에는 무리가 없다는 개인적인 판단하에 계속해서 재택근무를 하기로 결정했다.<br>감사하게도 회사에서 상비약을 마련해서 바로 퀵으로 보내주셨고 정말 많은 도움이 되었다.<br>양도 굉장히 많아서 우리가족 다같이 나눠먹었다.  </p><p>이제 자가격리가 3일 남았는데 얼른 끝났으면 좋겠다…<br>공부해야될게 산더미인데 스터디카페를 못간다니..  </p>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/personal/">personal</category>
      
      
      <category domain="https://tk-one.github.io/tags/covid19/">covid19</category>
      
      <category domain="https://tk-one.github.io/tags/coronavirus/">coronavirus</category>
      
      
      <comments>https://tk-one.github.io/2022/03/07/coronavirus-confirmed/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>마크다운(Markdown) 사용법</title>
      <link>https://tk-one.github.io/2022/03/05/how-to-use-markdown/</link>
      <guid>https://tk-one.github.io/2022/03/05/how-to-use-markdown/</guid>
      <pubDate>Sat, 05 Mar 2022 12:59:06 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;이번 포스트는 markdown에 대해 알아보는 포스트입니다.&lt;br&gt;자주 사용하는 markdown 키워드들 위주로 정리하였습니다.  &lt;/p&gt;
&lt;h2 id=&quot;Italics-and-Bolds&quot;&gt;&lt;a href=&quot;#Italics-and-Bolds&quot; cla</description>
        
      
      
      
      <content:encoded><![CDATA[<p>이번 포스트는 markdown에 대해 알아보는 포스트입니다.<br>자주 사용하는 markdown 키워드들 위주로 정리하였습니다.  </p><h2 id="Italics-and-Bolds"><a href="#Italics-and-Bolds" class="headerlink" title="Italics and Bolds"></a>Italics and Bolds</h2><ul><li>Italics<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">this is _italic_</span><br></pre></td></tr></table></figure></li></ul><p>this is <em>italic</em></p><ul><li>Bolds<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">this is **bold**</span><br></pre></td></tr></table></figure></li></ul><p>this is <strong>bold</strong></p><ul><li>Italics and Bolds<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">this is **_italics and bolds_**</span><br></pre></td></tr></table></figure></li></ul><p>this is <strong><em>italics and bolds</em></strong></p><h2 id="Headers"><a href="#Headers" class="headerlink" title="Headers"></a>Headers</h2><ul><li>Header 1부터 6까지 지원한다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># header 1</span><br><span class="line">## header 2</span><br><span class="line">### header 3</span><br><span class="line">#### header 4</span><br><span class="line">##### header 5</span><br><span class="line">###### header 6</span><br></pre></td></tr></table></figure></li></ul><h1 id="header-1"><a href="#header-1" class="headerlink" title="header 1"></a>header 1</h1><h2 id="header-2"><a href="#header-2" class="headerlink" title="header 2"></a>header 2</h2><h3 id="header-3"><a href="#header-3" class="headerlink" title="header 3"></a>header 3</h3><h4 id="header-4"><a href="#header-4" class="headerlink" title="header 4"></a>header 4</h4><h5 id="header-5"><a href="#header-5" class="headerlink" title="header 5"></a>header 5</h5><h6 id="header-6"><a href="#header-6" class="headerlink" title="header 6"></a>header 6</h6><h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><h4 id="inline-링크와-reference-링크가-있다"><a href="#inline-링크와-reference-링크가-있다" class="headerlink" title="inline 링크와 reference 링크가 있다."></a>inline 링크와 reference 링크가 있다.</h4><ul><li>inline link<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[블로그 바로가기](https://tk-one.github.io)</span><br></pre></td></tr></table></figure></li></ul><p><a href="https://tk-one.github.io">블로그 바로가기</a></p><ul><li>reference link<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Here&apos;s [블로그 바로가기][my github].</span><br><span class="line"></span><br><span class="line">[my github]: https://tk-one.github.io</span><br></pre></td></tr></table></figure></li></ul><p>Here’s <a href="https://tk-one.github.io">블로그 바로가기</a>.  </p><h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><h4 id="이미지도-링크와-비슷하게-inline-이미지와-reference-이미지가-있다"><a href="#이미지도-링크와-비슷하게-inline-이미지와-reference-이미지가-있다" class="headerlink" title="이미지도 링크와 비슷하게 inline 이미지와 reference 이미지가 있다."></a>이미지도 링크와 비슷하게 inline 이미지와 reference 이미지가 있다.</h4><ul><li>inline image<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![alt text](url)</span><br></pre></td></tr></table></figure></li></ul><p><img src="/images/markdown.png" alt="markdown"></p><ul><li>reference image<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![alt text][markdown image]</span><br><span class="line"></span><br><span class="line">[markdown image]: /images/markdown.png</span><br></pre></td></tr></table></figure></li></ul><p><img src="/images/markdown.png" alt="markdown"></p><ul><li>이미지의 width와 height를 직접 지정해야 하는 상황이면 HTML의 img tag를 직접 사용한다.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src=&quot;/path&quot; width=&quot;500px&quot; height=&quot;300px&quot; /&gt;</span><br></pre></td></tr></table></figure><p><img src="/images/markdown.png" width="500px" height="300px"></p><h2 id="BlockQuotes"><a href="#BlockQuotes" class="headerlink" title="BlockQuotes"></a>BlockQuotes</h2><ul><li>BlockQuotes를 만들기 위해서는 앞에 caret(&gt;)을 넣어준다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; this is quotes.</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>this is quotes.</p><ul><li>BlockQuotes안에서도 다른 markdown 문법을 사용할 수 있다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; this is _italic_.</span><br></pre></td></tr></table></figure></li></ul></blockquote><blockquote><p>this is <em>italic</em>.</p></blockquote><h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><ul><li><p>unordered list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* list 1</span><br><span class="line">* list 2</span><br><span class="line">* list 3</span><br><span class="line">* list 4</span><br></pre></td></tr></table></figure></li><li><p>list 1</p></li><li>list 2</li><li>list 3</li><li>list 4</li></ul><hr><ul><li>ordered list<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. list 1</span><br><span class="line">2. list 2</span><br><span class="line">3. list 3</span><br><span class="line">4. list 4</span><br></pre></td></tr></table></figure></li></ul><ol><li>list 1</li><li>list 2</li><li>list 3</li><li>list 4</li></ol><hr><ul><li><p>nested list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">* parent 1</span><br><span class="line"> * child 1</span><br><span class="line"> * child 2</span><br><span class="line">* parent 2</span><br><span class="line"> * child 1</span><br><span class="line"> * child 2</span><br><span class="line"> * child 3</span><br></pre></td></tr></table></figure></li><li><p>parent 1</p><ul><li>child 1</li><li>child 2</li></ul></li><li>parent 2<ul><li>child 1</li><li>child 2</li><li>child 3</li><li>child’s child 1</li></ul></li></ul><hr><ul><li>nested list 에서도 indent를 맞추어 주고 싶을때는 다음과 같이 한다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. this is for indent.</span><br><span class="line">same indent.</span><br><span class="line"></span><br><span class="line"> same indent 2.</span><br><span class="line"></span><br><span class="line">2. this is for number 2.</span><br><span class="line"></span><br><span class="line"> same indent.</span><br><span class="line"> &gt; do blockquotes.</span><br></pre></td></tr></table></figure></li></ul><ol><li><p>this is for indent.same indent.</p><p>same indent 2.</p></li><li><p>this is for number 2.</p><p>same indent.</p><blockquote><p>do blockquotes.</p></blockquote></li></ol><h2 id="hr-수평선"><a href="#hr-수평선" class="headerlink" title="hr(수평선)"></a>hr(수평선)</h2><p>수평선은 다음과 같이 나타낸다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* * *</span><br><span class="line">***</span><br><span class="line">*****</span><br><span class="line">- - -</span><br></pre></td></tr></table></figure></p><hr><hr><hr><hr><h2 id="Paragraphs"><a href="#Paragraphs" class="headerlink" title="Paragraphs"></a>Paragraphs</h2><p>다음과 같은 문단을 작성한다고 해보자.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this is paragraph.</span><br><span class="line">this is sentence 1.</span><br><span class="line">this is sentence 2.</span><br></pre></td></tr></table></figure></p><p>하지만 이를 markdown으로 렌더링하면 줄바꿈이 되지 않고 다음과 같이 보인다.</p><p>this is paragraph.this is sentence 1.this is sentence 2.</p><p>줄바꿈을 하려면 여러가지 방법이 있는데 첫번째는 <em>hard break</em> 라고 불리는 방법을 사용하는 것이다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">this is paragraph.</span><br><span class="line"></span><br><span class="line">this is sentence 1.</span><br><span class="line"></span><br><span class="line">this is sentence 2.</span><br></pre></td></tr></table></figure></p><p>this is paragraph.</p><p>this is sentence 1.</p><p>this is sentence 2.</p><hr><p><em>soft break</em> 방식은 다음과 같다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this is paragraph.··</span><br><span class="line">this is sentence 1.··</span><br><span class="line">this is sentence 2.··</span><br></pre></td></tr></table></figure></p><p>(여기서의 ·은 space를 의미한다.)<br>즉, 줄의 마지막에 space 2개를 붙여주면 된다.</p><p>this is paragraph.<br>this is sentence 1.<br>this is sentence 2.  </p>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/tool/">tool</category>
      
      
      <category domain="https://tk-one.github.io/tags/markdown/">markdown</category>
      
      
      <comments>https://tk-one.github.io/2022/03/05/how-to-use-markdown/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>파일시스템 5편 - 파일시스템 디자인-2</title>
      <link>https://tk-one.github.io/2020/09/08/file-system-design/</link>
      <guid>https://tk-one.github.io/2020/09/08/file-system-design/</guid>
      <pubDate>Mon, 07 Sep 2020 16:51:46 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;이 글은 지난번 파일시스템 디자인-1 에 이은 글입니다.&lt;/p&gt;
&lt;h2 id=&quot;파일시스템-계층화&quot;&gt;&lt;a href=&quot;#파일시스템-계층화&quot; class=&quot;headerlink&quot; title=&quot;파일시스템 계층화&quot;&gt;&lt;/a&gt;파일시스템 계층화&lt;/h2&gt;&lt;p&gt;파일시</description>
        
      
      
      
      <content:encoded><![CDATA[<p>이 글은 지난번 파일시스템 디자인-1 에 이은 글입니다.</p><h2 id="파일시스템-계층화"><a href="#파일시스템-계층화" class="headerlink" title="파일시스템 계층화"></a>파일시스템 계층화</h2><p>파일시스템은 계층화가 잘되어있다. 일반적으로 파일시스템은 여러 개의 layer로 나뉘어져 구성된다.  </p><p align="center">    <img style="max-width:200px" alt="파일시스템 계층화" src="/images/filesystem/file-system-layering.png"/></p><p>위의 그림에서 각 layer를 간단하게 살펴보자.  </p><h4 id="Logical-file-system"><a href="#Logical-file-system" class="headerlink" title="Logical file system"></a>Logical file system</h4><p>여기서는 파일시스템의 메타데이터를 관리한다.  </p><h4 id="File-organization-module"><a href="#File-organization-module" class="headerlink" title="File-organization module"></a>File-organization module</h4><p>이 layer에서는 파일의 logical block 주소를 physical block 주소로 변환해준다. 예를들어 하드디스크를 매체로 사용하게되면 sector 주소로 접근해야 하는데 이를 위한 변환을 진행한다.  </p><h4 id="Basic-file-system"><a href="#Basic-file-system" class="headerlink" title="Basic file system"></a>Basic file system</h4><p>여기서는 위에서 변환된 physical block 주소를 가지고 읽고 쓰도록 command를 날린다. 여기서 DMA를 사용한다.  </p><h4 id="I-x2F-O-control"><a href="#I-x2F-O-control" class="headerlink" title="I&#x2F;O control"></a>I&#x2F;O control</h4><p>이는 device driver다. device driver가 하드웨어에 맞게 명령을 전달한다.  </p><br/><h2 id="Virtual-File-System"><a href="#Virtual-File-System" class="headerlink" title="Virtual File System"></a>Virtual File System</h2><p>파일시스템은 어쩔 수 없이 사용하는 매체에 의존성이 있다. 그래서 파일시스템은 종류가 여러가지가 있을 수 있다. 예를들어 요즘은 잘 안쓰지만 CD-ROM이 있을 수 있고 USB, SSD, Disk 그리고 파일이 현재 호스트가 아닌 다른 네트워크의 호스트에 있을 수도 있다.  Linux kernel 에서는 여러가지 파일시스템을 지원한다.<br>VFS는 다양한 logical file system 들을 추상화한다. 따라서 VFS를 통해 실제로 시스템에는 여러개의 다른 파일시스템이 사용되더라도 마치 1개의 파일시스템만 사용하는 것처럼 프로프래밍 할 수 있다. 이는 OOP의 개념과 같은데 특정 파일에 대해 read&#x2F;write syscall 이 호출되면 해당 파일이 속한 파일시스템 구현체의 read&#x2F;write 가 호출되는 방식이다.  </p><p align="center">    <img style="max-width:500px" alt="VFS" src="/images/filesystem/vfs.png"/></p><br/><h2 id="File-System-data-structure"><a href="#File-System-data-structure" class="headerlink" title="File System data structure"></a>File System data structure</h2><p>파일시스템에서 사용하는 자료구조를 알아보면서 파일시스템에 대한 이해를 높여보자.<br>On-disk 그리고 In-memory 에서 사용하는 자료구조들을 볼 것이다.  </p><h4 id="On-disk-data-structure"><a href="#On-disk-data-structure" class="headerlink" title="On-disk data structure"></a>On-disk data structure</h4><p>On-disk 자료구조들은 이미 이전 포스트인 파일시스템 디자인-1 에서 vsfs(very simple file system)에서 살펴봤던 내용들이 많다.  </p><ul><li><strong>Boot block</strong><br>첫번째 block으로 운영체제가 booting 하기위해 필요한 정보를 담아놓는 block이다. 하지만 이를 안만드는 경우도 많다.  </li><li><strong>Super block</strong><br>파일시스템 관련 정보들이 어디에 저장되어있는지에 대한 metadata를 저장한다. 예를들어 inode table은 어디서 시작인지, data block은 어디 block부터 시작하는지, root directory에 대한 inode 번호 등을 저장한다. Super block은 보통은 각 disk partition의 첫번째 block 에 할당하고 이 super block 이 손상되면 파일시스템의 복구가 힘들기때문에 보통 중복해서 super block을 저장한다. 그리고 이 super block은 in-memory data structure로 메모리에 올려 캐싱한다.  </li><li><strong>File control block</strong><br>FCB(file control block)은 결국 inode와 같다. inode 자체도 disk에 저장이 필요하다.</li></ul><p>다음의 disk structure layout를 살펴보자.  </p><p align="center">    <img style="max-width:300px" alt="On-disk data structure" src="/images/filesystem/disk-structure.png"/></p><p>여기서 각 inode table을 여러개로 나누어 저장한 이유는 data block과 inode가 가까우면 성능을 높일 수 있기때문에 조금씩 inode 들을 나누어 설계했다. 다만 이런 내용들은 전적으로 파일시스템 구현에 달라진다.<br>결국 여기서의 그림을 보면 파일 접근을 위해서 super block, inode, data block 에 대한 접근으로 파일 하나의 접근에 대해 3번의 disk access가 필요하다.  </p><br/><h4 id="In-memory-data-structure"><a href="#In-memory-data-structure" class="headerlink" title="In-memory data structure"></a>In-memory data structure</h4><p>보통 파일시스템에서의 in-memory data structure 들은 거의 caching을 통한 성능향상 그리고 파일시스템 관리가 목표이다.  </p><ul><li><strong>Dentry</strong><br>이는 directory cache 이다. directory 접근을 위해 on-disk 의 super block 그리고 root directory 의 inode 부터 접근해서 dentry 라는 directory cache를 만든다. dentry로 파일접근시 해당 파일의 inode를 알기위해 여러번 disk 에 접근할 필요가 없이 memory 에서 처리할 수 있다.  </li><li><strong>open file table</strong><br>이는 예전에 살펴본 내용으로 system-wide open file table 그리고 per process open file table 이 있는데 각각 open 한 file을 관리한다. per-process open file table 에서 각 index 번호가 file descriptor 가 된다.  </li><li><strong>Buffer cache</strong><br>최근에 사용한 data block 을 memory 에 캐싱한다.</li></ul><br/><h2 id="Buffer-Cache"><a href="#Buffer-Cache" class="headerlink" title="Buffer Cache"></a>Buffer Cache</h2><p>Buffer cache는 조금 더 자세히 살펴보겠다.<br>Buffer cache는 파일의 메타데이터가 아닌 data block 자체를 메모리에 올려둔다. 보통 같은 data block을 다시 접근해야 하는 경우가 많기때문에 이를 캐싱해두면 다시 disk를 조회하지 않아도된다.<br>다만 buffer cache는 완전한 software로 구현된다. 이 말은 virtual memory의 주소변환을 위해 TLB 같은 하드웨어를 도입하는게 아닌 순수한 software 레벨에서 구현한 캐시라는 의미이다.<br>Buffer cache는 보통 물리메모리의 1 ~ 10% 정도로 할당하고 이는 kernel parameter로 수정이 가능하다. Buffer cache 도 공간이 한정되므로 교체알고리즘을 사용한다. 보통 disk access 에는 locality가 나타나기 때문에 LRU 방식을 사용한다.<br>다만 DBMS 나 multimedia application은 LRU 로 이득을 볼 수 없는 경우가 대부분이기 때문에 이들은 buffer cache를 거치지 않고 바로 disk 에 접근할 수 있는 flag를 사용해서 buffer cache를 거치지 않도록 프로그래밍 한다.  </p><p>Read syscall 과 Write syscall 각각의 동작방식을 buffer cache 의 관점에서 살펴보자.  </p><h4 id="Read-syscall"><a href="#Read-syscall" class="headerlink" title="Read syscall"></a>Read syscall</h4><p align="center">    <img style="max-width:600px" alt="Buffer cache read" src="/images/filesystem/buffer-cache-read.png"/></p><p>먼저 <code>read(fd, buf, size)</code> syscall 을 호출한다. <code>read</code> syscall 의 두번째 인자인 buf 는 사용자의 buffer를 의미한다.<br>그다음 VFS에서는 file descriptor를 보고 file이 있는 device를 알아낸 후 logical block number 를 physical block number 로 변환한다. buffer cache 쪽에 해당 block이 있는지 확인을 하고 block 이 없다면 파일시스템에서 가져와야한다. 만약 buffer cache에 이미 cache된 block이 있으면 VFS에 반환한다.<br>cache된 block이 없다면 파일시스템에 block을 요청하고 이 <code>read</code> 를 요청한 프로세스는 sleep 한다. 즉, context switching 이 일어난다. I&#x2F;O가 필요하기 때문이다. 나중에 disk에서 block을 읽어오면 interrupt 가 발생하고 그 block 을 다시 caching 해주고 block 을 반환한다.<br>VFS에 반환할때는 application의 buf에 block들을 copy해주고 읽어온 byte 수를 반환한다.  </p><p>만약 <code>read</code> 를 하더라도 buffer cache 에 대상 block 이 존재하여 cache hit 이 된다면, 해당 프로세스는 sleep 하지 않는다.<br>그리고 buffer cache 쪽을 보게되면 hash table 구조로 구현되어 있고 value로는 linked list 로 각 cache 되어있는 data block 들이 있는 것을 확인할 수 있다. hash 의 key로는 보통은 block number 를 사용한다고 한다.</p><br/><h4 id="Write-syscall"><a href="#Write-syscall" class="headerlink" title="Write syscall"></a>Write syscall</h4><p align="center">    <img style="max-width:600px" alt="Buffer cache write" src="/images/filesystem/buffer-cache-write.png"/></p><p>먼저 <code>write(fd, buf, size)</code> syscall 을 호출한다.<br>VFS에서 device 및 block number로 변환하여 buffer cache에 해당 block이 이미 있는지 확인한다. 만약 write 하려는 block 이 buffer cache에 없다면 먼저 파일시스템에서 읽어오도록 요청한다. 이 과정에서도 <code>write</code>를 요청한 프로세스는 sleep 한다.<br>파일시스템에서 읽어왔으면 다시 write를 시도하면 해당 block이 buffer cache 에 존재하므로 이 경우에는 해당 buffer cache의 data block에 직접 write를 수행하고(in-memory) 해당 block이 disk에 있는 block과 일치하지 않는다는 표시를 하기위해 dirty bit를 체크한다.<br>만약 처음부터 buffer cache에 write 시도시 해당 block 이 buffer cache에 있었다면 바로 그 block에 write 하고 반환한다.  </p><p>이런 방식이면 buffer cache와 disk 의 sync가 깨지게 되는데 이는 다른 worker 스레드가 주기적으로 dirty bit가 체크된 data block 들을 disk에 sync 시켜준다.  </p><h4 id="kworker"><a href="#kworker" class="headerlink" title="kworker"></a>kworker</h4><p>이 kworker 라는 kernel thread가 buffer cache와 disk block 간의 동기화를 수행한다.<br>예를들어 data block은 30초, metadata는 5초마다 동기화를 시킬수도 있다. 다만 이런 구현은 실제 OS 구현마다 다르며 파일시스템 구현마다도 다르다.<br>다만 이런 방식이면 순간적으로 머신이 꺼지거나 할때 동기화가 되지 않은 부분은 유실될 수 있다. 즉 write를 하더라도 그 내용이 disk에 반영된다는 보장이 없다. 이를 위해 저널링 파일시스템 같은 대안을 사용한다.<br>Application 에서 <code>fsync</code> system call을 사용하면 강제적으로 buffer cache의 내용을 disk에 반영한다.<br>DBMS도 구현마다 다르지만 매번의 update 마다 fsync 를 수행하지는 않는다. DBMS에서는 이를 write ahead log 과 transaction을 이용해 해결한다.  </p><br/><h2 id="Memory-Mapped-File-mmap"><a href="#Memory-Mapped-File-mmap" class="headerlink" title="Memory-Mapped File(mmap)"></a>Memory-Mapped File(mmap)</h2><p>mmap은 파일을 프로세스의 address space 한 부분으로 mapping 한다. 파일을 프로세스의 memory space 에 그냥 가져다가 붙힌다고 생각하면 쉽다.<br>이렇게 되면 단순히 memory access instruction 을 통해 파일에 대한 read, write을 할 수 있다. kernel은 이런 memory access instruction 을 적절하게 read, write로 변환하여 수행해준다.<br>mmap 함수는 다음과 같다.  </p><p><code>void* mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset)</code></p><p><code>start</code>는 단지 이 주소를 사용했으면 좋겠다는 의미로 0을 보통 넣는다. 그리고 <code>offset</code> 부터의  <code>length</code> 만큼의 바이트를 <code>start</code> 주소로 매핑하기를 원한다는 의미이다.<br><code>fd</code>는 파일에 대한 file descriptor 로 이미 존재하는 파일에 대한 file descriptor 를 넘겨 이를 읽거나 수정할 수도 있고, 새로운 파일을 <code>O_CREAT</code> flag 와 함께 open 하여 이 file descriptor 를 넘겨 새로운 파일을 쓸 수도 있다.  </p><p>유저는 파일에 대한 I&#x2F;O 를 단지 <code>memset</code>, <code>memcpy</code> 같은 memory access 로 단순화 할 수 있다.<br>그리고 여러개의 프로세스에서 동일한 파일을 open 하여 사용할 경우 kernel 은 동일한 파일에 대한 내용을 memory에 1개만 들고있으면 된다.  </p><p>mmap 파일의 동작을 그림으로 보자.  </p><p align="center">    <img style="max-width:600px" alt="mmap 과정" src="/images/filesystem/mmap.png"/></p><p>먼저 mmap 을 사용하지 않는 경우를 생각해보면, disk 에서 DMA로 data block을 가져오면 이를 buffer cache에 올리고 그다음 사용자 buffer 로 copy를 해야한다.<br>buffer cache 로는 DMA 가 copy 해주지만, buffer cache 에서 user buffer 로는 CPU가 직접 copy 해야한다. 따라서 file IO 가 빨라지려면 CPU도 중요하다.  </p><p>mmap은 조금 다르다. mmap은 buffer cache를 사용하지 않는다. mmap은 buffer cache에 data block을 쓰지 않고 바로 kernel space로 DMA가 쓴다. 그리고 이를 프로세스의 address space 에 page table을 통해 매핑한다.<br>따라서 user buffer 로의 copy가 존재하지 않는다.<br>위의 그림에서는 process A 와 process B 가 각각 mmap 되어있는 physical fragment 를 공유하고 있다. mmap 에서는 flag에 <code>MAP_SHARED</code> flag 를 통해 다른 프로세스와 mmap 된 파일을 공유할 수 있다. 하지만 다른 동기화 같은 장치는 제공하지 않는다.<br>process A 에서 먼저 특정 파일을 mmap 하였을때 그 다음 process B 에서 같은 파일에 대해 mmap을 하게되면 서로 반환받는 virtual address 주소는 다르지만, 결국 page table 에 매핑된 physical address 주소는 같아 같은 파일을 바라보게 된다.  </p>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      <category domain="https://tk-one.github.io/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/">파일시스템</category>
      
      
      <comments>https://tk-one.github.io/2020/09/08/file-system-design/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>파일시스템 4편 - 파일시스템 디자인-1</title>
      <link>https://tk-one.github.io/2020/09/08/file-system-design-1/</link>
      <guid>https://tk-one.github.io/2020/09/08/file-system-design-1/</guid>
      <pubDate>Mon, 07 Sep 2020 15:51:46 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 System Programming 수업을 듣고 파일시스템 관련내용을 정리한 글입니다.  &lt;/p&gt;
&lt;p&gt;이번 편에서는 크게 다음 2가지를 볼 것이다.  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;File System의 디자인&lt;/li&gt;
&lt;li&gt;Directory의 디자인&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;unix 시스템에서 파일과 디렉토리는 비슷하면서도 다르게 구현이된다.&lt;br&gt;파일의 디자인부터 살펴보자.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 System Programming 수업을 듣고 파일시스템 관련내용을 정리한 글입니다.  </p><p>이번 편에서는 크게 다음 2가지를 볼 것이다.  </p><ol><li>File System의 디자인</li><li>Directory의 디자인</li></ol><p>unix 시스템에서 파일과 디렉토리는 비슷하면서도 다르게 구현이된다.<br>파일의 디자인부터 살펴보자.  </p><span id="more"></span><h2 id="File-Implementation"><a href="#File-Implementation" class="headerlink" title="File Implementation"></a>File Implementation</h2><p>File 구현의 핵심은 file의 내용을 담고있는 data block이 저장되는 디스크상의 위치정보를 결정하는 것과 이에 대한 관리방법이다.  </p><p>data block의 디스크에서의 위치정보를 결정하기 위해서는 여러가지 할당방법들이 존재하는데 실제로 사용되고 있는 몇가지 방법들을 알아보자.  </p><h4 id="Contiguous-Allocation"><a href="#Contiguous-Allocation" class="headerlink" title="Contiguous Allocation"></a>Contiguous Allocation</h4><p>Contiguous는 말그대로 연속적인, 연결되어있는 이런 의미이다.<br>Contiguous Allocation 방식은 파일을 물리적으로 연속된 disk block에 저장한다. 우리가 프로그래밍 할때 고정 길이의 배열을 할당하고 사용하는 것과 유사하다.<br>이 방식은 구현이 간단하며, 물리적으로 연속된 공간에 block이 위치하기 때문에 전체 파일을 한번에 읽어들일경우 성능상 매우 큰 이득을 볼 수 있다. 이처럼 파일의 맨 처음부터 끝까지 읽는 방식을 sequential read라고 한다. 이의 반대는 random read이다.<br>Contiguous Allocation은 random read도 빠르다. 파일에서 특정 block의 위치를 바로 알 수 있다. 연속된 block이므로 index로 block 주소를 바로 계산할 수 있기 때문이다.  </p><p align="center">    <img alt="Contiguous Allocation" src="/images/filesystem/contiguous-allocation.png"/></p>  <p>위 그림에서 block 14, 15, 16을 을 차지하고 있는 파일을 수정할때는 어떻게 해야할까?<br>Contiguous Allocation에서는 파일 수정시 수정한 block들을 어디에 위치할지 빈공간을 계속해서 찾아야 한다. 새로운 빈 연속된 block들을 찾으면 이에 복사하면서 massive copy가 발생한다.<br>위의 예시에서는 파일크기가 더 작도록 수정하기 때문에 반드시 다른 비어있는 다른 block들을 찾아야하는 것은 아니지만, 파일의 크기가 늘어나면서 수정을 하게되면 copy가 불가피하다.<br>이런 비용을 최소화 하기위해 파일끝에 예비용 block을 남기는 경우도 있지만 이는 disk 공간을 낭비한다.  </p><p>이런 방식은 많이 사용할 것 같지 않아보이지만 예상외로 많이 사용된다. 실제로 데이터베이스는 이런 방식으로 구현하는 경우가 많다. Disk block을 연속해서 여러개 할당하고 그 안에서 내부적으로 읽고 쓰고를 반복한다.<br>static한 파일들을 담고있는 서버에서도 파일에 대한 sequential read 성능이 매우 중요하기 때문에 사용하는 경우가 있고, 나중에 보게되겠지만 log structured file system에서 flash 같은 메모리는 이런 방식을 사용한다.<br>또한 구현이 간단하기에 임베디드에서 이를 채택하는 경우가 많다.  </p><h4 id="Linked-List-Allocation"><a href="#Linked-List-Allocation" class="headerlink" title="Linked List Allocation"></a>Linked List Allocation</h4><p>위에서 본 Contiguous Allocation 방식은 파일을 쓸때 능동적으로할 수 없다. block이 연속적이여야 하기 때문이다. 그래서 disk의 block을 linked list로 구현해서 file의 data를 저장하도록 하는 방식이 Linked List Allocation 방식이다.  </p><p align="center">    <img alt="Linked List Allocation" src="/images/filesystem/linkedlist-allocation.png"/></p>  <p>위 그림을 보게되면 파일을 읽을때 파일의 시작 block에서 출발해서 그 다음 block을 찾아가는 방식이다. data block에 다음 block의 주소를 같이 저장한다.  </p><p>이 방식은 file의 data block이 disk의 어디든지 위치할 수 있기때문에 유연하다. 또한 Contiguous allocation의 방법과 다르게 공간의 낭비가 없다.  </p><p>단점도 존재하는데 Random access를 하는데 성능이 좋지 못하다. File의 특정위치를 찾아가기 위해서는 해당 file의 시작 block부터 찾아가야 하기 때문이다. 이를 해결하기 위해 file의 metadata를 in-memory에 캐싱을 하기도 한다.<br>Data block에 다음 block 주소를 저장해야 하는 공간낭비 문제도 존재한다.  </p><p>실제로 MS-DOS의 FAT(File Allocation Table) File System은 Linked List Allocation 방식을 사용한다.  </p><p align="center">    <img alt="Allocation in FAT" src="/images/filesystem/msdos-fat.png"/></p><p>Random access가 느리기 때문에 더 나은 방법으로 File의 data block의 위치를 별도의 block에 모아둘 수 있다. 이렇게 File의 data block을 모아놓은 block을 index block이라고 하며 이 block을 읽어 파일의 모든 data block의 위치를 알 수 있다.<br>이 방식이 Linked List Allocation 보다 개선된 점은 random access 시 index block만 읽으면 찾아가고자 하는 data block의 위치를 알 수 있으므로 빠르게 random access가 가능하다.<br>index block이 몇개의 block 주소를 가질 수 있느냐가 이 파일시스템이 지원하는 최대 파일크기를 결정한다.  </p><p align="center">    <img alt="Linked List Allocation With Index" src="/images/filesystem/linkedlist-with-index.png"/></p>  <p>위 그림을 보면 19번 block에 index block을 위치하고 이 block에 data block을 순서대로 위치한다.  </p><h4 id="inode"><a href="#inode" class="headerlink" title="inode"></a>inode</h4><p>inode는 unix 파일시스템에서 사용하는 방식이고, windows 에서도 이와 비슷한 방식을 사용한다.<br>inode는 index node의 줄임말로 file의 metadata를 가지고있는 구조체이다. inode는 파일시스템 디자인에서 매우 중요한 역할을 맡는다. inode는 파일에 대한 data block index를 계층 형태로 관리하도록 한다. 메모리에서의 multi-level paging 방식과 비슷하다. 이러한 방식으로 inode의 data block 관리방식으로 구현을 하면 많은 flexibility를 얻을 수 있다.  .  </p><p>PCB에는 프로세스 관련한 정보가 다 들어가있듯이, inode에는 파일에 대한 대부분의 데이터가 전부 다 들어가있다.<br>inode의 구성요소는 대략적으로 다음과 같다.  </p><ul><li>File에 대한 속성을 나타내는 field들이 존재한다. file owner, timestamp, hard link counter, last access time, acl 정보 등이 있다.  </li><li>작은 크기의 파일을 위한 direct index가 존재한다.  </li><li>파일의 크기가 커짐에 따라서 요구되는 data block의 index들을 저장하기 위한 index table들이 존재한다. 이 index table을 가리키는 single indirect, double indirect, triple indirect 이 있다.</li></ul><p>inode의 대략적인 도식을 그림으로 보자.  </p><p align="center">    <img alt="inode 도식" src="/images/filesystem/inode.png"/></p><h4 id="Multi-Level-Index-in-inode"><a href="#Multi-Level-Index-in-inode" class="headerlink" title="Multi-Level Index in inode"></a>Multi-Level Index in inode</h4><p>direct block은 그 자체의 값들이 data block을 가리키는 포인터이다.<br>direct block이 8개가 있고 block size가 4KB라면 파일크기가 최대 32KB까지는 indirect block을 사용하지 않는다.<br>파일이 그 크기가 넘어가면, 즉 direct block에서 처리가능한 block 개수가 넘어가게되면 indirect block을 사용하기 시작한다.<br>single direct를 사용하기 시작한다. single indirect는 data block의 주소를 담고있는 index block을 가리킨다. index block은 data block을 가리킨다. single direct에서 처리가능한 크기가 넘어가게 되면 double indirect를 사용한다.<br>double indirect는 index block을 가리키는데 이 index block은 또다시 index block을 가리킨다. 즉 2-level page table과 동일하다.<br>triple indirect는 이 단계가 3단계로 이루어져 있다.  </p><p>inode에서 direct block, single indirect, double indirect, triple indirect가 각각 지원할 수 있는 최대 크기를 계산해보자.<br>Block size는 4KB, direct block 개수는 12개, block 주소는 4byte 라고 가정하자.  </p><ul><li>Direct blocks : 4KB * 12 &#x3D; 48KB</li><li>Single Indirect : 1024 * 4KB &#x3D; 4MB</li><li>Double Indirect : 1024 * 1024 * 4KB &#x3D; 4GB</li><li>Triple Indirect : 1024 * 1024 * 1024 * 4KB &#x3D; 4TB</li></ul><p>이는 block 주소 크기 등을 가정한 후의 계산값으로 실제 파일시스템이 지원하는 파일 최대크기와는 다름을 참고하자.  </p><p>왜 이런 방식을 사용할까?<br>inode 같은 방식을 사용하면 inode의 크기가 고정이 된다. inode는 파일당 1개씩 존재하며 시스템에서 파일의 개수만큼 존재한다. inode는 파일의 metadata 그 자체인데 이에 대한 크기가 파일의 크기와 관계없이 모두 동일하므로, 파일의 크기가 늘어나도 충분히 유연성을 확보할 수 있어 관리하기 편하다.  </p><h4 id="Extent"><a href="#Extent" class="headerlink" title="Extent"></a>Extent</h4><p>Linux의 ext2, ext3 파일시스템은 위와같이 Multi-Level index 방식을 사용하여 block mapping 체계를 제공한다. 다만 이 방식은 크기가 작은 파일에 대해서는 효율적이나 큰 파일에 대해서는 높은 오버헤드를 가진다. 구조상 파일의 크기가 조금 커지면 disk 에 반드시 fragment가 일어나게 되고 파일을 처음부터 연속적으로 읽어도 random access 가 많이 발생할 수 밖에 없다.<br>그리고 위의 block pointer 지정방식은 공간낭비도 심하다. 예를들어 single direct 방식에서 각 block 이 4, 5, 6, 7, 8 를 가리킨다면 각 주소가 4byte 일때 4 * 5 &#x3D; 20 byte 를 차지한다. 하지만 이는 연속된 block 일때 (4, 5) 라고 표현할 수 있다. (5개의 block 이 연속되어 있다는 의미)<br>물론 연속된 block 이 아니라면 매 block 마다 (4, 1), (5, 1) 처럼 될 것이다.<br>이처럼 매 block 마다 pointer를 적용하는 방식대신 포인터와 length를 명시함으로서 연속된 block을 명시하는 방식을 적용한 것이 <strong>extent</strong> 이다.<br>ext4 파일시스템은 이 extent 를 사용한다. 위의 inode 구조체에서 앞부분은 ext3의 inode와 동일하고 single indirect block 의 필드부터 indirect block 대신 extent 들을 가짐으로서 최대한 하위호환성을 유지했다.<br>이런 extent 기반의 방식은 disk 에 충분히 여유있는 공간이 있고 파일의 block 이 연속적으로 할당될 수 있을때 가장 높은 효율을 낼 수 있다.  </p><br/><h2 id="Directory-Implementation"><a href="#Directory-Implementation" class="headerlink" title="Directory Implementation"></a>Directory Implementation</h2><p>Directory는 파일들의 묶음을 나타내는데, 이는 굉장히 독특한 자료구조를 가진다.<br><strong>Directory entry</strong>라는 Directory를 표현하기 위한 자료구조가 존재한다. 파일시스템에 따라서 directory entry를 구성하는 필드도 달라진다. MS-DOS에서의 Directory entry와 Linux에서의 Directory entry를 살펴보자.  </p><h4 id="Directory-in-MS-DOS"><a href="#Directory-in-MS-DOS" class="headerlink" title="Directory in MS-DOS"></a>Directory in MS-DOS</h4><p>MS-DOS는 파일구현에 Linked List Allocation 방식을 사용한다. 여기서 사용하는 Directory entry 구조는 다음과 같다.  </p><p align="center">    <img alt="Directory entry in MS-DOS" src="/images/filesystem/directory-msdos.png"/></p><p>그림을 보면 첫 8byte는 file name을 가지고 다음 3byte는 extension을 가진다. extension은 따라서 3글자까지만 지원을 한다. 그리고 뒤에 first block number 부분이 있는데 이 값이 linked list의 첫번째 block을 가리킨다.<br>Linked List Allocation 방식을 사용하므로 맨 처음의 data block만 알면 전체 파일의 data을 알 수 있다.  </p><h4 id="Directory-in-Linux"><a href="#Directory-in-Linux" class="headerlink" title="Directory in Linux"></a>Directory in Linux</h4><p>Linux에서는 directory를 특별한 타입의 file로 간주한다.<br>그래서 디렉토리도 inode가 존재한다.<br>Linux에서의 directory entry는 file name과 inode로 이루어진다.<br>MS-DOS와는 다르게 directory의 metadata 정보는 inode 자료구조에 담겨있다.  </p><p align="center">    <img alt="Directory entry in Linux" src="/images/filesystem/directory-linux.png"/></p><p>디렉토리의 inode는 type에 regular file이 아닌 directory라는 타입으로 명시되어있고 inode의 data block에는 그 directory의 내용이 저장되어있다. 그렇기에 우리의 directory 구조는 영구히 저장될 수 있다.  </p><h4 id="Directory-lookup-in-Linux"><a href="#Directory-lookup-in-Linux" class="headerlink" title="Directory lookup in Linux"></a>Directory lookup in Linux</h4><p>리눅스에서 directory를 lookup 하는 과정을 살펴보자.<br><code>/usr/ast/mbox</code> path에 있는 mbox라는 파일에 접근한다고 가정하자. 이 과정을 그림을 표현하면 다음과 같다.  </p><p align="center">    <img alt="Directory lookup in Linux" src="/images/filesystem/directory-lookup.png"/></p><p>처음의 시작은 Root Directory에서 시작한다. 그림의 맨 왼쪽은 root directory의 inode의 data block의 내용을 그림으로 표현한 것이다. 물리 disk에 있는 block들은 파일의 내용도 가질 수 있으며 directory의 내용도 가질 수 있다. 여기서 <code>usr</code> directory를 찾고 그에 대한 inode로 가서 data block을 읽는다.<br><code>usr</code>는 directory 이므로 이에 대한 data block은 다시 directory entry들을 담고있다. 여기서 <code>ast</code> directory의 inode를 찾고 이에 대한 data block을 다시 읽는다. <code>ast</code>도 directory 이므로 data block은 또다시 directory entry를 담고있다.<br>최종적으로 <code>mbox</code>파일을 찾고 이 inode의 data block을 읽음으로서 파일을 읽게된다.  </p><p>이를 정리해보면 disk 접근횟수가 상당히 많다. 이를 개선하기위해 directory cache를 사용하며 directory cache는 hit rate가 99% 정도로 매우 높은 hit rate를 가진다. directory entry가 inode number + file name 이므로 크기가 크지않다.<br>directory cache는 시스템이 부팅할때 root directory부터 읽어 캐싱을 한다.  </p><br/>  <h2 id="Free-space-management"><a href="#Free-space-management" class="headerlink" title="Free space management"></a>Free space management</h2><p>파일시스템이 inode가 할당되어 있는지, data block이 할당되어 있는지 tracking을 해야한다. 그래야 새로운 파일을 생성할 때 적절한 space를 할당해줄 수 있다.<br>예를들면 파일을 만들면 inode를 할당한다. 이때 inode table을 보고 비어있는 위치에 inode를 할당한다. 예를들어 inode table이 bitmap으로 구현되어있다면 해당 순서의 bit를 1로 설정한다.<br>또 새로운 파일에 대한 data block 또한 할당을 해야한다. 이 data block 을 할당할때에도 여러가지 기법을 사용한다.<br>ext2나 ext3 파일시스템의 경우에는 사용하지 않으면서 연속된 약 8개의 block들의 묶음을 찾고 이를 할당함으로서 파일시스템이 파일의 부분이 contiguous하도록 보장해주어 성능향상을 꾀한다.<br>이러한 방법을 <strong>pre-allocation</strong>이라고 하며 자주 쓰이는 방법이다.  </p><br/><h2 id="Very-Simple-File-System"><a href="#Very-Simple-File-System" class="headerlink" title="Very Simple File System"></a>Very Simple File System</h2><p>파일시스템이 정확히 어떻게 구성되어있고 작동하는지를 알아보기 위해 예시로 아주 간단한 파일시스템을 보자.<br>이를 Very Simple File System 이라고 부르고 이하 vsfs 라고 부르겠다.<br>우리는 disk partition 이 있고 이 disk를 block 단위로 나눌 수 있다. block은 일반적인 크기인 4KB로 하겠다.<br>아래는 우리의 disk partition 에 대한 그림이다. 총 64개의 block 들로 나누어져 있고 각 block의 주소는 0부터 63까지이다.<br>disk partition 의 크기는 64 * 4 KB 인 아주 작은 partition 이다.  </p><p align="center">    <img style="max-width: 550px" src="/images/filesystem/vsfs-1.png"/></p><p>우리는 이 block 들에 무엇을 저장해야할까? 가장먼저 user data 가 떠오를 것이다. 실제로도 파일시스템의 대부분의 데이터는 user data 이다.<br>이런 user data 가 담기는 부분의 영역을 <strong>data region</strong> 이라고 부르자. 그리고 간단하게 64개의 block 중 56개를 data region 이라고 미리 정해두자.  </p><p align="center">    <img style="max-width: 550px" src="/images/filesystem/vsfs-2.png"/></p><p>위에서 보았듯이 파일시스템에서 각 파일을 추적하기위해 <strong>metadata</strong> 가 필요하다. 즉 inode 가 필요하다.<br>각 파일의 data block 의 위치가 어떻게 되는지, 파일의 크기는 어떻게되는지 등을 저장한다.<br>우리는 이런 inode 들 또한 disk 에 저장해야한다. 이들을 저장하기 위한 영역도 미리 정해두자. 64개의 block 중 5개를 잡고 이 영역을 inode table 이라고 하자.  </p><p align="center">    <img style="max-width: 550px" src="/images/filesystem/vsfs-3.png"/></p><p>inode의 크기는 보통 그렇게 크지는 않다. inode 한개가 256 byte 라고 가정하자. 그러면 4KB 크기인 block 1개에는 64개의 inode를 저장할 수 있다.<br>그러므로 우리의 inode table 은 5개의 block 으로 구성되어 있으므로 총 80개의 inode를 저장할 수 있겠다. 이는 우리가 설계한 vsfs 에서 가질 수 있는 최대 파일개수와 같다.<br>실제 파일시스템에서는 훨씬 더 큰 크기의 disk를 가지고, 단순하게 더 큰 크기의 inode table 영역을 잡을 수 있기때문에 훨씬 더 많은 파일을 가질 수 있다.  </p><p>우리는 inode table 영역을 할당하고 이를 <code>I</code> 로 표시하였고 date region 영역을 <code>D</code> 로 표시하였다. 하지만 궁극적으로 inode 나 data block 이 할당되어있는지를 추적할 수 있는 장치가 필요하다. 이런 할당여부에 대한 추적은 어떤 파일시스템이든 꼭 필요한 내용이다.<br>이런 할당여부를 추적하기 위해 여러가지 방법을 생각해볼 수 있지만 우리는 간단하게 bitmap 을 활용하여 표시할 것이다. 2개의 bitmap을 사용하여 한 개는 inode bitmap 으로 사용하고 다른 한개는 data bitmap 으로 사용한다.  </p><p align="center">    <img style="max-width: 550px" src="/images/filesystem/vsfs-4.png"/></p><p>각 bitmap 을 각각 한 개의 block 에 할당했다. block 한개는 4KB 이므로 약 3만 2천개의 bitmap을 저장할 수 있다. 우리는 80개의 inode 그리고 56개의 data block 을 가질 수 있기때문에 bitmap 을 과도하게 크게 설계했지만 간단한 예제이므로 그렇다고 하고 넘어가자.<br>그리고 0번째 block 이 남아있다. 이를 <strong>super block</strong> 으로 할당하자. super block 은 파일시스템의 정보를 포함한다.<br>파일시스템에 몇 개의 inode가 있는지 몇개의 data block을 사용하는지 inode table 은 몇번째 block 에서 시작하는지 등의 정보를 담고있다. </p><p align="center">    <img style="max-width: 550px" src="/images/filesystem/vsfs-5.png"/></p><p>그래서 파일시스템을 mount 하면 OS가 먼저 super block을 읽어서 어느부분 부터 읽어들여야 하는지 알아낸다.  </p><p>inode는 보통 inode 마다 할당되는 번호가 있다. 이 vsfs 에서는 총 80개의 inode를 담을 수 있으므로 0번부터 79번까지 번호가 존재한다.<br>따라서 파일시스템이 특정번호의 inode에 접근이 필요할때에면 inode의 번호로 바로 주소를 계산해서 접근할 수 있다.<br>보통 disk는 sector 단위로 접근이 가능하기 때문에 <code>(inode table 시작주소 + (N * sizeof(inode)) / 512</code> 로 나누면 sector 번호를 알아낼 수 있다.<br>inode table의 시작주소는 super block 을 보고 알아낼 수 있다.  </p><p align="center">    <img style="max-width: 600px" src="/images/filesystem/vsfs-6.png"/></p><p>파일을 새로 만들어야 할때에는 inode bitmap을 보고 비어있는 곳을 찾아 inode 를 새로 할당하고, data block 도 data bitmap 을 보고 비어있는 block 을 찾아 할당한다.<br>이 vsfs 예제를 보고 파일시스템 구현의 감을 잡으면 된다.  </p><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      <category domain="https://tk-one.github.io/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/">파일시스템</category>
      
      
      <comments>https://tk-one.github.io/2020/09/08/file-system-design-1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>파일시스템 3편 - 파일시스템이란?</title>
      <link>https://tk-one.github.io/2020/09/07/file-system-concept/</link>
      <guid>https://tk-one.github.io/2020/09/07/file-system-concept/</guid>
      <pubDate>Mon, 07 Sep 2020 07:14:20 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 System Programming 수업을 듣고 파일시스템 관련내용을 정리한 글입니다.  &lt;/p&gt;
&lt;h2 id=&quot;파일-File-이란&quot;&gt;&lt;a href=&quot;#파일-File-이란&quot; class=&quot;headerlink&quot; title=&quot;파일(File)이란?&quot;&gt;&lt;/a&gt;파일(File)이란?&lt;/h2&gt;&lt;p&gt;파일은 무엇일까?&lt;br&gt;파일은 &lt;strong&gt;linear array of bytes&lt;/strong&gt; 이다. 이게 무슨말일까?&lt;br&gt;이전 운영체제 메모리편에서 보았듯이 address space는 sparse 하다. 즉 프로세스가 사용하는 주소공간은 중간부분을 사용하지 않고 비어있을 수 있다. 그러므로 효율적으로 사용하고자 multi-level page table을 사용하곤 했었다.&lt;br&gt;이와 다르게 파일은 linear array of bytes로 중간이 비어있지 않다. 그리고 파일은 byte addressable하다. Byte 단위로 접근이 가능하다는 것이다. 그리고 여기서의 linear array는 밑에서 보겠지만 logical한 block이 linear하다는 것이다.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 System Programming 수업을 듣고 파일시스템 관련내용을 정리한 글입니다.  </p><h2 id="파일-File-이란"><a href="#파일-File-이란" class="headerlink" title="파일(File)이란?"></a>파일(File)이란?</h2><p>파일은 무엇일까?<br>파일은 <strong>linear array of bytes</strong> 이다. 이게 무슨말일까?<br>이전 운영체제 메모리편에서 보았듯이 address space는 sparse 하다. 즉 프로세스가 사용하는 주소공간은 중간부분을 사용하지 않고 비어있을 수 있다. 그러므로 효율적으로 사용하고자 multi-level page table을 사용하곤 했었다.<br>이와 다르게 파일은 linear array of bytes로 중간이 비어있지 않다. 그리고 파일은 byte addressable하다. Byte 단위로 접근이 가능하다는 것이다. 그리고 여기서의 linear array는 밑에서 보겠지만 logical한 block이 linear하다는 것이다.  </p><span id="more"></span><p>Address space와 파일을 표로 비교해보자.<br>만약 Address space를 잘 모른다면 <a href="/2019/08/08/os-memory/">운영체제 메모리편</a>을 참고하자.  </p><table><thead><tr><th>File</th><th>Address Space</th></tr></thead><tbody><tr><td>Variable length</td><td>Fixed length</td></tr><tr><td>Persistent</td><td>Volatile</td></tr><tr><td>연속적</td><td>비연속적</td></tr></tbody></table><p>Address space는 fixed space이다. 즉 주소공간이 48bit이라면 2<sup>48</sup>의 주소공간이 있다. 반면 파일은 variable한 길이를 가진다.<br>또 파일은 연속적이라고 표현하는데, 예를들어 파일은 이부분에 쓰고 이부분은 비워두고 하는 개념이 아니다. 하지만 Address space는 가능하다.  </p><p>그럼 파일로 Database를 구현할 수 있을까?<br>구현할 수 있다. 다만 Database에서 제공하는 여러 기능들도 동일하게 구현을 한다면 말이다.<br>하지만 운영체제는 general한 서비스를 구현하는 것이 목적이고, 데이터베이스는 말그대로 data에 대한 더 구체적인 목적을 가지고 있기때문에 이 둘은 지향점이 다르다.  </p><h2 id="파일시스템-File-System"><a href="#파일시스템-File-System" class="headerlink" title="파일시스템(File System)"></a>파일시스템(File System)</h2><p>파일시스템이란 무엇일까? 파일시스템은 순수한 software이다. memory처럼 성능향상을 위해 추가적인 hardware(MMU)를 도입하거나 하는 것없이 순수한 software로 작성되었다.<br>파일시스템은 크게 2가지 의미를 가지고 있다.<br>먼저 파일은 Abstraction이다. 즉, 사용자인 우리는 파일이 정확히 디스크 어디에 있는지 모른다.<br>하지만 파일을 구현하려면 디스크가 있어야한다. 파일을 write하는 것과 디스크에 저장되는 것과 연결이 되어있어야 구현할 수 있을것이다.  </p><p>이런 관점에서 파일시스템의 첫번째 의미로는 파일시스템은 <strong>physical storage의 종류에 관계없이 프로세스에게 read와 write을 할 수 있게끔 파일과 physical storage block간의 매핑을 제공</strong>해준다.  </p><h6 id="File-blocks를-disk-blocks로-매핑예시"><a href="#File-blocks를-disk-blocks로-매핑예시" class="headerlink" title="File blocks를 disk blocks로 매핑예시"></a>File blocks를 disk blocks로 매핑예시</h6><table><thead><tr><th>File block number</th><th>Disk block number</th></tr></thead><tbody><tr><td>1</td><td>108</td></tr><tr><td>2</td><td>3010</td></tr><tr><td>3</td><td>3011</td></tr></tbody></table><p>이렇기에 사용자는 파일을 쓸때 이 파일이 실제 어디에 저장되는지 고민을 할 필요가 없다. 그 마법을 파일시스템이 부린다.  </p><h4 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h4><p>파일은 block 단위로 자를 수 있다. 이 block은 대부분 page 크기에 맞춘다. 즉 4KB에 맞춘다.<br>만약 1MB 짜리 파일은 256개의 block으로 나뉜다.<br>왜 block size를 page size로 맞출까?<br>왜냐하면 파일을 읽거나 할때 block을 들고와서 memory에 써주어야 하기 때문이다. 다만 어떤 파일시스템은 4MB 파일 block을 가지는 파일시스템도 존재한다. 대신 block을 메모리에 써줄때에는 contiguous한 4MB page 묶음을 확보하기 쉬지않다.  </p><p>다시 파일시스템으로 돌아가서 파일시스템의 두번째 의미를 살펴보자.<br>파일시스템은 <strong>디스크에 들어가있는 파일 전체를 총칭</strong>한다. 즉, 파일시스템이 파일들의 집합이라는 의미이다. 파일시스템에 따라서 파일들의 배치나 구성이 달라진다. 파일시스템은 Windows는 NTFS를 사용하고 Linux는 ext4 등을 사용한다.  </p><p>즉 파일시스템의 첫번째 의미로는 software로서의 의미에 가까운 반면 두번째 의미로는 파일자체의 집합을 의미한다.  </p><p>파일시스템을 그림으로 나타내면 다음과 같다.  </p><p align="center">    <img alt="File System overview" src="/images/filesystem/filesystem-overview.png"/></p><p>위에서의 Buffer cache 라는 것은 다음편에서 보겠지만 디스크에서 읽어온 내용을 메모리에 저장해둔 것이다. 그래서 파일을 읽는다는 것은 Buffer cache를 읽는 것이다. Buffer cache는 Kernel 영역 메모리에 위치하는데 파일시스템이 Kernel mode로 돌아가기 때문이다. 이정도로만 우선 알고있자.  </p><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><p>metadata는 간단하게 파일에 대한 data이다. 위에서 본 파일시스템의 block 매핑정보 자체도 metadata이다. metadata를 살펴보는 이유는 파일시스템이 하는 일이 metadata를 관리하는 것이기 때문이다. 파일시스템에 따라서 파일들의 metadata 구성이 달라진다.<br>파일의 metadata에는 파일의 이름, 타입, 파일크기, storage의 block 위치, protection 관련정보들이 존재한다. 이 외에도 시간관련 data, inode(index node), directory 정보 등이 존재한다.  </p><p>inode라는 것은 나중에 더 자세히 보겠지만 위에서 본 매핑정보를 가지고 있는 파일시스템에서의 핵심적인 data structure이다. 파일마다 inode가 한개씩 존재하며 metadata에 파일 개수만큼의 inode가 존재한다.  </p><p>directory 혹은 폴더 라는 개념은 결국 metadata들을 모아놓은 개념이라고 볼 수 있겠다.  </p><h4 id="Metadata-and-caching"><a href="#Metadata-and-caching" class="headerlink" title="Metadata and caching"></a>Metadata and caching</h4><p>Metadata는 어디에 저장이 될까? metadata는 디스크에 저장이 되어 persistence가 제공된다.<br>우리는 파일에 접근할때 먼저 metadata를 접근하고 그 metadata에서 data 위치를 읽어서 다시 그 data에 접근해야 한다. 즉 최소한 metadata에 한번은 접근을 해야한다는 것이다.<br>이는 마치 메모리에서 메모리에 접근하기 위해 page table을 먼저 읽고 그 다음 적절한 physical memory address로 변환하여 다시 메모리에 접근하는 문제와 같다. 즉 메모리에 접근하기 위해 메모리를 2번 접근하는 문제이다. 이를 극복하기 위해 TLB를 사용하여 page mapping을 캐싱한다.<br>이와 비슷한 이유로 metadata도 캐싱한다. metadata의 캐싱은 위에서 살짝 살펴보았던 Buffer cache와는 다르다. 말그대로 metadata 캐싱은 metadata를 위한 캐싱이다.<br>그래서 운영체제는 부팅할때 맨처음 metadata들을 읽은다음 이들을 미리 캐싱한다.  </p><p>metadata 캐싱은 asynchronous update를 사용한다. metadata 변경이 일어날때 cache를 업데이트하고 나중에 disk에 저장한다. 즉 metadata 수정이 메모리에서 수정된다는 의미이다. 그러면 inconsistency를 어떻게 처리할 수 있을까? 만약 metadata 수정이 되었지만 이 내용이 disk에 반영되지 못하고 컴퓨터가 종료되면 어떻게될까?<br>일단 처리하지 못한다. 그래서 OS에서 파일시스템에서 failure semantics가 보장되지 않는다고 한다. 물론 찾아보면 Journaling file system 같은 failure semantics를 보장해줄 수 있는 파일시스템이 존재한다. 혹은 checkpoint 같은 값을 써서 추가적인 failure semantics를 보장하기도 한다.<br>metadata caching 말고도 Directory cache 라는 것도 존재한다. 이는 directory에 대한 cache로 hit율이 99% 이상으로 매우 높은편이다.  </p><h4 id="Journaling-File-System"><a href="#Journaling-File-System" class="headerlink" title="Journaling File System"></a>Journaling File System</h4><p>저널링 파일시스템이라고 부른다.<br>이는 file system에 변경사항을 반영하기 전에, journal 이라는 circular log의 변경사항을 보고 이를 기반으로 추적하는 파일시스템이다.<br>이를 활용하면 시스템 충돌이 발생했을때 빠르게 복구하면서 data inconsistency를 방지할 수 있다.<br>현재 리눅스에서 채택하고있는 ext4 파일시스템은 저널링 파일시스템이다. 파일시스템에서 보통 128MB 정도의 연속된 block을 미리 예약하고 이 공간을 Journal로 사용한다. 즉 중요한 데이터들을 이 공간에 빠르게 write한다.  ext4 파일시스템은 여러가지 모드로 설정할 수 있는데 기본적인 방식은 </p><h4 id="포렌식"><a href="#포렌식" class="headerlink" title="포렌식"></a>포렌식</h4><p>포렌식은 지워진 파일을 복구한다. 어떻게 이것이 가능할까?<br>파일을 지울때 실제 파일의 데이터를 지우는 것이 아닌 해당 파일에 해당하는 metadata를 지운다. 그래서 파일삭제는 파일크기에 관계없이 보통 빠르게 진행된다.<br>결국 실제 파일의 데이터는 디스크 어딘가에 남아있는데 이를 가르키고 있는 metadata들이 전부 삭제된 상황이다. 그래서 포렌식은 파일시스템의 매핑방식과 disk 데이터를 적절하게 보고 실제 데이터들을 찾아낸다.  </p><h2 id="File-System-layout"><a href="#File-System-layout" class="headerlink" title="File System layout"></a>File System layout</h2><p>파일시스템의 레이아웃을 보면 다음과 같다.  </p><p align="center">    <img style="max-width:600px" alt="File System layout" src="/images/filesystem/filesystem-layout.png"/></p><p>왼쪽 그림을 보면 하나의 디스크에 partion을 나누어서 partition마다 다른 파일시스템을 올릴 수 있다. 또 오른쪽 그림을 보면 여러 디스크를 묶어서 하나의 partition을 만들 수도 있다. 이는 디스크 경계를 넘어서는 파일을 만들 수 있다는 뜻과도 같다.  </p><p>또 그림에는 metadata가 전부 맨 앞에있는데 이는 logical한 그림임을 참고하면 좋겠다. 나중에 조금 더 자세한 layout을 살펴보게 될 것이다.  </p><h2 id="File-Operations"><a href="#File-Operations" class="headerlink" title="File Operations"></a>File Operations</h2><p>File을 읽을때 우리는 어떤 과정을 거칠까? 먼저 file을 open하고 그 다음 read or write을 하고 이를 close 해주어야 한다. 여기서의 open, read, close는 모두 시스템콜이다.  </p><h4 id="open-2"><a href="#open-2" class="headerlink" title="open(2)"></a>open(2)</h4><p><code>int open(const char *pathname, int flags);</code>  </p><p>open은 파일의 path를 인자로 받아 파일을 open한다.<br>open을 호출하면 file descriptor를 반환받는다.<br>file descriptor는 음수가 아닌 숫자로 handle 이라고도 부른다. handle은 자동차의 핸들과 같은 의미인데, 자동차의 핸들은 바퀴를 돌려주는 것처럼 우리는 이 handle을 이용해 파일을 조작할 수 있다.  </p><h4 id="read-2"><a href="#read-2" class="headerlink" title="read(2)"></a>read(2)</h4><p><code>ssize_t read(int fd, void *buf, size_t count);</code><br>read는 file descriptor를 인자로 받는다. 그리고 읽을 byte 수인 count를 인자로 받고 결과값을 buf에 넣어준다. </p><h4 id="close-2"><a href="#close-2" class="headerlink" title="close(2)"></a>close(2)</h4><p><code>int close(int fd)</code><br>file descriptor를 닫는다. </p><p>read, write는 어떤 파일을 읽겠다는 것을 나타내는 file descriptor를 인자로 받는다.  나중에 네트워크쪽에서도 socket 이라는 것이 있는데 socket의 생성에도 file descriptor를 반환받는다. 이를 가지고 send, recv를 하게된다.  </p><p>우리는 read, write로 파일을 읽거나 쓸때 정확히 어디를 읽고 어디를 쓰겠다라는 위치를 전달하지 않는다. 그럼에도 파일을 계속해서 읽고 쓸수있다. 이것이 어떻게 가능할까?<br>리눅스는 내부적으로 current-file-position 포인터를 프로세스마다 들고있다. 그래서 프로세스가 파일을 읽거나 쓸때 해당 프로세스가 어디까지 작업했는지 이 포인터에 기록한다.  </p><p>이 current-file-position을 변경하기 위해 seek 이라는 시스템콜을 제공한다.<br>여기서 알 수 있듯이 파일은 메모리랑은 다르다. 메모리는 seek이 필요없고 random access가 가능하다.  하지만 파일은 linear 하게만 왔다갔다 할 수 있으며 random access가 불가능하다.  </p><h4 id="open-close의-역할"><a href="#open-close의-역할" class="headerlink" title="open, close의 역할"></a>open, close의 역할</h4><p>File은 프로세스가 공유할 수 있다. 즉 파일 A가 있을때 프로세스1도 A를 읽을 수 있고, 프로세스2도 A를 읽을 수 있다. 이를 지원하기 위해서는 이 파일 A가 어떤 프로세스에게 읽히고 있다는 것을 내부적으로 저장해야한다. 그렇지 않으면 프로세스3이 파일 A를 그냥 delete 해버릴 수도 있다.<br>이를 가능하게 한게 open이다. open의 대략적인 역할을 살펴보자.<br>먼저 파일을 읽기위해 open을 하게되면 내부적으로 파일이 실제로 존재하는 것인지 탐색한다. 만약 존재한다면 해당 파일의 metadata를 메모리에 캐싱한다.<br>그 다음으로 open을 하게되면 내부적으로 open-file table이라는것이 만들어진다. open-file table은 두가지 형태로 존재하는데 하나는 per-process open-file table이고 또다른 하나는 system-wide open-file table이다.<br>이름으로 유추할 수 있듯이 per-process open-file table은 프로세스별로 존재하는 open-file table이고, system-wide open-file table은 시스템의 모든 open된 파일들에 대한 open-file table이다.  </p><p>per-process open-file table에는 각 프로세스에서 유지하는 state를 저장한다. 예를들어 프로세스가 파일을 읽을때 내부적으로 파일을 어디를 읽고있는지를 나타내는 current-file-position 포인터를 저장한다.  </p><p>system-wide open-file table에는 file open count, file access date, file location 등을 저장한다.  open을 하면 file open count를 1 증가시키고, close를 하면 file open count를 1 감소시킨다.<br>delete는 system-wide open-file table에서 file open count가 0인지 확인을 한 후 이루어진다. 누군가 이를 open 하고있다면 delete 할 수 없다.  </p><p>다음 그림을 보자.  </p><p align="center">    <img alt="Open file table" src="/images/filesystem/open-file-table.png"/></p><p>open은 per-process open-file table에서의 index 주소를 반환한다. 이것이 file descriptor 이다.<br>그리고 per-process open-file table에서의 값은 다시 그 파일의 system-wide open-file table entry를 가리킨다. 그리고 system-wide open-file table은 다시 그 파일의 metadata를 가리킨다.<br>이 metadata는 open할때에 메모리에 캐싱하게되며 flush daemon에 의해 disk에는 asynchronous하게 update된다.  </p><p>만약 프로세스가 파일에 대해 close 하는 것을 까먹고 exit 했으면 어떻게 될까? system-wide open-file table에서의 file open count가 줄지 않았으므로 영원히 파일삭제는 못하는 것일까? 그렇지 않다.<br>실제로 close를 하지 않아도 file open count는 정상적으로 count 된다.<br>PCB(Process Control Block)에 해당 프로세스가 어떤 파일을 open 하였는지 이미 다 기록을 하고있기 때문에 close를 하지않고 exit하여도 이 내용을 보고 close count를 유지한다.  </p><p>리눅스에서 file descriptor 0, 1, 2는 미리 정해져 있다. 0은 표준입력, 1은 표준출력, 2는 표준에러이다.  </p><h2 id="Directory"><a href="#Directory" class="headerlink" title="Directory"></a>Directory</h2><p>Directory는 파일을 나누어 저장하는 방법이다.<br>초창기에는 Single-level directory를 사용했다. 이는 한 사용자에 대해서 단일 directory만 생성하는 방식으로 하부 directory를 만들지 못한다.  </p><p align="center">    <img alt="Single Level directory" src="/images/filesystem/single-level-directory.png"/></p><p>이 글을 쓸때만 해도 안드로이드, iOS는 single-level directory였다. 이렇게 설계한 이유는 단순하기 때문이다.  </p><p>나중에는 우리가 흔히 알고 사용하고 있는 tree structure directory를 만들었다. 이는 sub-directory를 생성할 수 있다.<br>다만 tree structure에는 root가 존재해야하며, acyclic graph 이여야 한다. </p><p align="center">    <img alt="Tree 구조 directory" src="/images/filesystem/tree-structure-directory.png"/></p>  <h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><p>File이나 directory의 공유를 허용하지 않으므로 이를 위해 두가지 종류의 link를 제공한다.<br>soft link, hard link이다.  </p><h4 id="Soft-Link"><a href="#Soft-Link" class="headerlink" title="Soft Link"></a>Soft Link</h4><p>soft link는 symbolic name의 개념이다. 삭제시 link만 삭제된다. 리눅스에 흔히 사용하는 symlink와 동일하다. 윈도우에서는 shortcut(바로가기)가 있다. 우리가 윈도우에서도 shortcut을 삭제하면 shortcut이 삭제되지 실제 파일이 삭제되지는 않는다.   </p><h4 id="Hard-Link"><a href="#Hard-Link" class="headerlink" title="Hard Link"></a>Hard Link</h4><p>실제 파일을 가리키는 alias이다. 삭제시 이 파일을 보고있는 process가 없다면 삭제한다.  </p><p>그림을 보고 이해하면 쉽게 이해할 수 있다.  </p><p align="center">    <img alt="Soft Link and Hard Link" src="/images/filesystem/soft-link-hard-link.png"/></p><p>각 원본파일들에 대한 inode는 항상 존재한다. inode는 이 파일의 metadata를 의미한다.<br>내가 만약 파일을 복사(cp)하였다면 데이터도 복사되고 이를 가르키는 새로운 inode도 생성된다. 실제로 파일을 복사하는 개념이다.  </p><p>hard link를 보자.<br>hard link는 원본파일의 inode를 그냥 가리킨다. 그래서 삭제시 실제 파일이 지워지는 개념이다.<br>이와 다르게 soft link는 inode를 새로 만든다. 다만 이 inode가 가르키는 데이터에는 원래 파일의 경로가 들어가있다. 그래서 이 원래파일의 경로를 보고 다시 파일에 접근한다. 위의 예제에서는 soft link의 파일을 읽을때에는 결국 <code>inode #1</code>을 통해 원본을 읽게된다.  </p><p>그러므로 soft link 삭제시 본인의 inode를 삭제하므로 원본파일은 삭제되지 않는다.  </p><h2 id="Mount"><a href="#Mount" class="headerlink" title="Mount"></a>Mount</h2><p>Mount는 directory와 disk를 분리하는 개념이다.<br>Mount는 <strong>비어있는 directory</strong>에 임의의 device를 붙일 수 있다. 비어있는 directory라는 것을 강조한 이유는 다른 device가 mount 되어있지 않아있어야 한다는 것이다. 만약 이미 mount되어있다면 기존의 것을 unmount 해야한다.<br>파일시스템을 사용하려면 mount를 해야한다.<br>그림을 보고 이해해보자.  </p><p align="center">    <img alt="Mount disk 예제" src="/images/filesystem/mount-disk.png"/></p><p>이와같이 두개의 disk가 있고 각각 파일시스템의 directory들이 이와 같다고 해보자.<br>disk2를 disk1의 비어있는 <code>/usr/</code> directory에 mount를 해보자.<br><code>$ mount disk2 /usr/</code>  </p><p align="center">    <img alt="Mount 결과" src="/images/filesystem/mount-result.png"/></p><p>이처럼 mount를 하면 <code>/usr/bill</code>로 접근이 가능하다. 사용자는 disk1, disk2를 알 필요가 없이 그저 path로 접근하면 된다. 사용자는 본인이 어떤 device로 접근하고 있는지 알필요가 없다. 그러므로 mount는 scaleability가 좋고 device가 바뀌어도 사용자는 영향받지 않으므로 distributed File System으로 확장이 매우 용이하다. NFS(Network File System)을 사용하여 mount를 하게되면 다른 머신에 있는 file system을 local에서 나의 local에서 사용하는 것처럼 접근할 수 있다.<br>리눅스는 시스템이 부팅할때 mount를 통해 device tree를 결정한다.  </p><p>Windows를 보면 <code>c:</code> 드라이브가 존재한다. 이는 <code>c:</code>의 device를 분리하지않고 미리 directory를 고정시키고 미리 정해놓은 것이다. 또 다른 device를 추가하면 <code>d:</code>, <code>e:</code> 로 추가된다. 이는 일반 사용자의 편의성을 고려한 방식으로 우리가 살펴보았던 directory와 disk를 분리하는 mount 방식이랑 다르다.  </p><h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h2><p>namespace는 그냥 unique하고 식별가능한 이름이 있는 공간이라고 생각하면 된다.<br>위에서 본 mount는 namespace를 define한다. 프로세스들의 file namespace는 동일하게 mount 된 곳을 가진다면 서로 동일한 namespace를 보게 된다.<br><code>chroot</code>를 사용하면 프로세스별로 격리된 file namespace를 만들 수 있다. 즉 프로세스마다 서로 다른 루트를 가질 수 있도록 설정할 수 있다. 이는 컨테이너 기술에서 핵심적인 부분을 맡고있다.  </p><h2 id="Protection"><a href="#Protection" class="headerlink" title="Protection"></a>Protection</h2><p>이전에 보았던 운영체제 메모리편에서 프로세스는 protection domain을 가진다고 했다. 그리고 일반 프로세스가 kernel에 접근하려고 하면 접근을 할 수 없다. page table entry에 read, write에 대한 권한을 설정하며 권한관리를 하고있다.<br>하지만 파일은 이와는 다른 access control에 대한 정책을 가지고 있다.<br>어떤 방식을 채택하고 있는지 바로 말하기 전에 어떻게하면 파일에 대한 access control을 구현할 수 있을까 생각해보자.  </p><p>protection을 어느 것을 주체로 할지부터 정해야한다.<br>예를들어 프로세스를 기준으로 protection을 할 수도 있을 것이다. 예를들어 프로세스 A는 파일 1번과 2번에 접근할 수 있도록 access control을 설정하도록 설계할 수 있겠다. 실제로 이런 방식의 file protection을 구현한 운영체제도 존재한다.  </p><p>접근의 종류는 어떤 것으로 정해야할까? read, write 등의 권한종류가 있고 프로세스가 특정 파일에 대해 write 권한을 가진다면 이 파일을 삭제할 수 있도록 해야할까?  </p><p>이는 구현하기 나름이며 각각 장단점이 있다.<br>우리는 unix 계열의 File Protection을 살펴보도록 하자.  </p><p>Linux에서는 프로세스가 아닌 파일을 주체로하여 사용자에게 파일권한을 부여한다고 정했다.<br>그리고 access type을 다음과 같이 3가지로 정의했다.  </p><ul><li>Read</li><li>Write</li><li>Execute</li></ul><p>그리고 파일의 사용자들을 세가지로 분류하였다.  </p><ol><li>owner : 파일을 만든 사람</li><li>group : 사용자들의 집합</li><li>public : 이외의 모든 사용자</li></ol><p>그리고 access model을 위의 세가지 분류를 활용해 bit level로 protection을 정의한다. 이것이 무슨말인지 예제로 알아보자.  </p><p>각 사용자들의 분류마다 RWX(<strong>R</strong>ead, <strong>W</strong>rite, e<strong>X</strong>ecute) 순서의 bit를 정의하고 각 bit가 설정이 되어있으면 그에 대한 권한이 있다고 표현한다.<br>RWX가 7이면 R(4) + W(2) + X(1) 이므로 모든 bit가 설정되어있으므로 읽기, 쓰기, 실행권한이 모두 있는 것이다.<br>RWX가 6이라면 R(4) + W(2) 이므로 실행권한은 없으며 읽기, 쓰기 권한은 있는 것이다.<br>이에 대한 RWX를 owner, group, public 순으로 나열한다.<br>즉 파일에 대해 protection이 761로 설정이 되어있다면 owner(7), group(6), public(1) 로 설정되어 있다는 의미이다.<br>프로세스를 실행한 사용자가 파일의 owner라면 7이므로 이 프로세스는 read, write, execute를 할 수 있다는 의미이다.<br>만약 owner는 아니지만 파일에 설정된 group에 속한 사용자라면 6이므로 읽고 쓸수는 있지만 파일을 실행할 수는 없다.<br>만약 이 group에도 속하지 못한 사용자라면 public으로 파일을 실행만 가능하며 읽고 쓸수는 없다.  </p><p>보통 text file은 실행을 할 수 없으므로 666으로 많이 설정하기도 한다.<br>download 한 파일들은 자동으로 보안을 위해 파일실행을 할 수 없도록 파일권한을 설정하기도 한다.<br>그리고 1은 자주사용할 것 같지 않아보이지만 실제로는 kernel이 관리하는 시스템 파일들은 execute만 가능하도록 1로 설정하도록 한다.  </p><p>그리고 unix 계열에서는 파일에 대해 write 권한이 있으면 파일을 삭제할 수 있다.  </p><p>파일에 대한 protection mode는 <code>chmod</code> 명령어를 사용하여 변경할 수 있다.<br><code>chmod 761 filename</code> 을 수행하면 해당 파일의 mode가 변경되며 이 명령은 파일의 owner 혹은 root만 수행할 수 있다.<br>파일의 owner 또한 <code>chown</code> 명령어로 변경할 수 있으며, 위의 group 또한 변경이 가능하다.<br>group은 <code>/etc/group</code>에서 정의되고 파일의 group도 <code>chgrp</code> 명령어로 변경할 수 있다.</p><p>파일은 owner와 group 이 각각 1개씩만 존재할 수 있다.  </p><p>다음편에서는 파일시스템이 어떤 방식으로 구현이 되어있는지 살펴보도록 하자.  </p><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      <category domain="https://tk-one.github.io/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/">파일시스템</category>
      
      
      <comments>https://tk-one.github.io/2020/09/07/file-system-concept/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>파일시스템 2편 - Flash 그리고 SSD</title>
      <link>https://tk-one.github.io/2020/09/06/file-system-flash/</link>
      <guid>https://tk-one.github.io/2020/09/06/file-system-flash/</guid>
      <pubDate>Sat, 05 Sep 2020 16:51:46 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.&lt;br&gt;이전 포스트의 하드디스크에 이어 이번 포스트에서는 Flash 그리고 SSD에 대해 알아볼 것입니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 이전의 &lt;a href=&quot;/2019/07/09/os-computer-architecture/&quot;&gt;운영체제 3편 - 컴퓨터 구조와 I&amp;#x2F;O&lt;/a&gt; 글의 I&amp;#x2F;O 부분을 이해하고보면 도움이 됩니다.  &lt;/p&gt;
&lt;br/&gt;

&lt;h2 id=&quot;Flash-Memory&quot;&gt;&lt;a href=&quot;#Flash-Memory&quot; class=&quot;headerlink&quot; title=&quot;Flash Memory&quot;&gt;&lt;/a&gt;Flash Memory&lt;/h2&gt;&lt;p&gt;하드디스크와 다르게 Flash는 메모리나 CPU처럼 트랜지스터 들로 구성이 되어있다. 즉 disk의 arm이 움직이거나 platter가 회전하는 물리적인 움직임이 없다.&lt;br&gt;Reliability 측면에서도 disk에 비해 훌륭한 편이다. Disk는 물리적인 head crash가 날수도 있고, dead block(더이상 사용할 수 없는 block) 자체가 생길 수 있다. 물리적으로 읽고 쓰기 때문이다. 그리고 disk는 생각보다 자주깨진다.&lt;br&gt;이와는 다르게 Flash는 순수한 silicon으로 구성되어 있으며 전자적으로 동작하기에 더 신뢰성이 높다. Flash는 또한 매우 빠른 access time을 제공하고 power가 적게드는 반도체 특성으로 disk에 비해 매우매우 저전력이다. Flash Memory는 여러타입이 있는데 보통은 NAND-Flash를 의미한다. 
다만 Flash가 가진 특이한 특성들 때문에 이를 해결하기 위해 몇가지 기법들을 적용해야한다. 이들은 밑에서 자세히 알아볼 것이다.   &lt;/p&gt;
&lt;p&gt;Flash chip은 하나의 transistor의 1개 이상의 bit를 저장할 수 있다. &lt;strong&gt;SLC(Single-Level Cell)&lt;/strong&gt; flash는 오직 1개의 bit만 transistor에 저장할 수 있고, &lt;strong&gt;MLC(Multi-Level Cell)&lt;/strong&gt; flash는 2개의 bit를 저장할 수 있다. 그러므로 00, 01, 10, 11을 저장할 수 있다. &lt;strong&gt;TLC(Triple-Level Cell)&lt;/strong&gt; flash는 3개의 bit를 저장가능하다. 전반적으로 SLC가 더 성능이 좋고 가격이 비싸다.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.<br>이전 포스트의 하드디스크에 이어 이번 포스트에서는 Flash 그리고 SSD에 대해 알아볼 것입니다.  </p><p>이번 편은 이전의 <a href="/2019/07/09/os-computer-architecture/">운영체제 3편 - 컴퓨터 구조와 I&#x2F;O</a> 글의 I&#x2F;O 부분을 이해하고보면 도움이 됩니다.  </p><br/><h2 id="Flash-Memory"><a href="#Flash-Memory" class="headerlink" title="Flash Memory"></a>Flash Memory</h2><p>하드디스크와 다르게 Flash는 메모리나 CPU처럼 트랜지스터 들로 구성이 되어있다. 즉 disk의 arm이 움직이거나 platter가 회전하는 물리적인 움직임이 없다.<br>Reliability 측면에서도 disk에 비해 훌륭한 편이다. Disk는 물리적인 head crash가 날수도 있고, dead block(더이상 사용할 수 없는 block) 자체가 생길 수 있다. 물리적으로 읽고 쓰기 때문이다. 그리고 disk는 생각보다 자주깨진다.<br>이와는 다르게 Flash는 순수한 silicon으로 구성되어 있으며 전자적으로 동작하기에 더 신뢰성이 높다. Flash는 또한 매우 빠른 access time을 제공하고 power가 적게드는 반도체 특성으로 disk에 비해 매우매우 저전력이다. Flash Memory는 여러타입이 있는데 보통은 NAND-Flash를 의미한다. 다만 Flash가 가진 특이한 특성들 때문에 이를 해결하기 위해 몇가지 기법들을 적용해야한다. 이들은 밑에서 자세히 알아볼 것이다.   </p><p>Flash chip은 하나의 transistor의 1개 이상의 bit를 저장할 수 있다. <strong>SLC(Single-Level Cell)</strong> flash는 오직 1개의 bit만 transistor에 저장할 수 있고, <strong>MLC(Multi-Level Cell)</strong> flash는 2개의 bit를 저장할 수 있다. 그러므로 00, 01, 10, 11을 저장할 수 있다. <strong>TLC(Triple-Level Cell)</strong> flash는 3개의 bit를 저장가능하다. 전반적으로 SLC가 더 성능이 좋고 가격이 비싸다.  </p><span id="more"></span><br/><h4 id="Block-and-Page"><a href="#Block-and-Page" class="headerlink" title="Block and Page"></a>Block and Page</h4><p>이를 설명하기 전에 명확히 해야할게 여기서도 block과 page 용어가 등장한다. 하지만 Flash에서 말하는 block과 page는 디스크에서의 block 그리고 virtual 메모리에서의 page와 다르다.  </p><p>Flash는 크기에 대해 두가지 단위를 사용하는데 그것이 <strong>block</strong>과 <strong>page</strong>이다.<br>전형적으로 block은 128KB 혹은 256KB 의 크기를 가진다. page는 이보다 작은 2KB 혹은 4KB의 크기를 가진다.<br>Flash는 여러개의 block들로 이루어져 있고 1개의 block은 여러개의 page를 가진다. </p><p align="center">    <img style="max-width: 450px;" src="/images/filesystem/simple-flash.png"/></p><p>위 그림은 아주 간단하게 표현한 Flash chip의 구성이며 총 3개의 block, 그리고 각 block은 4개의 page로 구성되어 있다.  </p><br/><h4 id="Basic-Flash-Operations"><a href="#Basic-Flash-Operations" class="headerlink" title="Basic Flash Operations"></a>Basic Flash Operations</h4><p>Flash chip 에서 제공하는 기본적인 3개의 low-level operation 들이 있다. <strong>read</strong>, <strong>erase</strong>, <strong>program</strong> 이 3가지이다.  </p><h6 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h6><p>Read는 읽으려는 page number를 받아 page 1개를 읽어들인다. 보통 page를 읽어들이는데 10 micro sec 정도 걸리며 읽으려는 location에 무관하다. 즉 disk처럼 이전에 실행한 operation에 따라 head 위치와 rotation에 의존하는 방식이 아니며 어떤 location이라도 균일한 높은 읽기성능을 제공한다.  </p><h6 id="Erase"><a href="#Erase" class="headerlink" title="Erase"></a>Erase</h6><p>Flash는 page에 쓰기전에 page가 속한 block 전체의 지움이 선행되어야 한다. 즉 page가 속한 block을 먼저 <strong>erase</strong>해야한다. 그러므로 erase 전에 block을 memory나 다른 flash block에 copy 해놓아야 한다. Block이 지워지면 모든 bit이 1로 설정된다. erase operation은 비싼 작업이며 millisecond 단위이다.<br>erase가 되면 해당 block은 program 가능한 상태가 된다.  </p><h6 id="Program"><a href="#Program" class="headerlink" title="Program"></a>Program</h6><p>Block에 erase가 먼저 진행된 page에 program을 할 수 있다. erase가 진행되었으니 해당 block은 모든 bit이 1로 설정되어있을 것이다. 이 1로 설정되어 있는 bit를 적절하게 0으로 바꾸면서 데이터를 write 하는데 이를 <strong>program</strong>이라고 한다. program은 erase에 비해 빠른편이며 100 micro sec 정도 걸린다.  </p><br/><p>Flash chip의 각 page는 metadata 정보를 담을 수 있는 조그만 공간이 존재한다. 이곳에 state 정보를 담고있다.<br>Page는 <strong>INVALID</strong> 상태에서 시작한다. Block이 erase 되었으면 그 안에있는 page 들은 <strong>ERASED</strong> 상태가 된다. 이들은 programmable 한 상태이다. page에 program을 하면 <strong>VALID</strong> 상태가 되며 이는 내용이 써져있는 상태이고 읽을 수 있는 상태임을 뜻한다.<br>Page는 한번 program 된 이후에 이 내용을 바꾸기 위해서는 해당 page의 block을 먼저 erase해야한다.<br>다음 예제를 보며 이해하면 쉽다. block마다 4개의 page가 있는 구조에서 page에 program 하는 과정을 표현했다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">              IIII  // 초기는 4개의 page 모두 INVALID state<br>Erase()    -&gt; EEEE  // block erase로 모두 ERASED state<br>Program(0) -&gt; VEEE  // page 0을 program<br>Program(0) -&gt; ERROR // page 0에 다시 program 할 수 없음<br>Program(1) -&gt; VVEE  // page 1을 program<br>Erase()    -&gt; EEEE  // block erase로 모두 ERASED state<br></code></pre></td></tr></table></figure><br/><h4 id="Flash-chip-Reliability"><a href="#Flash-chip-Reliability" class="headerlink" title="Flash chip Reliability"></a>Flash chip Reliability</h4><p>Flash에서 read는 굉장히 쉽고 그냥 읽으면된다. disk보다 훨씬 빠른 access time을 제공하고 random read 성능이 뛰어나다. SLC의 경우 10 micro sec만 걸릴뿐이다. Disk는 읽기도 millisecond 단위였던걸 기억하자.<br>하지만 Flash의 물리적인 특성상 page에 쓰기위해서는 해당 block이 먼저 지워져야 한다. 이를 Program &#x2F; Erase 라고 하여 PE cycle이라고 부른다. 하지만 block에 PE cycle이 반복될시 flash chip에 reliability 문제가 생긴다.<br>Flash에서는 block 당 가능한 P&#x2F;E cycle 횟수가 <strong>한정적</strong>이다. MLC(Multiple-Level Cell) 에서는 1만번의 P&#x2F;E cycle 수명을 가진다. SLC는 이보다 높은 10만번의 P&#x2F;E cycle 수명을 가진다.<br>이를 넘어가면 그 block이 unstable 해진다. 조금 더 정확히는 bit를 0과 1을 구분하기가 점점 어려운 상태가 되므로 그 block은 더이상 사용되지 못하게 된다. 이를 <strong>wear out</strong> 이라고 부른다.<br>따라서 Flash에서는 wear out을 해결하기 위한 또다른 숙제가 존재한다.  </p><p>또 다른 reliability 문제로는 <strong>disturbance</strong>라는 문제가 존재한다. Flash의 page를 읽을때 같은 block에 있는 주변의 page들의 bit를 flip할 수 있는 가능성이 있다. 이를 <strong>read disturb</strong>라고 부르며 block erase 이후의 page read count의 threshold를 넘어가면 해당 문제가 일어날 가능성이 높다고 한다. 이를 해결하기 위한 숙제도 존재한다.  </p><br/><h2 id="Flash-to-SSD"><a href="#Flash-to-SSD" class="headerlink" title="Flash to SSD"></a>Flash to SSD</h2><p>이제 flash chip의 특성에 대해 어느정도 알아보았다. 이 flash chip 하나로는 storage로 활용하기 힘들고 flash chip 여러개를 모아 flash 기반의 SSD를 만든다.  </p><p align="center">    <img alt="Flash 기반의 SSD" style="max-width: 450px;" src="/images/filesystem/flash-based-ssd.png"/></p><p>표준 storage 인터페이스는 block 기반으로 512 byte인 sector 크기 단위로 읽고 쓰여질 수 있다.<br>flash 기반의 SSD의 역할중 하나가 내부적으로는 flash를 사용하지만 그 위에서 이 standard storage block 인터페이스를 외부에 제공하는 것이다.  </p><p>내부적으로 SSD는 여러개의 flash chip 들로 구성되어 있으며 위 구조에서도 볼수있듯이 내부에 SRAM 같은 메모리도 존재한다. 이 메모리로 캐싱이나 buffering에 활용한다. 그리고 device operation을 위한 control logic을 포함한다.<br>이 control logic 에서 해야하는 주된 내용은 client로부터의 read, write를 내부적으로 적절하게 flash operation으로 변환하는 것이다. <strong>FTL</strong>(<strong>Flash Translation Layer</strong>)가 이 역할을 하게된다.<br>FTL은 logical block 기반의 read, write 요청을 받고 이를 flash의 read, erase, program operation 으로 적절하게 변환한다. 그리고 FTL은 이런 작업들을 높은 성능과 높은 reliability를 제공하면서 진행해야 하는 책임도 가지고 있다.<br>높은 성능을 위해 여러개의 flash chip들을 병렬로 활용하기도 한다.<br>그리고 flash 특성상 INVALID 혹은 VALID 상태에 있는 page에 program을 하기 위해서는 반드시 그 block은 먼저 erase가 선행이 되어야 하는데 page 1개만 변환하려 해도 전체 block을 다시 써야하는 문제가 있다. 이를 쓰기 증폭, 즉 <strong>write amplification</strong> 이라고 부르는데 이를 줄이기 위한 노력도 같이한다.  </p><p>그리고 높은 reliability를 제공하기 위해 <strong>wear out</strong> 문제도 고려해야한다. 한개의 block에만 PE cycle을 하는 것이 아닌 최대한 모든 block에 균등하게 처리되도록 해야 wear out을 막을 수 있다. 이를 <strong>wear leveling</strong> 이라고 한다.<br>또 위에서 본 disturbance를 최소화하기위해 erased 된 block에 page를 낮은 page부터 높은 page 순서로 program 하는 방법을 택한다.  </p><h4 id="FTL-구현"><a href="#FTL-구현" class="headerlink" title="FTL 구현"></a>FTL 구현</h4><p>어떻게 하면 FTL을 구현할 수 있을까?<br>간단하게 logical page number 그대로 physical page number 로 매핑을 해주는 FTL이 있다고 생각해보자.<br>이 방식이라면 그대로 매핑해주는 방식이기에 logical page N 에 대해 read 요청이 오면 그대로 physical page N 을 읽어 반환한다. read에는 큰 문제가 없다.<br>write를 생각해볼때, write 요청이 오면 flash 특성상 해당 page의 block을 erase 한 후 그 block의 page로 다시 program 해야한다. 그래야 다음 read에 대해 동일한 physical page N을 읽을 수 있도록 보장할 수 있다.<br>이 방식으로 FTL을 구현하면 첫번째로 성능측면에서 문제가 있고 reliability 문제도 존재한다.<br>flash의 특성으로 page에 write을 할때마다 해당 block의 모든 page를 먼저 읽어들이고, 해당 block을 erase, 그리고 이 block의 page에 다시 program을 해주어야한다. 이를 매 write 마다 해주어야 하므로 디스크의 write보다 느리다.  </p><p>그리고 파일시스템의 metadata 혹은 data block이 업데이트 될때마다 같은 block이 PE cycle을 반복하게 되므로 그 block은 빠르게 wear out이 될 것이다. 이처럼 write 요청을 다른 여러 물리 block에 균일하게 배정하지 않으면 block이 빠르게 wear out 된다. 그러므로 direct로 logical page를 그대로 physical page로 매핑하는 방식은 좋은방법이 아니다.  </p><br/><h2 id="Log-Structured-FTL"><a href="#Log-Structured-FTL" class="headerlink" title="Log-Structured FTL"></a>Log-Structured FTL</h2><p>여기서는 log structured 방식을 설명하는데 이는 storage device 뿐만아니라 file system 에서도 유용한 아이디어이다. 대부분의 FTL은 log structured 방식을 사용한다. 이를 살펴보자.  </p><p>Log-Structured 방식에서는 logical page에 write 요청이 왔을시, 현재까지 쓰여진 block에 바로 다음 free spot에 데이터를 쓴다. 이런 방식을 <strong>logging</strong> 이라고 부른다.<br>하지만 이런 방식으로 쓰면 read를 할때 logical page가 어떤 physical page에 쓰여졌는지 기록을 해놓아야 하므로 <strong>mapping table</strong>이 필요하다.  </p><p>예시를 보며 이해해보자.  </p><p>클라이언트는 그저 device를 sector 단위인 512 byte(혹은 sector의 group)로 읽고 쓸수있는 전형적인 disk 라고 생각한다는걸 잊지말자. 여기서의 클라이언트는 파일시스템이라고 하자.<br>예시를 간단하게 하기위해 몇가지 가정을 하자.  </p><ul><li>파일시스템은 4KB 단위의 chunk로 데이터를 read, write 한다.</li><li>SSD는 16KB의 block들로 이루어져있고 각 block은 4KB의 page들로 이루어져 있다.</li></ul><p>클라이언트는 다음 순서로 operation을 요청한다.  </p><ol><li>데이터 “a1”을 logical block <code>100</code>에 write한다.</li><li>데이터 “a2”을 logical block <code>101</code>에 write한다.</li><li>데이터 “b1”을 logical block <code>2000</code>에 write한다.</li><li>데이터 “b2”을 logical block <code>2001</code>에 write한다.</li></ol><p>여기서의 logical block number는 flash의 block과는 다름을 주의하자.  </p><p>위 요청을 받았을때 초기의 SSD는 모든 block의 page들이 INVALID 상태이므로 어느 block에 이를 program하든지 erase가 먼저 선행되어야 한다. FTL이 이를 block 0에 program 한다고 가정했을때 먼저 block 0을 erase 한다.  </p><p align="center">    <img style="max-width: 450px;" src="/images/filesystem/ftl-example-1.png"/></p><p>block 0 은 이제 program을 할 수 있는 상태이다. 대부분의 SSD는 앞에서 본 disturbance(read 시 주변 셀들을 변경할 수 있음) 문제를 줄이기 위해 page를 앞에서부터 차례로 쓴다.<br>그러면 처음 logical block <code>100</code>에 대한 요청을 physical page 0에 write한다.</p><p align="center">    <img style="max-width: 450px;" src="/images/filesystem/ftl-example-2.png"/></p><p>위처럼 physical page 0에 data가 쓰여졌다. 만약 파일시스템이 logical block <code>100</code>을 다시 읽으려면 어떤 처리를 해주어야할까? FTL은 read 요청을 받았을때 logical block을 physical page로 적절하게 mapping 해줄 수 있어야한다. 이를 위해 <strong>in-memory mapping table</strong>을 만들어 기록하자.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/ftl-example-3.png"/></p><p>SSD에 write을 할때에는 현재까지 작성한 block의 다음 비어있는 page에 program한다. 그리고 이에 대한 정보를 mapping table에 기록한다. 다음 해당 page들에 대한 read요청이 왔을때 client로 부터 온 logical block 을 physical page로 mapping table을 이용해 변환하여 실제로 어떤 physical page를 봐야하는지 결정한다. 나머지 3개의 logical block도 다 쓰게되면 다음과 같다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/ftl-example-4.png"/></p><p>최종상태는 위와같은 그림이 될 것이다.<br>이런 logging 기반의 구조는 성능적으로 뛰어난데 매번 write 할때마다 block을 erase할 필요가 없다. 그리고 reliability 측면에서도 FTL이 write 시 여러 block을 골고루 사용하도록 설계할 수 있으며 이 덕에 device의 수명을 늘릴 수 있다. 이런 해결을 <strong>wear leveling</strong>한다 라고 표현한다.  </p><p>하지만 이런 logging 기반 구조는 단점이 존재하는데 logical block을 수정해야 할때에는 garbage가 생긴다. 새로운 page에 다시 program 하므로 이전에 있던 page는 garbage가 된다. 그래서 SSD는 주기적으로 이런 garbage들을 정리하고 free space를 확보하는 <strong>garbage collection</strong>을 진행해야한다. 과도한 GC는 write amplification을 발생시키고 성능을 낮춘다.<br>그리고 mapping table을 관리해야 하는 문제도 있다. mapping table이 클수록 그만큼 더 큰 memory를 요구하게 된다.  </p><p>mapping table을 in-memory로 관리하는데 power가 꺼지면 어떻게 될까? memory는 휘발성으로 당연히 날아간다. 하지만 mapping table이 없으면 read operation을 처리해주지 못하므로 SSD는 mapping table을 복구할 수 있는 장치를 마련해야한다.<br>간단하게는 각 page에 <strong>out-of-band</strong>(<strong>OOB</strong>)라는 영역에 mapping 정보를 기입해놓을 수 있다. 이를 기반으로 SSD를 켤때 모든 page들을 읽어 mapping table을 재구성할 수 있다. 하지만 모든 page를 scan해야 하므로 성능이 좋지않다.<br>최근에는 복잡한 logging과 checkpoint 방식의 기법으로 빠른 recovery를 지원한다.  </p><br/><h2 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a>Garbage Collection</h2><p>위의 예제를 다시 이어서 보면 마지막에는 page 0과 1에 각각 logical block 100 그리고 101 이 매핑되어 있었다. 만약 logical block 100과 101을 다시 write 하면 어떻게될까?<br>다음 block의 free page에 쓰여질 것이다. 그리고 page 0과 1은 VALID 상태이지만 최신버전이 아니므로 garbage이다.<br>log-structure 기반의 device는 garbage를 계속 만들어내므로 free space를 더 확보하기 위해 <strong>garbage collection</strong>을 진행해야한다. 이는 최신 SSD에서도 고려해야하는 중요한 요소이다.  </p><p>GC의 기본적인 과정은 이와같다.  </p><ol><li>최소 1개이상의 garbage page를 가지고 있는 block을 찾는다.</li><li>그 block의 아직 live 한 page들을 읽는다.</li><li>읽은 live한 page들을 log(block)에 쓴다. 그리고 기존 block을 erase 하여 새로운 write에 사용할 수 있도록 한다.</li></ol><p>GC가 이 과정을 수행하기 위해 어떤 page가 live 한지 혹은 garbage 인지 판단할 수 있어야하는데, 간단하게는 mapping table 을 이용할 수 있다. mapping table에는 실제 physical page들이 명시되어있으므로 이들이 live한 page이다.<br>이런 방식으로 위의 예제에서 logical block 100과 101에 각각 데이터 “c1”과 “c2”를 다시 쓰게되면 다음과 같이된다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/gc-example-1.png"/></p><p>현재 mapping table을 보게되면 page 0과 1이 garbage인 것을 알 수 있다. 그러므로 block 0 에 있는 live한 page인 page 2와 3을 읽어 이들을 log에 쓴다. 즉 다음 free space에 쓰고 block 0을 erase 하여 다음 program operation에서 활용할 수 있도록 한다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/gc-example-2.png"/></p><p>따라서 mapping table도 새로 update 되고 block 0 도 erase된 상태가 되었다.<br>이처럼 GC는 live data를 판단하기 위한 read 그리고 이들을 copy 하여 rewrite, 만약 block이 전부 dead page들로 이루어져 있으면 erase 까지 진행하므로 비싼 작업이다. 그래서 현대 SSD에서는 추가적으로 flash 용량을 더 두어서 device가 바쁘지 않을때 background 에서 GC를 수행하도록 한다. flash 용량을 더 두게되면 data cleaning에 사용할 수 있고 storage bandwidth도 높일 수 있다.  </p><br/><h2 id="Mapping-Table"><a href="#Mapping-Table" class="headerlink" title="Mapping Table"></a>Mapping Table</h2><p>위에서 확인했듯이 Log-Structured 방식은 mapping table이 필요하다. 다만 이 mapping table 크기가 문제이다. 예를들어 4KB page로 구성되어있는 1TB의 SSD가 있다고 할때 mapping table entry가 4byte라면 mapping table의 크기는 1GB가 된다. 즉 mapping table 만을 위해서 1GB의 memory가 필요한 것이다.<br>이를 극복하기 위한 <strong>Block-Based Mapping</strong> 방식과 최근 많은 SSD에서 채택하는 <strong>Hybrid Mapping</strong> 방식을 알아볼 것이다.  </p><br/><h4 id="Block-Based-Mapping"><a href="#Block-Based-Mapping" class="headerlink" title="Block-Based Mapping"></a>Block-Based Mapping</h4><p>Block 기반 mapping은 page 별로 mapping을 하는게 아니라 block 단위로 mapping을 한다.<br>이렇게 block 단위로 매핑을 하면 mapping table의 크기는 <code>size of block / size of page</code> 로 나눈 값으로 줄일 수 있다. 예를들어 block 1개에 4개의 page가 들어간다면 page-based mapping 방식에 비해 mapping table의 크기를 4분의 1로 줄일 수 있다.  </p><p>간단하게 예시를 보며 이해해보자.<br>위에서 본 예시와 조금 다르게 지금 우리가 logical block 2000, 2001, 2002, 2003 까지 각각 데이터 a, b, c, d를 write한 상태라고 하자.<br>block 단위로 mapping을 하므로 block 번호의 맨 뒤 2bit만 offset으로 활용하고 그 앞부분은 block number로 활용할 수 있다. 마치 page table 에서 page number와 offset으로 물리메모리를 찾는 방식과 비슷하다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/bbm-example-1.png"/></p><p>따라서 2000, 2001, 2002, 2003 은 각각 offset 0, 1, 2, 3을 가지고 block number는 같다. 예시에서는 block 1개에 page가 4개 있고 위의 write을 physical page 4에 할당하였으므로 mapping table에 <code>500 -&gt; 4</code>로 기록한다.<br>read 요청이 왔을때에는 logical block 번호를 4로 나눈 값으로 physical page number를 찾고 offset으로 page 순서를 계산할 수 있다.  </p><p>하지만 Block-Based Mapping은 성능적으로 많이 좋지않다. 문제는 작은 단위의 write이 일어날때 발생한다. 작은 단위의 write이 발생해도 old block 전체를 읽어 새로운 block에 다시 써줘야한다. 이런 data 복사는 write amplification 으로 이어진다.  </p><p>만약 위의 예제에서 logical block 2002를 다른값으로 수정하면 어떻게될까?<br>FTL은 logical block 2000, 2001, 2003을 읽고 이들을 다른 block에 새로 써주어야 한다. 그리고 block 1은 erase될 수 있다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/bbm-example-2.png"/></p><p>이처럼 mapping table 크기관점에서는 훨씬 작은 크기를 가져갈 수 있기때문에 좋은 해결책일 수 있으나 성능이 매우 좋지않아 이 방식을 그대로는 활용하기 힘들다.</p><br/><h4 id="Hybrid-Mapping"><a href="#Hybrid-Mapping" class="headerlink" title="Hybrid Mapping"></a>Hybrid Mapping</h4><p>많은 현대 SSD 제품들은 <strong>Hybrid Mapping</strong> 방식을 사용한다. 이를 알아보자.<br>Hybrid Mapping은 이름에서 유추할 수 있듯이 block-based mapping과 page-based mapping을 함께 사용한다.<br>먼저 FTL은 몇개의 block들을 erased 상태로 남겨두고 모든 write를 이 block들에 쓴다. 이 block 들을 <strong>log blocks</strong> 라고 부른다. Hybrid Mapping 에서는 page에 대한 write을 log block의 아무 위치에 쓸수있도록 고안되었기 때문에 이 log block에 속한 block들에 한하여 page-based mapping table을 가진다.   </p><p>따라서 FTL은 두종류의 mapping table을 사용한다. 먼저 log block 들을 대상으로 작은 크기의 page-based mapping table을 가지고 나머지 부분인 data table에 대해 block-based mapping table을 가진다. logical block에 대한 read 요청이 오면 먼저 log table을 확인하고 없다면 그때 data table을 확인한다. (mapping table을 확인한다는 것이다.)  </p><p>Hybrid Mapping에서 중요한 것은 log block을 작게 유지하는 것이다. 그러려면 주기적으로 log block을 검사해 이들을 data block으로 만들어 block-based mapping을 사용하도록 해야한다. 예제를 보며 이해해보자.  </p><p>FTL이 이미 logical page 1000, 1001, 1002, 1003을 write을 한 상태이고, 값을 각각 a, b, c, d 라고 하자. 이들은 physical block 2에 쓰였다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/hm-example-1.png"/></p>  <p>그런데 파일시스템이 logical page 1000, 1001, 1002, 1003 순서대로 하나씩 수정했다고 해보자. 이때 program 할 수 있는 log block은 block 0 이라고 해보자. 그러면 다음과 같은 상태가 된다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/hm-example-2.png"/></p><p>log block에 대해서는 page-based mapping을 사용하는 것을 주목하라.<br>운이 좋게도 이전에 block 2 에 쓰였던 순서와 동일하게 block 0 에 쓰여졌으므로 FTL은 <strong>switch merge</strong>라는 것을 진행할 수 있다. 여기서는 block 0 이 data block이 되고 block-based mapping 방식으로 변경될 수 있다. 그리고 block 2는 erase될 수 있다. 이는 FTL이 가질 수 있는 최상의 시나리오다. 결과는 다음과 같다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/hm-example-3.png"/></p><p>page-based mapping이 모두 block-based mapping으로 대체되었고 block 2는 erase 되어 log block으로 사용할 수 있게 되었다.<br>만약 이와 같은 최상의 시나리오가 아니라 파일시스템이 logical page 1000, 1001 만 수정했다고 하면 어떻게 될까? 다음과 같은 상태가 될 것이다.  </p><p align="center">    <img style="max-width: 550px;" src="/images/filesystem/hm-example-4.png"/></p><p>여기서 page 들을 합쳐 재구성하기위해서 FTL은 <strong>partial merge</strong>라는 것을 진행할 수 있다. physical block 2에서 logical block 1002, 1003 을 읽어 이 내용을 다시 log block에 append 한다. 그러면 결국 결과는 위의 switch merge와 같아진다. 다만 여기서는 추가적인 read, write가 필요하므로 write amplification이 증가할 수 있다.  </p><p>만약에 최악의 상황에서 logical block 0, 4, 8, 12 가 physical block A 에 써있다고 하자. 그러면 이 log block 들을 data block로 변경하기 위해서는 logical block 1, 2, 3 이 써져있는 physical page를 찾아 이들을 읽고 새로운 block에 logical block 0, 1, 2, 3을 차례로 써주어야한다. 그다음 logical block 5, 6, 7 이 써져있는 page를 찾아 다시 새로운 block에 써준다. 그리고 이들을 data block으로 만든다. 이를 <strong>full merge</strong> 방식이라고 부르며, 이 full merge는 성능상 좋지않으므로 자주 실행되면 안된다.  </p><br/><h2 id="Wear-Leveling"><a href="#Wear-Leveling" class="headerlink" title="Wear Leveling"></a>Wear Leveling</h2><p>FTL은 반드시 <strong>wear leveling</strong>을 구현해야 한다. FTL은 이를 위해 최대한 모든 block에 균등하게 write하려고 노력한다.<br>앞서본 log-structured 방식은 구현자체가 write를 여러 block에 나누어할 수 있고 GC 또한 wear leveling에 도움이 된다. 하지만 몇몇 data block들은 오랜 수명을 가지고있어 수정되지 않고 계속 변하지 않고 그대로 저장되어 있을 수 있다. 이들은 정상적인 data로 GC 대상도 아니다.<br>전체 block을 대상으로 write을 균일하게 써야하는 FTL 입장에서는 이런 long-lived data 가 문제일 수 있다. 이를 극복하기 위해 FTL은 주기적으로 모든 live data를 읽고 다른 block에 다시 write하는 작업을 진행한다.<br>이는 write amplification을 증가시키고 성능저하를 일으킬 수 있지만 모든 block이 동일한 수명을 가지게 할 수 있다. 이 방법 말고도 많은 알고리즘이 이미 고안되어 있다.  </p><br/><h2 id="SSD-Architecture"><a href="#SSD-Architecture" class="headerlink" title="SSD Architecture"></a>SSD Architecture</h2><p>SSD 전체구조를 그림으로 한번 보고가자.  </p><p align="center">    <img alt="SSD 구조" src="/images/filesystem/ssd-architecture.png"/></p><p>FTL layer를 보고 어떤 일들을 하고있고 이들을 왜 해야하는지 이제 이해를 할 수 있을것이다.  </p><br/><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>SSD는 하드디스크처럼 모터로 돌아가는 기게적인 부분이 없이 전자적으로 작동한다. 성능측면에서 SSD와 하드디스크를 비교를 해보자.  </p><p align="center">    <img alt="SSD vs Hard Disk" style="max-width: 550px;" src="/images/filesystem/ssd-vs-hdd.png"/></p>  <p>Random I&#x2F;O의 경우 하드디스크는 거의 1초에 몇백개의 byte만 처리할 수 있는반면 SSD는 훨씬많이 할 수 있다. SSD는 random I&#x2F;O를 거의 몇십 몇백 MB&#x2F;s 로 처리할 수 있으므로 아주 높은 성능을 가진다.   </p><p>Sequential I&#x2F;O를 보면 SSD가 더 빠르긴 하지만 하드디스크도 괜찮은 선택일 수 있다. 하드디스크도 충분히 좋은 성능을 내고 있음을 볼 수 있다.<br>조금 특이한건 SSD의 random write이 매우 좋은 성능을 가지고 있는데 이는 대부분의 SSD가 채택하고 있는 log-structured 구조 덕분이다. 이는 random write을 sequential write처럼 동작하도록 FTL이 처리하기 때문이다.<br>또 SSD에서 random read 보다는 sequential read가 더 빠른걸 볼 수 있는데, 이는 sequential read는 read에 대한 operation을 큰 data 를 대상으로 한번에 받을 수 있기 때문이다. random read는 매번 각기 다른 주소에 대한 read 요청을 받아야 한다. 그러므로 처리량이 sequential read가 더 높을 수 밖에 없다. 경우에 따라 내부적으로 가진 cache의 영향도 있을 수 있다.  </p><p>SSD가 하드디스크에 비해 가격이 높으므로 사용하려는 시스템에 따라 높은 성능과 random read performace가 중요한 시스템은 SSD를, 엄청난 양의 데이터를 저장해야하는 data center를 구축해야 하는 경우는 하드디스크가 가성비 측면에서 좋은 선택이 될 수 있겠다.  </p><br/><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4">https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4</a></li></ul><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      <category domain="https://tk-one.github.io/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/">파일시스템</category>
      
      
      <comments>https://tk-one.github.io/2020/09/06/file-system-flash/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>파일시스템 1편 - 하드디스크</title>
      <link>https://tk-one.github.io/2020/09/05/file-system-hard-disk/</link>
      <guid>https://tk-one.github.io/2020/09/05/file-system-hard-disk/</guid>
      <pubDate>Fri, 04 Sep 2020 15:51:46 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.  &lt;/p&gt;
&lt;p&gt;처음으로 파일시스템 자체를 보기전에 물리적인 device의 작동원리에 대해 이해를 하고 파일시스템을 보게되면 더 이해가 쉽습니다. 이번 편에서는 하드디스크에 대해 다룰 것입니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 이전의 &lt;a href=&quot;/2019/07/09/os-computer-architecture/&quot;&gt;운영체제 3편 - 컴퓨터 구조와 I&amp;#x2F;O&lt;/a&gt; 글의 I&amp;#x2F;O 부분을 이해하고보면 도움이 됩니다.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.  </p><p>처음으로 파일시스템 자체를 보기전에 물리적인 device의 작동원리에 대해 이해를 하고 파일시스템을 보게되면 더 이해가 쉽습니다. 이번 편에서는 하드디스크에 대해 다룰 것입니다.  </p><p>이번 편은 이전의 <a href="/2019/07/09/os-computer-architecture/">운영체제 3편 - 컴퓨터 구조와 I&#x2F;O</a> 글의 I&#x2F;O 부분을 이해하고보면 도움이 됩니다.  </p><span id="more"></span><br/><h2 id="Disk-Interface"><a href="#Disk-Interface" class="headerlink" title="Disk Interface"></a>Disk Interface</h2><p>가볍에 Disk의 Interface를 알아보자.<br>Disk는 많은 <strong>sector</strong>라는 것으로 이루어져 있고 각 sector는 읽거나 sector에 값을 쓸 수 있다. sector에는 번호가 있는데 <code>0</code> 부터 <code>n - 1</code> 까지 있다고 하면 총 n개의 sector가 disk에 있다고 할 수 있다.<br>우리는 disk를 sector의 리스트로 생각할 수 있고 이 sector 들의 리스트가 disk의 address space를 결정한다.  </p><p>sector는 특별한 이유가 있지 않은이상 512byte의 크기를 가진다.<br>다만 파일시스템에서는 <strong>block</strong> 이라는 단위로 디스크에 접근하는데 이는 4KB이다. 즉 파일시스템은 block 단위로 디스크에 읽고 쓴다.<br>하지만 disk에서는 sector 단위(512byte)에서의 write만 atomic을 제공한다. 즉 block write를 하면 연속된 8개의 sector에 데이터를 쓴다. 그러나 각 sector 단위에서의 atomic만 보장하므로 write 하는 순간 disk에 문제가 생기면 block의 일부분만 쓰여지는 문제가 발생할 수 있다. 이를 <strong>torn write</strong>이라고 부른다.  </p><p>밑에서 보게되겠지만 서로 인접해있는 sector들을 읽는 것이 random한 위치의 sector들을 읽어들이는 것보다 훨씬 빠르다.  </p><br/><h2 id="Disk-Structure"><a href="#Disk-Structure" class="headerlink" title="Disk Structure"></a>Disk Structure</h2><p>우선 대략적인 그림을 보며 Disk 구조를 알아보자.  </p><p align="center">    <img style="max-width: 550px" alt="Disk Structure" src="/images/filesystem/disk-structure-1.png"/></p>  <h4 id="platter"><a href="#platter" class="headerlink" title="platter"></a>platter</h4><p>디스크는 최소 하나이상의 platter들로 이루어져 있다.<br>위 그림에서 CD처럼 생긴 것을 <strong>platter</strong>라고 부른다. platter는 원형의 강한 재질로 이루어진 표면들로 구성되어 있고 이곳에 전자기적으로 데이터를 영구히 저장한다.<br>platter는 윗부분, 밑부분 각각 데이터를 저장할 수 있다.<br>보통 platter는 알루미늄으로 만들고 겉부분을 아주 얇은 자기성 성질을 가진 층으로 코팅한다. 이 부분에 자기성 성질을 가지므로 데이터를 영구히 저장할 수 있다.  </p><p>그리고 이 platter는 가운데 <strong>spindle</strong>이라는 것에 붙어있으며, 이 spindle은 모터에 연결되어있어 platter들을 회전하도록 한다.<br>이런 회전율을 <strong>RPM</strong>(Rotations Per Minute)으로 보통 측정하며 요즘에는 보통 7200rpm 부터 15000rpm 까지 가진다.<br>10000rpm은 single rotation이 대략적으로 6ms정도 걸린다.<br>일반적으로 platter는 계속해서 회전을 하고있다.  </p><h4 id="track"><a href="#track" class="headerlink" title="track"></a>track</h4><p>위에서 본 sector들이 원형으로 배치된다. 이런 원형 하나를 <strong>track</strong>이라고 부른다. 하나의 platter 표현에는 수천개의 track이 존재하며 거의 사람머리카락 수준의 두께를 가지고있다.<br>하나의 track에 대한 대략적인 그림은 다음과 같다.  </p><p align="center">    <img alt="Single track" src="/images/filesystem/single-track.png"/></p>  <p>이 track은 12개의 sector들로 이루어져 있고 각 sector들은 512byte이다. 각 sector들에는 주소가 있는데 여기에서는 0 부터 11까지 존재한다.  </p><h4 id="cylinder"><a href="#cylinder" class="headerlink" title="cylinder"></a>cylinder</h4><p>Cylinder는 각 platter들에 같은 반지름을 가진 track들을 의미한다. 위 그림을 보고 이해할 수 있다.  </p><h4 id="Read-amp-Write"><a href="#Read-amp-Write" class="headerlink" title="Read &amp; Write"></a>Read &amp; Write</h4><p>platter의 표면으로부터 데이터를 읽고 쓰기 위해서는 disk의 전자기적 패턴을 읽어들이거나 이를 쓸 수 있어야 한다.<br>이러한 것들이 <strong>disk head</strong>를 통해 가능하다.<br>head는 platter 표면마다 한개씩 존재하며 이 head 들은 <strong>disk arm</strong>이라는 것에 붙어있다. 표면을 따라 움직이며 이들을 움직임으로서 적절한 track에 접근할 수 있다.  </p><p>위에서 platter는 시계반대방향으로 회전하며, track이 회전하면서 head가 적절한 sector위에 놓여지면서 올바른 데이터를 읽고 쓸수 있다.  </p><br/> <h2 id="Disk-Access"><a href="#Disk-Access" class="headerlink" title="Disk Access"></a>Disk Access</h2><p>위에서 본 track 그림을 다시보자.  </p><p align="center">    <img alt="Single track with head" src="/images/filesystem/track-with-head.png"/></p>    <h4 id="Rotational-Delay"><a href="#Rotational-Delay" class="headerlink" title="Rotational Delay"></a>Rotational Delay</h4><p>head가 6번 sector위에 놓여있는 상황에서 sector 0을 읽어야 하는 요청을 디스크가 받았다고 하자. 이를 어떻게 읽을 수 있을까?  </p><p>여기서는 그냥 sector 0이 head위에 오도록 rotate 시키면된다.<br>이를 회전시키는데 시간이 걸리는데 이를 <strong>rotational delay</strong>라고 부른다.<br>위 예제에서 rotational delay가 <code>R</code>이라면 sector 0까지 회전하는데에는 <code>R/2</code>의 시간이 걸린다.<br>만약 sector 5를 읽어야 한다면 전체 한바퀴를 돌려야하므로 <code>R</code>의 시간이 걸린다.  </p><h4 id="Seek-time"><a href="#Seek-time" class="headerlink" title="Seek time"></a>Seek time</h4><p align="center">    <img alt="Three track with head" src="/images/filesystem/three-tracks.png"/></p>  <p>위의 왼쪽 그림에서 현재 head는 가장 안쪽 track에 sector 30번 위에 놓여있다.<br>여기서 sector 11번을 읽으라는 요청이 들어오면 어떻게 해야할까?<br>Disk는 먼저 head를 sector 11번이 있는 track으로 이동을 해야한다.<br>이러한 과정을 <strong>seek</strong>라고 한다. 그리고 이에 걸리는 시간을 <strong>seek time</strong>이라고 한다.  </p><p>위의 오른쪽 그림에서 sector 11번을 읽기위해 head가 세번째 track으로 이동했다. 그 와중에 platter 또한 회전했다는 것을 알 수 있다. 현재 sector 9에 head가 위치하므로 rotational delay만 기다리면 원하는 sector를 읽을 수 있겠다.  </p><h4 id="Transfer-Delay"><a href="#Transfer-Delay" class="headerlink" title="Transfer Delay"></a>Transfer Delay</h4><p>위의 예제에서 sector 11번이 head를 지날때 데이터를 읽거나 데이터를 write한다.<br>이를 <strong>transfer</strong>라고 한다. 그리고 이에 걸리는 시간을 <strong>transfer delay</strong>라고 한다.<br>보통 transfer delay는 I&#x2F;O Bus를 통해 memory에 저장이 될때까지의 시간을 포함한다. 여기서 disk head가 읽어들이는 시간과 이를 DMA engine이 읽어들이는 시간보다 가장 영향을 많이 받는 것은 I&#x2F;O Bus의 latency 이다. DMA engine은 충분히 빠르다.  </p><p>그래서 결국 데이터를 읽거나 쓰려면 원하는 track을 seek하고 적절한 sector에 접근하기 위해 rotational delay를 기다리고 그리고 transfer 하게된다.  </p><h4 id="Track-skew"><a href="#Track-skew" class="headerlink" title="Track skew"></a>Track skew</h4><p>보통 track의 경계를 넘어 head가 왔다갔다 할때에도 rotational delay를 줄여 연속적인 읽기가 가능하도록 sector 번호를 배치한다.  아래 그림을 보자.  </p><p align="center">    <img alt="track skew" style="max-width: 350px;" src="/images/filesystem/track-skew.png"/></p>  <p>위 그림에서 읽고 싶은 sector 번호가 22, 23, 24, 25라고 하자.<br>head가 가장 가까운 다음 track으로 이동한다고 하더라도 head를 정확하게 reposition하는 시간이 필요하다.<br>이런 시간들을 고려하여 sector을 위와같이 배치한다면 sector 22, 23까지 읽고 head를 옮겨 seek time이 지나면 head가 sector 24번에 위치함으로서 서로 다른 track에 위치한 연속적인 sector를 읽어들일때 rotational delay를 줄일 수 있다는 것이다.   </p><p>위에서는 각 track에서 sector의 시작점이 2개씩 차이난다.   </p><h4 id="Multi-zoned"><a href="#Multi-zoned" class="headerlink" title="Multi-zoned"></a>Multi-zoned</h4><p>track이 바깥쪽에 있을수록 원형구조상 sector들을 더 많이 가질 수 밖에 없다.<br>그래서 연속적인 track을 묶어 이들을 각각 zone이라고 부르고, 같은 zone에 있는 track들은 같은 sector 개수를 가진다. 원형의 가장 바깥쪽에 위치한 zone들의 track은 가장 많은 sector 개수를 가진다.  </p><h4 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h4><p>요즘 대부분의 disk 안에는 캐시가 존재한다. 이를 <strong>track buffer</strong>라고 부르며 보통 작은 크기의 메모리로 구성된다.<br>예를들어 특정 sector을 읽어야하는 요청이 들어오면, 그 track의 모든 sector를 읽고 buffer에 저장해놓는다.  </p><p>Write 연산은 크게 두가지 방식으로 캐시를 활용할 수 있다.<br>첫번째로는 write 요청을 memory에만 써놓고 이를 비동기적으로 disk에 동기화시킨다. 이는 write 요청에 대한 응답이 매우 빠르지만 disk에 반드시 쓰여졌음을 보장할 수 없다. 이런 방식을 <strong>write back</strong>이라고 부른다.  </p><p>두번째로는 실제로 write 요청시 disk에 쓰고 buffer을 날려버릴 수 있다. 이런 방식을 <strong>write through</strong>라고 부른다.  </p><br/><h2 id="I-x2F-O-시간-계산"><a href="#I-x2F-O-시간-계산" class="headerlink" title="I&#x2F;O 시간 계산"></a>I&#x2F;O 시간 계산</h2><p>결국 Disk I&#x2F;O 시간은 다음과 같이 계산된다.  </p><h4 id="TI-O-Tseek-Trotation-Ttransfer"><a href="#TI-O-Tseek-Trotation-Ttransfer" class="headerlink" title="TI/O = Tseek + Trotation + Ttransfer"></a><code>T<sub>I/O</sub> = T<sub>seek</sub> + T<sub>rotation</sub> + T<sub>transfer</sub></code></h4><p>Disk에서는 Rate of I&#x2F;O라는 단위로 성능을 측정해서 비교하곤 한다. Rate of I&#x2F;O는 간단하게 <strong>1초동안 얼마나 I&#x2F;O를 처리할 수 있는가</strong>로 이해할 수 있다. Rate of I&#x2F;O는 일정시간동안 I&#x2F;O를 처리하고 transfer 할 수 있는 크기를 I&#x2F;O를 처리하는데 걸린 시간으로 나누면 된다.  </p><h4 id="RI-O-Sizetransfer-TI-O"><a href="#RI-O-Sizetransfer-TI-O" class="headerlink" title="RI/O = Sizetransfer / TI/O"></a><code>R<sub>I/O</sub> = Size<sub>transfer</sub> / T<sub>I/O</sub></code></h4><p>예제로 disk의 random access와 sequential access의 Rate of I&#x2F;O를 측정해보자.<br>예제의 기준으로 사용한 disk는 Seagate의 높은 성능을 가진 SCSI drive인 Cheetah 15K.5와 SATA drive인 Barracuda이다. 이에 대한 스펙은 다음과 같다.  </p><table><thead><tr><th></th><th>Cheetah 15K.5</th><th>Barracuda</th></tr></thead><tbody><tr><td>Capacity</td><td>300 GB</td><td>1 TB</td></tr><tr><td>RPM</td><td>15,000</td><td>7,200</td></tr><tr><td>Average Seek</td><td>4ms</td><td>9ms</td></tr><tr><td>Max Transfer</td><td>125 MB&#x2F;s</td><td>105 MB&#x2F;s</td></tr><tr><td>Platters</td><td>4</td><td>4</td></tr><tr><td>Cache</td><td>16 MB</td><td>16&#x2F;32 MB</td></tr><tr><td>Connects via</td><td>SCSI</td><td>SATA</td></tr></tbody></table><p>먼저 random location의 4KB 크기의 read가 발생할때 시간이 얼마나 걸릴지 계산해보자.<br>먼저 Chheta 15K.5 의 경우이다.  </p><h6 id="Tseek-4ms-Trotation-2ms-Ttransfer-30-micro-sec"><a href="#Tseek-4ms-Trotation-2ms-Ttransfer-30-micro-sec" class="headerlink" title="Tseek = 4ms, Trotation = 2ms, Ttransfer = 30 micro sec"></a><code>T<sub>seek</sub> = 4ms, T<sub>rotation</sub> = 2ms, T<sub>transfer</sub> = 30 micro sec</code></h6><p>average seek time은 표에서 찾을 수 있다.<br>rotation은 15000rpm을 가지므로 한번 rotate시 4ms 걸린다. 보통 평균으로 절반을 회전하므로 2ms로 계산한다.<br>transfer time은 125 Mega Byte per sec을 역산하면 30 micro sec이 걸린다. 대략적으로 4KB의 I&#x2F;O 시간은 6ms 가 되겠다.<br>그러므로 이를 기반으로 Rate of I&#x2F;O를 계산하면 Cheetah 15K.5의 random access read의 R<sub>I&#x2F;O</sub>는 대략적으로 0.66MB&#x2F;s 이다.  </p><p>만약 sequential read 이고 100MB 대상으로 파일을 읽는다고 해보자. 이 경우에는 sequential read이므로 1 seek, 1 rotation 그리고 대량의 transfer time이 걸릴 것이다.(물론 sector들이 여러개의 track에 있을 수 있지만 그냥 가정이다.)<br>4KB에 30 micro sec의 시간이 걸렸으므로 100MB에는 768ms가 걸린다. 대략적으로 800ms 걸린다고 가정한다면 100MB의 read에는 800ms가 걸린다.<br>이를 기준으로 sequential read의 R<sub>I&#x2F;O</sub>는 125MB&#x2F;s 이다.  </p><p>Barracuda도 이와 동일하게 계산하면 결과는 다음과 같다.  </p><table><thead><tr><th></th><th>Cheetah 15K.5</th><th>Barracuda</th></tr></thead><tbody><tr><td>R<sub>I&#x2F;O<sub> Random</td><td>0.66 MB&#x2F;s</td><td>0.31 MB&#x2F;s</td></tr><tr><td>R<sub>I&#x2F;O<sub> Sequential</td><td>125 MB&#x2F;s</td><td>105 MB&#x2F;s</td></tr></tbody></table><p>위의 결과를 보면 Random access와 Sequential access의 성능차이는 어마어마 하다. 거의 200배, 300배 차이가 나는 것을 볼 수있다.  </p><p>또한 제품마다 특성이 다르며 각 용도에 따라 올바르게 disk를 선택하는 것이 좋다.<br>예를들어 RPM 수치가 높다고 반드시 좋은 것은 아니다. RPM이 높아질수록 안정성이 떨어지기 때문이다. 데이터를 오랫동안 안정적으로 보관하려면 낮은 RPM 제품을 선택하는게 도움이 될 수 있다.  </p><br/><h2 id="Disk-Scheduling"><a href="#Disk-Scheduling" class="headerlink" title="Disk Scheduling"></a>Disk Scheduling</h2><p>위에서 성능을 보았듯이 disk I&#x2F;O는 millisecond 단위로 동작하기 때문에 비용이 꽤 높은 작업이다. 따라서 전통적으로 OS는 이를 개선하기 위해 I&#x2F;O 작업들의 순서를 결정하는 역할도 같이 했다.  </p><p>Kernel 내부 disk controller에 Disk I&#x2F;O 요청을 담아두는 queue를 두고 이 queue를 보고 알고리즘에 따라 순서를 변경한다.  </p><p>Scheduling에는 여러가지 방식이 있는데 이를 하나하나 상세히 알아보지는 않을 것이다. 이해하는데 난이도도 어렵지 않으며 우리가 Disk scheduling을 걱정할 필요가 없기때문이다. 이는 이미 기술이 다 완성되었다고 판단할 수 있으며 우리가 scheduling 알고리즘을 바꾸거나 할 경우는 없다.  </p><p>Disk Scheduling 알고리즘에는 seek time을 최소화하는 <strong>SSTF</strong>, 엘레베이터 알고리즘과 비슷한 <strong>SCAN</strong> 방식 등이 존재한다.<br>하지만 이들은 head의 움직임에 대한 cost만 고려하지 rotate는 고려하지 않는다. 이들을 해결하기 위한 알고리즘이 <strong>SPTF</strong> 알고리즘이다.  </p><br/><h4 id="SPTF-Shortest-Position-Time-First"><a href="#SPTF-Shortest-Position-Time-First" class="headerlink" title="SPTF(Shortest Position Time First)"></a>SPTF(Shortest Position Time First)</h4><p>SPTF는 어떤 정해진 알고리즘이라는 느낌보다는 그 때의 경우에 따라 적절하게 처리하는 알고리즘이라고 이해하면 쉽다.<br>아래의 예시를 보자.  </p><p align="center">    <img style="max-width: 450px;" src="/images/filesystem/sptf.png"/></p>  <p>현재 head가 sector 30번에 있고 그 다음에 방문해야 하는 sector가 16과 8이 있다. 어느 sector를 먼저 방문해야할까?<br>이는 경우에 따라 다르다.<br>만약 seek time이 rotational delay보다 훨씬 오래걸린다면 sector 16번부터 방문하는 것이 합리적일 것이다. 만약 그 반대인 rotational delay가 훨씬 길면 어떨까? 이 경우에는 sector 8번을 먼저 방문하는 것이 합리적이다.  </p><p>요즘의 disk들은 seek time와 rotational delay가 대략적으로 비슷하다. 그러므로 각각 spec과 상황에 따라 적절하게 결정하는 것이 SPTF의 핵심이다.<br>SPTF는 OS에서 구현하기는 힘들고 device 내부에서 각자의 상황에 맞게 구현되고는 한다.  </p><br/><h4 id="Scheduling은-어디서"><a href="#Scheduling은-어디서" class="headerlink" title="Scheduling은 어디서?"></a>Scheduling은 어디서?</h4><p>Disk Scheduling은 결국 어디서 일어날까?<br>위에서도 살짝 언급했듯이 예전에는 Disk scheduling을 모두 OS에서만 처리했다. OS에서 기다리고 있는 request 들을 queueing 하고 있다가 다음 request를 요청할때 이 queue에서 적절한 request를 선택하여 disk에 요청했다.  </p><p>요즘은 disk 자체가 내부적으로 각자 device에 맞는 뛰어난 scheduling 기법을 내재하고있다. 그리고 OS도 적절하게 OS가 판단한 sector 번호를 disk에 요청하면 disk도 이를 queueing 하고 적절하게 구현한 알고리즘에 따라 처리한다.  </p><p>또 한가지 볼만한 점은 OS 레벨에서 연속된 sector에 대한 요청이 있을시 이들을 merge한다.<br>예를들어 read 요청이 33, 8, 34 순서로 도착했다면 OS는 이를 보고 33, 34을 2개의 sector를 대상으로 하는 1 개의 request로 merge한다. 이는 disk 오버헤드를 크게 줄여준다. </p><br/><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4">https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4</a></li></ul><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      <category domain="https://tk-one.github.io/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/">파일시스템</category>
      
      
      <comments>https://tk-one.github.io/2020/09/05/file-system-hard-disk/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>파일시스템 2편 - RAID</title>
      <link>https://tk-one.github.io/2020/09/05/raid/</link>
      <guid>https://tk-one.github.io/2020/09/05/raid/</guid>
      <pubDate>Fri, 04 Sep 2020 15:51:46 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.&lt;br&gt;이전 포스트의 하드디스크에 이어 이번 포스트에서는 RAID에 대해 알아볼 것입니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 이전의 &lt;a hr</description>
        
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.<br>이전 포스트의 하드디스크에 이어 이번 포스트에서는 RAID에 대해 알아볼 것입니다.  </p><p>이번 편은 이전의 <a href="/2019/07/09/os-computer-architecture/">운영체제 3편 - 컴퓨터 구조와 I&#x2F;O</a> 글의 I&#x2F;O 부분을 이해하고보면 도움이 됩니다.  </p><br/><h2 id="What-is-RAID"><a href="#What-is-RAID" class="headerlink" title="What is RAID?"></a>What is RAID?</h2><p><strong>RAID</strong>는 <strong>Redundant Array of Inexpensive Disks</strong>의 약자이다. 즉 여러개의 disk를 사용해서 더 빠르고, 더 크고, 더 신뢰성 있는 disk 시스템을 구축하는 방식이다.<br>외부적으로 보는 관점에서는 RAID는 그냥 disk와 동일하다. 외부에서는 그저 큰 disk처럼 바라볼 뿐이다. disk와 동일하게 block의 묶음을 읽고 쓸수있다. 하지만 RAID 내부적으로는 시스템을 관리하는 프로세서도 존재하고 메모리도 존재하며 무엇보다 여러개의 disk로 구성되어 있다. </p><p>RAID는 왜 쓰는 것일까?<br>RAID를 사용하면 먼저 성능상으로 이득을 볼 수 있다. 여러개의 disk에 병렬로 I&#x2F;O를 할 수 있다. 그리고 용량(capacity)측면에서도 이득을 볼 수 있다. 한개의 disk 크기를 넘어서는 크기를 저장할 수 있기 때문이다.<br>그리고 신뢰성(reliability)도 더 높은데 RAID를 구성하는 방식에 따라 다를 수 있지만 데이터를 여러 disk에 중복해서 저장함으로서 single disk failure에 대해 극복할 수 있고 외부에서는 single disk failure가 일어나도 아무 일도 일어나지 않은 것처럼 수행할 수 있다.  </p><br/><h2 id="RAID-Internals"><a href="#RAID-Internals" class="headerlink" title="RAID Internals"></a>RAID Internals</h2><p>파일시스템 밑에서 RAID는 그저 매우 크고 신뢰성있고 빠른 disk 처럼 보일뿐이다. single disk처럼 linear array of blocks로 보이고 각 block은 파일시스템에 의해 읽고 쓰여질 수 있다.<br>파일시스템이 RAID에 logical block에 대해 I&#x2F;O 요청을 하면 RAID는 내부적으로 여러 disk를 가지고 있는데 어떤 disk에 접근해서 physical I&#x2F;O를 할 지 결정하고, 수행한 후 그 결과를 반환한다. 이 physical I&#x2F;O를 어떻게 수행할 것인가는 밑에서 볼 RAID level에 따라 다르다.<br>간단하게 RAID가 각 disk의 copy본을 들고있는 mirrored RAID 라고 해보자. 그러면 write을 수행할 때 각 block들을 disk에 2번씩 써주어야 한다.  </p><br/><h2 id="RAID-Evaluation"><a href="#RAID-Evaluation" class="headerlink" title="RAID Evaluation"></a>RAID Evaluation</h2><p>RAID를 구성하는 방법은 여러가지가 있다. 구성하는 방법에 따라 특징 그리고 장단점이 존재하는데 이런 특징을 수치로 측정할 수 있으로면 비교하기 쉽다. 그래서 우리는 3가지 측면에서 RAID의 특징을 측정할 것이다.  </p><h4 id="capacity"><a href="#capacity" class="headerlink" title="capacity"></a>capacity</h4><p>먼저 capacity이다. <code>B</code>개의 block을 저장할 수 있는 <code>N</code>개의 disk가 있다. 클라이언트는 RAID에 얼마나 많은 용량을 저장할 수 있을까?<br>중복없이 저장한다면 <code>N * B</code>가 되겠다. 각 block마다 copy본을 둔다면 <code>N * B / 2</code>가 되겠다.  </p><h4 id="reliability"><a href="#reliability" class="headerlink" title="reliability"></a>reliability</h4><p>두번째는 reliability 즉 신뢰성이다. RAID가 몇개의 disk failure 까지 허용할 수 있을까에 대해 이야기한다.  </p><h4 id="performance"><a href="#performance" class="headerlink" title="performance"></a>performance</h4><p>마지막은 performance이다. Performance는 측정하기 조금 힘들 수 있는데 RAID에 요청되는 작업의 종류에 의존하는 경우가 많기 때문이다.<br>RAID performance를 측정할때는 크게 2가지 측면을 고려할 것인데 첫번째는 <strong>single-request latency</strong>이다. RAID에서 single I&#x2F;O 요청에 대해 어떤 latency를 가지는지를 측명하면 얼마나 해당 RAID가 병렬성을 가지고 있는지 그대로 이해할 수 있다.<br>두번째는 <strong>steady-state throughput</strong> 이다. 어떤 규칙적인 요청이 왔을때 그때의 처리량을 의미한다. 예를들어 여러개의 요청이 동시에 왔을때 그때의 RAID 전체의 bandwidth를 측정할 수 있겠다.<br>이들을 조금 더 자세히 측정하기 위해 각 요청에 대한 내용들이 두가지 종류가 있다고 가정한다. <strong>sequential</strong>과 <strong>random</strong>이다.<br>sequential workload는 요청들이 큰 단위의 연속된 block을 읽는 요청들로 이루어져 있다고 가정한다. 이런 sequential workload는 큰 파일을 읽어 특정 키워드를 찾고싶을때처럼 자주 오는 요청들이다.<br>random workload는 각 요청은 매우 작은 크기의 block을 읽는 요청들이고 각 요청들은 서로 다른 disk location을 대상으로 한다. 예를들어 처음 4KB에 대해 logical address 10에 접근하고 그다음은 logical address 55,000 그 다음은 logical address 4,500 에 접근하는 방식이다. 이런 random workload는 database에서 빈번하게 일어난다.  </p><p>위의 sequential, random workload는 disk의 특성으로 인해 각각 다른 performance 특징을 가진다.<br>sequential access에서는 disk는 가장 효율적인 방식으로 동작한다. seek time과 rotational delay에 매우 적은 시간을 사용하므로 대부분의 시간을 data transfer에 활용할 수 있다.<br>다만 random access의 경우는 대부분의 시간을 seek time과 rotational delay에 사용하므로 상대적으로 data transfer에는 작은 시간을 할애할 수밖에 없다.<br>그래서 이런 차이점을 더 명확히 확인하기 위해 sequential workload 에서는 <code>S MB/s</code> 로 data transfer가 가능하다고 하고, random workload 에서는 <code>R MB/s</code> 속도로 data transfer가 가능하다고 하자. 보통은 S가 R보다 훨씬 크다. 이 측정치를 밑에서 RAID level 별로 계산해보며 성능을 비교해볼 것이다.  </p><p>밑에서는 몇가지 RAID 종류들을 보게될 것이다. <strong>RAID Level 0</strong>(striping), <strong>RAID Level 1</strong>(leveling), <strong>RAID Levels 4, 5</strong>(parity-based redundancy)이다.  </p><br/><h2 id="RAID-Level-0-Striping"><a href="#RAID-Level-0-Striping" class="headerlink" title="RAID Level 0: Striping"></a>RAID Level 0: Striping</h2><p>RAID 0은 <strong>striping</strong>으로 더 잘 알려져 있다. 이 방식은 capacity와 performance 측면에서 높은 결과를 낸다.<br>striping은 말그대로 줄무늬 방식이라고 이해해도 좋다. striping 방식에서는 각 block들을 여러 disk에 걸쳐 줄무늬처럼 배열한다. 다음 그림을 보자.  </p><p align="center">    <img alt="RAID 0: Striping" style="max-width: 350px;" src="/images/filesystem/raid-0.png"/></p>  <p>여기서는 4개의 disk를 사용했다. striping의 기본 아이디어는 block의 array를 라운드로빈 방식으로 disk에 하나씩 할당한다. 이 방식은 large sequential read 요청에 대해 병렬로 처리할 수 있도록 설계한 방식이다.<br>위의 예에서는 1개의 block(4KB) 기준으로 라운드로빈으로 disk에 할당하였는데 다음과 같이 할당할수도 있다.  </p><p align="center">    <img alt="RAID 0: Bigger chunk size" style="max-width: 400px;" src="/images/filesystem/raid-0-bigger.png"/></p><p>여기서는 다음 disk로 넘어가기 전에 2개의 block(8KB)를 할당하고 다음 disk로 넘어갔다. 이 단위를 <strong>chunk size</strong>라고 한다. 여기서는 8KB의 chunk size를 사용하였다. </p><h4 id="Chunk-Size"><a href="#Chunk-Size" class="headerlink" title="Chunk Size"></a>Chunk Size</h4><p>chunk size는 performance에 영향을 많이 끼친다. 작은 chunk size를 사용한다면 많은 파일들이 여러 disk에 striped 되어 저장될 것이다. 그러므로 파일 read write 시에 병렬성을 증가시킬 수 있을것이다.<br>다만 block에 접근하기 위한 positioning time(seek time + rotational delay)가 증가한다. 왜냐하면 여러 disk에 병렬로 읽거나 쓰게될때 결국 완료시간은 가장 오랜시간이 걸린 positioning time에 의해 결정되기 때문이다.  </p><p>chunk size를 크게잡으면 어떨까?<br>작은 크기의 파일에 대해선은 read, write에 병렬성은 떨어질 것이다. 그러나 positioning time이 줄어들 것이다. 만약 작은 크기의 파일의 크기가 chunk size보다 작아 single disk에 저장된다면 그 single disk 에서의 positioning time이 결정한다.  </p><p>그러므로 최적의 chunk size를 찾기 위해서는 어떤 요청들을 위주로 처리할지에 대한 지식이 먼저 있으면 결정하기 좋다.<br>대부분은 큰 chunk size(64KB)를 사용하고는 한다.  </p><h4 id="RAID-0-Evaluation"><a href="#RAID-0-Evaluation" class="headerlink" title="RAID 0 Evaluation"></a>RAID 0 Evaluation</h4><p>RAID 0 에서 capacity, reliability, performance를 측정해보자.<br>먼저 capacity는 간단하다. <code>B</code>개의 block들로 이루어진 <code>N</code>개의 disk가 있다면 <code>N * B</code>의 block을 저장할 수 있다.<br>reliability도 간단한데 striping 방식에서는 reliability가 좋지 않다. 하나의 disk가 fail이 나더라도 바로 data loss로 이어진다.   </p><p>performance는 위에서 본 sequential workload의 <code>S</code>와 random workload의 <code>R</code>을 구해보며 비교해보자.<br>sequential transfer size는 평균적으로 10MB, random transfer size는 평균적으로 10KB라고 가정하자. 이들을 전송하는데 속도가 얼마인지 계산해보자.<br>Disk의 spec은 다음과 같다.  </p><ul><li>Average seek time: 7ms</li><li>Average rotational delay: 3ms</li><li>Transfer rate of disk: 50MB&#x2F;s</li></ul><p><code>S</code>는 <code>(Amound of Data) / (Time to access)</code> 이므로 <code>10MB / (7ms + 3ms + 200ms) = 47.62 MB/s</code> 와 같다. Time to access는 seek time + rotational delay와 10MB를 전송하는데 200ms가 걸리므로 이를 합치면 계산할 수 있다.<br><code>R</code>은 <code>10KB / 10.195ms = 0.981 MB/s</code>이다. 10KB를 전송하는데 0.195ms가 걸린다.<br>이 <code>S</code>와 <code>R</code>은 disk 1개에서 고려한 속도이다.  </p><p>이처럼 striping의 performace를 보게되면 single-block request에 대해서는 single disk 성능과 동일하다.<br>하지만 sequential, random workload 관점에서 볼때 sequential workload는 하나의 disk가 <code>S</code>의 속도를 낼때 <code>N</code>개의 disk가 있다면 전체 처리량은 <code>S * N</code>이 되겠다.<br>randon workload 에서의 전체 처리량은 <code>R * N</code>이 된다. </p><br/><h2 id="RAID-Level-1-Mirroring"><a href="#RAID-Level-1-Mirroring" class="headerlink" title="RAID Level 1: Mirroring"></a>RAID Level 1: Mirroring</h2><p>RAID Level 1은 <strong>mirroring</strong>으로 잘 알려져있다. Mirrored System에서는 각 block에 대해 copy본을 같이 저장함으로서 disk failure를 극복할 수 있다. 전형적인 mirroring 방식은 다음과 같다.  </p><p align="center">    <img alt="RAID 1: Mirroring" style="max-width: 350px;" src="/images/filesystem/raid-1.png"/></p><p>위의 방식에서는 RAID는 물리적으로 두개의 물리적 copy를 저장한다. disk 0은 disk 1과 동일한 내용을 들고있고 disk 2는 disk 3과 동일한 내용을 들고있다. 데이터들은 이 mirror pair들에 걸쳐 striped 된다.<br>disk들에 copy본들을 어디에 위치시킬지는 여러가지 방법이 있는데 위에서 본 방식은 가장 일반적인 방식으로 이런 방식을 <code>RAID-10</code>이라고 부르기도 한다. <strong>stripe of mirror</strong>로 RAID1+0 라는 의미이다.  </p><p>disk read를 할때는 복제본 disk 두개중 어디에서 읽어도 상관없다. 다만 write은 두개의 disk에 모두 써주어야 한다. 이 2번의 write은 병렬로 처리할 수 있다.<br>또 위의 경우 복제본을 2개 저장했는데 이를 <strong>mirroring level</strong>이라고도 한다. 여기서는 mirroring level이 2이다.  </p><h4 id="RAID-1-Evaluation"><a href="#RAID-1-Evaluation" class="headerlink" title="RAID 1 Evaluation"></a>RAID 1 Evaluation</h4><p>RAID 1 에서 capacity를 먼저보자. capacity 관점에서 RAID 1은 비용이 비싸다. mirroring level 2 에서는 전체 용량의 절반만 저장할 수 있으므로 capacity는 <code>N * B / 2</code>가 되겠다.  </p><p>reliability의 관점에서는 좋다. 아무 disk 1개의 failure를 허용할 수 있다. 사실 정확히는 최대 <code>N/2</code>개의 disk failure 까지 허용가능하다. 위에서 본 예제에서도 만약 운이 좋게도 disk 0과 disk 2가 동시에 fail 했다고 하더라도 data loss가 발생하지 않는다. 운이 좋게 각 copy 본을 저장하는 disk가 중복없이 fail이 발생했기 때문이다.  </p><p>performance 관점을 살펴보자. single read request는 single disk와 성능이 동일하다.<br>다만 single write request는 살짝 더 latency가 늘어날 수 있는데, 각 copy 본을 병렬로 write한다고 하더라도 완료되는 시점은 두개의 disk중 더 오래걸린 시간이기 때문이다.  </p><p>steady-state throughput을 살펴보자. sequential write에서는 각 logical write은 두개의 physical write으로 나누어진다. 그러므로 mirroring 방식에서 전체 bandwidth는 <code>(N / 2) * S</code>가 된다. 최대 bandwidth의 절반밖에 안된다.<br>sequential read도 똑같은데 얼핏 생각하면 각 logical read를 모든 disk에 나누어 처리하면 상황이 더 나아질 것 같지만 disk의 물리적 특성을 생각하면 그렇지 않다. 예를들어 위의 그림에서 block 0 부터 7까지 읽는다고 했을때 block 0은 disk 0에서, block 1은 disk 2에서, block 2는 disk 1에서 block 3은 disk 3에서 읽는다고 해보자. disk 0에서는 block 0을 읽고 그다음 block 4를 읽으면 될 것 같지만 어차피 중간에 block 2가 존재하기 때문에 rotation 하면서 이를 지나가야한다. 그러므로 성능향상이 없다. 그러므로 sequential read 에서의 전체 bandwidth도 <code>N / 2) * S</code>와 같다.  </p><p>Random read는 mirroring 방식에서 best case이다. read를 전체 disk에 분산시킬 수 있으므로 <code>N * R MB/s</code>의 대역폭을 가진다.<br>Random write은 sequential write과 동일하게 두개의 physical write로 나누어지므로 <code>(N / 2) * R MB/s</code> 이다.  </p><br/><h2 id="RAID-Level-5-Rotating-Parity"><a href="#RAID-Level-5-Rotating-Parity" class="headerlink" title="RAID Level 5: Rotating Parity"></a>RAID Level 5: Rotating Parity</h2><p>RAID Level 5에서는 parity 정보를 활용한다.<br>Parity bit은 오류가 생겼는지 검사하는 bit 인데 2가지 종류가 존재한다. 짝수 parity와 홀수 parity이다.<br>개념은 간단한데 짝수 parity에서는 데이터의 각 bit의 값에서 parity bit를 포함한 1의 개수가 짝수가 되도록 하는 것이다. 홀수 parity는 홀수가 되도록 하는것이다. 예를들어 다음과 같다.  </p><p align="center">    <img style="max-width: 350px;" src="/images/filesystem/parity.png"/></p><p>짝수 parity라면 C0, C1, C2, C3의 bit에서 1의 개수가 2개이므로 parity bit을 0으로 둔다. 그래야 짝수인 2개로 유지되기 때문이다. parity bit을 활용하면 C0, C1, C2, C3 중 한개가 유실되어도 그 값을 bit 계산으로 알아낼 수 있다.  </p><p>다시 RAID로 돌아와서 RAID Level 5 방식을 그림으로 보자.  </p><p align="center">    <img alt="RAID 5: Rotating Parity" style="max-width: 400px;" src="/images/filesystem/raid-5.png"/></p>  <p>여기서는 parity bit을 disk 별로 돌아가며 설정한다. 즉 위의 block 0, 1, 2, 3의 각 block의 bit를 계산해서 그에 대한 parity bit을 disk 4에 저장한다.  </p><h4 id="RAID-5-Evalutation"><a href="#RAID-5-Evalutation" class="headerlink" title="RAID 5 Evalutation"></a>RAID 5 Evalutation</h4><p>capacity 부터 확인해보자. stripe 당 1개의 parity block을 두기때문에 <code>(N - 1) * B</code>의 용량을 가지게 된다.<br>reliability 는 1개의 disk failure를 허용한다. parity bit를 활용해 recovery 가능하다. 다만 2개이상의 disk failure가 나면 복구할 방법이 없다.<br>그렇다면 performance를 계산해보자.<br>single read request는 1개의 disk로 매핑되기 때문에 single disk와 성능이 동일하다.<br>다만 single write request는 다른데 위 예제에서 block 0에 write을 하려면 block 0을 읽고 parity block도 읽어서 parity bit를 다시 계산해야한다. 즉 read 2번, write 2번이 필요하고 read, write 각각 병렬로 처리할 수 있으므로 single disk의 latency의 약 2배정도 걸린다.  </p><p>sequential read는 parity block으로 인해 <code>(N - 1) * S MB/s</code>의 대역폭을 가진다. sequential write도 parity bit을 같이 써주어야 하므로 <code>(N - 1) * S MB/s</code>가 되겠다.<br>random read는 모든 disk를 활용할 수 있다. 다만 random write은 <code>(N / 4) * R MB/s</code>을 가진다.<br>왜냐하면 만약 random write가 위 예제에서 block 1과 block 10에 대해 요청이 왔다면 parity bit 계산을 위해 disk 1과 disk 4를 읽어 block 1을 처리하고, disk 0과 disk 2를 읽어 block 10을 처리한다. 각 요청안에서 block data write과 parity block write은 병렬로 처리할 수 있므으로 총 4개의 I&#x2F;O 가 필요하다.(disk 1의 read&#x2F;write + disk 0의 read&#x2F;write)  </p><p>RAID Level 별로의 evaluation 결과는 다음과 같다.  </p><p align="center">    <img alt="RAID Evaluation" style="max-width: 700px;" src="/images/filesystem/raid-evaluation.png"/></p><br/><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>reliability는 고려하지 않고 performance가 중요한 상황이라면 RAID 0 striping이 좋은 선택이 될 수 있다.<br>반대로 reliability가 중요하고 random I&#x2F;O 성능이 중요하다면 RAID 1 mirroring이 좋은 선택이다. 다만 비용이 비싸다.<br>capacity와 reliability가 중요하다면 RAID 5 가 좋은 선택이다. 다만 small-write의 성능이 좋지않은면이 있다.<br>만약 sequential I&#x2F;O가 주된 접근이고 capacity를 최대화 해야하는 상황이라면 이 경우에도 RAID 5가 좋은 선택이다.  </p><p>여기서 살펴본 RAID 디자인 말고도 다른 여러가지 디자인이 존재한다. 예를들어 RAID 6은 multiple disk failure를 허용한다.<br>RAID는 hardware 자체로 구현되어 제공되기도 하고 software로도 구현되어 제공될 수 있다. </p><br/><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4">https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4</a></li></ul><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      <category domain="https://tk-one.github.io/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/">파일시스템</category>
      
      
      <comments>https://tk-one.github.io/2020/09/05/raid/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Maven(메이븐) 이란?</title>
      <link>https://tk-one.github.io/2020/03/24/maven-basic/</link>
      <guid>https://tk-one.github.io/2020/03/24/maven-basic/</guid>
      <pubDate>Tue, 24 Mar 2020 12:16:21 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;이 글은 박재성님이 쓰신 &lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&amp;amp;ejkGb=KOR&amp;amp;barcode=9788979148138&quot; rel=&quot;exte</description>
        
      
      
      
      <content:encoded><![CDATA[<p>이 글은 박재성님이 쓰신 <a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&amp;ejkGb=KOR&amp;barcode=9788979148138" rel="external nofollow noopener noreferrer" target="_blank">자바세상의 빌드를 이끄는 메이븐</a>이라는 책과 <a href="https://maven.apache.org/index.html" rel="external nofollow noopener noreferrer" target="_blank">메이븐 공식문서</a>를 보고 정리한 글입니다.<br>책이 있으신분은 메이븐 개념을 한번 머릿속에 정리하고 싶을때 읽으시면 매우 좋습니다. 책에 스토리로 이끌어가는 부분이 있어 매우 재밌게 읽을 수 있습니다.<br>다만, 책이 절판되어 책을 구하고싶으신 분들은 아마 알라딘이나 다른 곳에서 중고서적으로 구매를 하셔야 합니다.  </p><h2 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h2><p>메이븐은 자바기반 프로젝트를 빌드하고 관리하기 위한 툴이다.<br>요즘은 Gradle이 많이 쓰이지만 아직 maven을 사용하고 있는 프로젝트도 많다.<br>메이븐은 빌드 프로세스를 최대한 쉽게 하는것을 목표로 하고 이 뿐만 아니라 프로젝트에 질높은 정보를 제공하고, 단일 빌드시스템을 제공하는 것을 목표로 한다.  </p><p>다른 build tool 없이 자바프로젝트를 개발하게 되면 의존성관리 등 신경써야할게 한두가지가 아니다. 메이븐이 이를 도와준다.<br>메이븐의 장점은 다음과 같다.  </p><ul><li>편리한 의존관계 관리를 지원한다.</li><li>모든 프로젝트가 일관된 프로젝트 디렉토리 구조, 빌드 프로세스를 유지할 수 있다.</li><li>다양한 메이븐의 플러그인을 활용할 수 있다.</li><li>프로젝트의 template을 만들수있다.</li></ul><p>메이븐은 저장소를 지원해서 메이븐만 설치하면 프로젝트 build에 필요한 라이브러리, plugin을 저장소에서 우리의 PC로 자동으로 다운로드한다. 다운로드한 라이브러리들은 특정 디렉터리에 위치하게 되는데 이를 localRepository(로컬저장소)라고 부른다. 기본적으로는 <code>~/.m2/repository</code> 에 위치하고 settings.xml로 설정을 변경할 수도 있다.<br>또 메이븐은 처음 생성하는 프로젝트 종류에 따라 기반이 되는 template을 제공한다. 이를 이용해서 메이븐 기반 프로젝트를 생성할 수 있는데, 그러면 프로젝트의 기본적인 뼈대를 자동으로 생성할 수 있다. 메이븐의 이 같은 기능을 archetype이라고 한다.  </p><p>메이븐 공식문서의 <a href="https://maven.apache.org/guides/getting-started/" rel="external nofollow noopener noreferrer" target="_blank">getting started</a>에서 처음 메이븐 프로젝트를 만들게 될때도 archetype을 사용한다.<br>다음 명령어로 메이븐 프로젝트를 생성해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn -B archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4</span><br></pre></td></tr></table></figure></p><p>이를 실행하면 <code>my-app</code> 프로젝트의 디렉터리가 만들어지고 그 안에 pom.xml 파일이 생성된다.<br>메이븐은 source code와 test code를 분리해서 관리하는데 source code는 src/main/java 에 위치하고, test code는 src/test/java 에 위치한다.<br>여기서 사용한 groupId, artifactId는 뒷부분에서 다루겠다.  </p><h2 id="메이븐-기본명령어"><a href="#메이븐-기본명령어" class="headerlink" title="메이븐 기본명령어"></a>메이븐 기본명령어</h2><p>메이븐 명령어는 다음과 같은 형태를 가진다.<br><code>mvn [options] [goal] [phase]</code>위의 명령어에서도 사용했던 -D 옵션들은 메이븐 설정파일(pom.xml)에 인자를 전달한다.<br>예를 들어 단위테스트를 실행하지 않으려면 <code>mvn -Dmaven.test.skip=true [&lt;phase&gt;]</code>와 같이 실행할 수 있다.메이븐에는 phase와 goal 개념이 있는데 이들을 이용하며 빌드를 실행할 수 있고, 빌드를 실행할 때 여러개의 phase와 goal을 실행할 수 있다. 예를들어 다음과 같이 다양한 형태로 실행이 가능하다.<br><code>mvn clean test</code>: clean phase와 test phase를 실행한다.<br><code>mvn clean compiler:compile</code>: clean phase와 compiler plugin의 compile goal을 실행한다.<br>phase와 goal은 밑에서 자세히 다루겠다.  </p><h2 id="Pom-xml"><a href="#Pom-xml" class="headerlink" title="Pom.xml"></a>Pom.xml</h2><p>위의 메이븐 archetype:generate 명령어로 메이븐 프로젝트를 생성했으면 pom.xml 파일이 생성된다. POM은 Project Object Model을 의미한다. 그러면 pom.xml의 각 element 들을 살펴본다.  </p><p>먼저 생성된 pom.xml은 다음과 같다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;my-app&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;maven-app&lt;/name&gt;</span><br><span class="line">  &lt;!-- FIXME change it to the project&apos;s website --&gt;</span><br><span class="line">  &lt;url&gt;http://www.example.com&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;</span><br><span class="line">    &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.11&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">  .. 이하생략 ..</span><br></pre></td></tr></table></figure></p><ul><li>project: pom.xml의 최상위 element</li><li>modelVersion: POM version. 최근버전이 4.0.0이다.</li><li>artifactId: project를 식별하는 id를 의미한다. groupId 안에서 여러개의 project가 있을 수 있다. </li><li>groupId: project를 생성하는 조직의 고유 id를 결정한다. 도메인 이름을 많이 사용하는데 꼭 그럴필요는 없다.<br>groupId + artifactId는 값이 유일해야한다. 그렇지 않으면 중앙저장소에서 충돌한다.</li><li>packaging: 어떤 방식으로 패키징할지 결정한다. jar, war 등을 설정가능하다.</li><li>version: project의 현재 버전을 의미한다. 프로젝트 개발중에는 SNAPSHOT을 suffix로 사용가능하다.<br>SNAPSHOT은 maven의 예약어이며 SNAPSHOT을 사용하면 라이브러리를 다른방식으로 관리한다.</li><li>name: project 이름이다.</li><li>url: project url이 있다면 이를 기입한다.</li><li>dependencies: 프로젝트와 의존관계에 있는 라이브러리들을 관리한다.</li></ul><p>각 프로젝트의 pom.xml은 기본적으로 최상위 POM이라고 불리는 설정을 상속한다. 그래서 pom.xml의 설정내용이 단순하더라도 메이븐의 기본 규약들을 전부다 따르는 것이 가능하다. 실제 정의된 설정들을 보려면 다음 명령어를 사용하면 된다.<br><br><code>mvn help:effective-pom</code><br><br>설정되어있는 repository 정보를 담고있는 repositories 태그, plugin 설정정보를 담는 태그등 기존 pom.xml에 보이지 않았던 태그들을 볼 수 있다. 이 내용들이 기본적으로 최상위 POM에 존재했기 때문에 우리가 만든 project의 pom.xml은 단순하게 가져갈 수 있다.  </p><h2 id="Lifecycle"><a href="#Lifecycle" class="headerlink" title="Lifecycle"></a>Lifecycle</h2><p>메이븐은 모든 빌드 단위가 이미 정의되어 있으며 이는 개발자가 임의로 변경할 수가 없다.여기서 말하는 빌드 단위란 compile, test, package, deploy 등을 말한다.<br>메이븐은 이와같이 미리 정의되어 있는 빌드 순서를 lifecycle이라고 하며 메이븐은 3개의 lifecycle을 제공한다.</p><ol><li>compile, test, package, deploy를 담당하는 기본 lifecycle</li><li>빌드 결과물 제거를 위한 clean lifecycle</li><li>project document site를 생성하는 site lifecycle이다.</li></ol><p>메이븐은 기본적으로 빌드후의 모든 산출물을 target 디렉터리에서 관리한다.  </p><p>target 디렉터리에 생성되는 하위디렉터리는 다음과 같다.  </p><ul><li>target/classes: src/main/java의 소스코드가 컴파일된 class 파일들과 src/main/resources 디렉터리의 자원이 복사된다.</li><li>target/test-classes: src/test/java의 소스코드가 컴파일된 class 파일들과 src/test/resources 디렉터리의 자원이 복사된다.</li><li>target/surefire-reports: report 문서들이 위치한다.</li></ul><h4 id="기본-Lifecycle"><a href="#기본-Lifecycle" class="headerlink" title="기본 Lifecycle"></a>기본 Lifecycle</h4><p>기본 lifecycle을 활용해 source code를 compile, test 등을 할 수 있는데 각 phase들을 살펴보면 다음과 같다.  </p><ul><li>process-resources: src/main/resources의 모든 자원을 target/classes 로 복사한다.</li><li>compile: src/main/java에 있는 source code를 compile한다.</li><li>process-test-resources: src/test/resources의 모든 자원을 target/test-classes 로 복사한다.</li><li>test-compile: src/test/java에 있는 source code를 compile한다. </li><li>test: Junit 같은 unit test framework로 test를 진행하고 test가 실패하면 빌드실패로 간주한다.<br>결과물을 target/surefire-reports 디렉터리에 생성한다. </li><li>package: pom.xml의 packaging 값에 따라 압축한다.(jar, war)</li><li>install: local repository에 압축한 파일을 배포한다.</li><li>deploy: 원격저장소에 압축한 파일을 배포한다.</li></ul><p>이처럼 maven 기본 lifecycle은 여러개의 phase로 구성되어 있으며, 각 phase는 의존관계를 가진다.<br>process-resources ← compile ← process-test-resources ← test-compile ← test ← package<br>이 순으로 의존관계를 가지고 있어서 package phase를 실행(mvn package)하면 의존관계에 있는 test phase가 먼저 실행되고, test phase는 compile phase에 의존관계가 있기때문에 compile phase가 먼저 실행된다.<br>따라서 package phase를 실행하면 process-resources → compile → process-test-resources → test-compile → test → package 순으로 빌드가 진행된다.  </p><p>process-resources phase는 src/main/resources 에 있는 모든 자원을 test/classes 디렉터리로 복사하는데, 만약 다른 디렉터리에도 자원이 존재한다면 pom.xml에 따로 설정할 수 있다.  </p><p>package phase는 jar나 war형태로 압축하여 target 디렉터리에 위치시킨다.<br><code>&lt;build&gt;/&lt;finalName&gt;</code>에 값이 설정되어 있으면 {finalName}.{packaging} 형태로 압축파일이 생기고,<br>값이 설정 안되어있다면 {artifactId}-{version}.{packaging} 형태로 된다.  </p><p>clean phase는 빌드한 결과물들을 제거하는 phase인데 이는 다른 phase와 관련이 없다.<br>clean phase를 실행하지 않고 다른 phase를 실행할 때 불필요한 산출물들 때문에 오류가 날 수 있으므로 clean을 실행하고 빌드하는 습관을 가지면 좋다.  </p><h4 id="Clean-Lifecycle"><a href="#Clean-Lifecycle" class="headerlink" title="Clean Lifecycle"></a>Clean Lifecycle</h4><p>clean lifecycle은 빌드를 통해 나온 산출물을 모두 삭제한다.<br>target directory를 삭제하는 것과 동일하다.</p><h4 id="Site-Lifecycle"><a href="#Site-Lifecycle" class="headerlink" title="Site Lifecycle"></a>Site Lifecycle</h4><p>site lifecycle은 사용안하는 경우가 많은데 핵심만 짚고 넘어가자면,<br>site, site-deploy phase를 사용해 실행가능하다. site lifecycle은 메이븐에 설정되어 있는 기본설정, 플러그인 설정에 따라 target/site directory에 문서 사이트를 생성한다. site-deploy는 이를 배포한다.  </p><h2 id="Plugin"><a href="#Plugin" class="headerlink" title="Plugin"></a>Plugin</h2><p>메이븐에서 제공하는 모든 기능은 plugin을 기반으로 동작한다.<br>메이븐 phase 또한 메이븐 plugin을 통하여 실질적인 작업이 실행된다. 따라서 phase가 실행되는 과정을 이해하려면 maven plugin을 먼저 이해해야 한다.  </p><p>사용하고자 하는 maven plugin이 있다면 pom.xml에 다음과 같이 설정한다.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;project&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1&lt;/version&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p>이와 같이 사용하고자 하는 plugin의 groupId, artifactId, version을 명시하면 된다.<br>version을 생략하면 가장 최신버전의 plugin이 설치된다.<br>메이븐 plugin은 하나의 plugin에서 여러 작업을 실행할 수 있도록 지원하는데 여기서 각각 실행할 수 있는 작업을 goal이라고 정의한다.<br>위의 compiler plugin은 하나지만 이 플러그인이 지원하는 goal은 compile(source directory의 compile), testCompile(test directory의 compile) 등이 있다.  </p><p>plugin은 다음과 같이 실행할 수 있다.<br><code>mvn groupId:artifactId:version:goal</code>  </p><p>예를들어 앞의 compiler plugin의 compile goal은 다음과 같이 실행한다.<br><code>mvn org.apache.maven.plugins:maven-compiler-plugin:2.1:compile</code>  </p><p>만약 settings.xml에 pluginGroup이 설정이 되어있다면 groupId를 생략이 가능하고,<br>version을 생략하면 local repository에 있는 가장 최신 버전의 플러그인을 사용하며,<br>plugin 이름이 <code>maven-$name-plugin</code> 이나 <code>$name-maven-plugin</code> 형식을 따른다면 $name 값만 명시할 수 있다.<br>앞에서 실행했던 compile 플러그인을 다음과 같이 실행할 수 있다.<br><code>mvn compiler:compile</code></p><p>앞부분에서 실행했던 <code>mvn archetype:generate</code> 명령도 <code>mvn org.apache.maven.plugins:maven-archetype-plugin:generate</code>를 축약한 것이다.  </p><p>메이븐은 매우 많은 플러그인들을 활용할 수 있는게 큰 장점이다. 다양한 플러그인을 제공하고 있어서 원하는 개발환경을 얼마든지 만들어 나갈 수 있다.</p><h2 id="Phase와-Goal"><a href="#Phase와-Goal" class="headerlink" title="Phase와 Goal"></a>Phase와 Goal</h2><p>메이븐에서 phase는 build lifecycle에서 각 단계와 순서를 정의하는 개념으로 실제로 빌드작업을 하지는 않는다.<br>실제 빌드작업은 해당 phase와 연결되어 있는 plugin의 goal에서 진행한다.<br><code>mvn compile</code>은 compile phase를 실행한 것인데 이는 compile phase와 연결되어 있는 compiler plugin의 compile goal이 실행되면서 컴파일 작업을 진행한다.<br>기본 lifecycle에서 phase를 실행할 때 기본으로 연결된 plugin의 goal을 실행하는 구조이다.<br>기본 lifecycle에서 phase에 연결되어 있는 plugin을 실행할 때에는 자동으로 메이븐 중앙저장소에서 plugin을 다운로드 한다.  </p><p>phase와 goal과의 관계를 보여주는 그림이다.  </p><p align="center">    <img alt="phase와 goal의 관계" src="/images/maven-phase-goal.jpeg"></p><p>각 핵심 phase 별로 구체적인 내용을 알아보자.  </p><h4 id="mvn-compile"><a href="#mvn-compile" class="headerlink" title="mvn compile"></a>mvn compile</h4><p>compile phase를 실행하면 먼저 의존관계에 있는 process-resources phase가 먼저 실행된다. process-resources phase는 src/main/resources 디렉터리에 있는 모든 자원을 target/classes 디렉터리로 복사한다.<br>만약 src/main/java 안에서도 소스와 같은 패키지로 관리하는 리소스들이 있고 이들또한 target/classes에 복사되기를 원한다면 다음과 같은 설정을 하면된다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;project&gt;</span><br><span class="line">  &lt;resources&gt;</span><br><span class="line">    &lt;resource&gt;</span><br><span class="line">      &lt;directory&gt;src/main/resources&lt;/directory&gt;</span><br><span class="line">    &lt;/resource&gt;</span><br><span class="line">    &lt;resource&gt;</span><br><span class="line">      &lt;directory&gt;src/main/java&lt;/directory&gt;</span><br><span class="line">      &lt;excludes&gt;</span><br><span class="line">        &lt;exclude&gt;**/*.java&lt;/exclude&gt;</span><br><span class="line">      &lt;/excludes&gt;</span><br><span class="line">    &lt;/resource&gt;</span><br><span class="line">  &lt;/resources&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p>그러면 compile phase 실행시 src/main/java에 있는 <code>*.java</code> 파일을 제외한 모든 설정파일을 target/classes 로 복사한다.<br>resource plugin과 compiler plugin 에 대한 자세한 정보는 다음 공식문서에서 확인할 수 있다.<br>resources plugin : <a href="https://maven.apache.org/plugins/maven-resources-plugin/" rel="external nofollow noopener noreferrer" target="_blank">resources-plugin</a><br>compiler plugin : <a href="https://maven.apache.org/plugins/maven-compiler-plugin/" rel="external nofollow noopener noreferrer" target="_blank">compiler-plugin</a>  </p><h4 id="mvn-test"><a href="#mvn-test" class="headerlink" title="mvn test"></a>mvn test</h4><p>test phase를 실행하면 process-test-resources phase가 먼저 실행되면서 src/test/resources 디렉터리의 자원복사를 먼저 진행한다.<br>그리고 test-compile phase에서 src/test/java 디렉터리의 test code들을 컴파일한다.<br>test phase는 target/test-classes 에 컴파일한 단위 테스트 클래스를 실행하고 그 결과물을 target/surefire-reports 디렉터리에 생성한다.<br>기본적으로 test phase는 target/test-classes 에 있는 모든 단위 테스트 클래스를 실행하는데 특정 테스트 suite 별로 실행할 필요가 있다면 test option을 사용할 수 있다.  </p><p><code>mvn -Dtest=MyUnitTest test</code><br><br>이와 같이 특정 테스트 클래스만 실행할 수 있고 여러개의 test 클래스 들을 실행하고 싶다면 쉼표로 여러개를 정의하면된다.  </p><h4 id="mvn-package"><a href="#mvn-package" class="headerlink" title="mvn package"></a>mvn package</h4><p>package phase는 compile, test-compile, test, package 순으로 실행된 후 jar, war 파일이 target 디렉터리 하위에 생성된다.<br><code>&lt;build&gt;/&lt;fileName&gt;</code> 에 값이 설정되어 있고 jar로 패키징을 하게되면 {finalName}.jar 형태로 jar 파일이 생성된다. 만약 finalName element가 설정되어 있지 않다면 {artifactId}-{version}.{packaging} 이 압축파일 그리고 디렉터리 이름이 된다.<br>예를들어, finalName element가 설정되어있지 않고 pom.xml 설정이 다음과 같다면<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;project&gt;</span><br><span class="line">  &lt;groupId&gt;io.github.tk-one&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;myapp&lt;/artifactId&gt;</span><br><span class="line">  &lt;packaging&gt;war&lt;/packaging&gt;</span><br><span class="line">  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p>파일은 다음 위치에 생성된다.<br><code>target/myapp-1.0-SNAPSHOT/myapp-1.0-SNAPSHOT.war</code>  </p><h4 id="mvn-install"><a href="#mvn-install" class="headerlink" title="mvn install"></a>mvn install</h4><p>install phase는 package phase와 의존관계에 있기때문에 package phase를 먼저 실행한다. package phase에서 jar or war 파일로 압축을 완료하면 이를 local repository에 배포한다.</p><h4 id="mvn-deploy"><a href="#mvn-deploy" class="headerlink" title="mvn deploy"></a>mvn deploy</h4><p>deploy phase는 jar or war 파일을 원격저장소에 등록한다.</p><p><br>  </p><h2 id="라이브러리-의존관계"><a href="#라이브러리-의존관계" class="headerlink" title="라이브러리 의존관계"></a>라이브러리 의존관계</h2><p>메이븐은 의존관계에 있는 라이브러리를 관리하기 위해 의존 라이브러리 관리기능을 제공한다. 이는 메이븐의 lifecycle과 더불어 메이븐의 핵심기능이기 때문에 반드시 이해하는게 좋다.  </p><p>메이븐 저장소는 로컬저장소와 원격저장소로 나뉜다.  </p><h4 id="로컬저장소"><a href="#로컬저장소" class="headerlink" title="로컬저장소"></a>로컬저장소</h4><p>로컬저장소는 개발자 PC에 있는 저장소로 메이븐을 빌드할때 다운로드하는 라이브러리나 플러그인을 관리 및 저장한다.<br>로컬저장소는 기본값으로는 <code>~/.m2/repository</code> 에 위치한다.  </p><h4 id="원격저장소"><a href="#원격저장소" class="headerlink" title="원격저장소"></a>원격저장소</h4><p>원격저장소는 외부에 위치하는 저장소로 사내에서 사용하는 저장소도 있고 중앙저장소라고 불리는 오픈소스 라이브러리나, 메이븐 플러그인 등을 저장하고 있는 저장소도 있다. 중앙저장소는 원격저장소 중 하나라고 생각하면 된다.  </p><p>메이븐은 빌드를 할때 로컬저장소에 이미 다운로드한 라이브러리가 있으면 원격저장소에서 다운로드 하지않고 로컬저장소에 있는 라이브러리르 사용한다. 메이븐이 다운로드 하고자 하는 저장소는 repositories 태그로 설정할 수 있다.<br>기본적으로 우리가 repositories 태그로 설정을 안하고 다운로드할 수 있는 이유는 최상위 POM에 이미 정의가 되어있기 때문이다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;project&gt;</span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;central&lt;/id&gt;</span><br><span class="line">      &lt;name&gt;Central Repository&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p>여러개의 repository들을 추가할 수 있는데 그러면 메이븐은 라이브러리를 다운로드할때 repositories 태그에 있는 저장소 순서대로 다운로드를 시도한다.  </p><p>위에서 생성한 myapp에서는 다음과 같이 dependencies 태그로 라이브러리를 관리한다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">  &lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.11&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">  &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure></p><p>이와 같이 설정하고 빌드하면 메이븐은 먼저 로컬저장소에 해당 라이브러리가 있는지 확인한다.<br>없다면 메이븐은 중앙저장소에서 junit 4.11 버전이 있는지 확인하고 있다면 jar 파일을 로컬저장소에 다운로드한다.<br>중앙저장소에 해당 라이브러리와 버전이 존재하는지 확인할 때에는 위에 repository 설정에 적힌 url을 바라보고<br><a href="https://repo.maven.apache.org/maven2/junit/junit/4.11/junit-4.11.jar" rel="external nofollow noopener noreferrer" target="_blank">https://repo.maven.apache.org/maven2/junit/junit/4.11/junit-4.11.jar</a> 파일이 있는지를 파악한다.<br>이와 같이 설정하고 빌드하면 메이븐은 중앙저장소에서 junit 4.11 버전의 jar 파일을 로컬저장소에 다운로드한다.<br>로컬저장소에 다운로드 받는 위치는 기본적으로 다음과 같다.<br>~/.m2/repository/junit/junit/4.11/junit-4.11.jar  </p><p>그리고 메이븐은 로컬저장소에 다운로드한 라이브러리를 활용해 src/main/java 그리고 src/test/java 에 있는 source code들을 컴파일한다.  </p><p>version을 LATEST 혹은 RELEASE로 설정할 수도 있는데 그러면 항상 가장 최신버전의 라이브러리와 의존관계를 갖게된다.<br>또, 한번 로컬저장소에 다운로드한 라이브러리는 다시 원격저장소에서 다운로드하지 않는데 이부분에서 애플리케이션이 개발단계에 있어 코드가 지속적으로 변경되는 상황이라면 SNAPSHOT을 활용하자.<br>version 정보에 SNAPSHOT을 포함하게되면 빌드할 때마다 가장 최근에 배포한 라이브러리가 있는지 확인하고 로컬저장소에 있는것보다 최신일경우 이를 다운로드한다.  </p><h4 id="scope"><a href="#scope" class="headerlink" title="scope"></a>scope</h4><p>메이븐에서는 사용하는 라이브러리의 성격에 따라 scope를 지정할 수 있다.<br>JUnit 라이브러리의 경우 실제 배포할때는 필요없고 테스트를 진행할때만 필요하다. 이런경우 scope를 test로 주면된다.  </p><p>scope는 6가지 종류가 있다.  </p><ul><li>compile: default scope이다. compile 및 deploy시 같이 제공해야하는 라이브러리이다.</li><li>provided: compile 시점엔 필요하지만 deploy에 포함할 필요는 없는경우 사용한다.</li><li>runtime: compile에는 필요없지만 runtime에는 필요한 경우 사용한다.</li><li>test: test 시점에만 사용하는 라이브러리에 설정한다.</li><li>system: provided scope와 비슷한데 로컬저장소에서 관리되는 jar파일이 아닌 우리가 직접 jar 파일을 제공해야한다.</li><li>import: 다른 pom.xml 에 정의되어있는 의존관계설정을 가져온다.  </li></ul><h2 id="Dependency-Mechanism"><a href="#Dependency-Mechanism" class="headerlink" title="Dependency Mechanism"></a>Dependency Mechanism</h2><p>Dependency Mechanism은 메이븐의 핵심중 하나이다.<br>그러므로 메이븐이 어떻게 의존성을 관리하는지는 꼭 이해하고 넘어가는게 좋다.  </p><h4 id="Dependency-Transitive-의존성-전이"><a href="#Dependency-Transitive-의존성-전이" class="headerlink" title="Dependency Transitive(의존성 전이)"></a>Dependency Transitive(의존성 전이)</h4><p>프로젝트에서 의존하는 라이브러리들의 숫자는 제한이 없지만 의존성 cycle이 있으면 문제가 발생한다.<br>프로젝트에 외부 라이브러리를 하나씩 추가할때마다 그 라이브러리가 또 의존하고있는 라이브러리를 또 추가해야 하므로 의존관계에 있는 라이브러리 숫자가 증가한다.<br>예를들어, project A가 B, C에 의존하고있다면 B가 의존하고있는 D, E, F 라이브러리가 또 필요할 것이고 또 D 가 의존하는 G 라이브러리도 필요할 것이다. 연쇄작용으로 의존하는 프로젝트는 점점 커진다.<br>참고로 메이븐은 의존성이 있는 라이브러리가 또 어떤 라이브러리에 의존성을 가지고있는지 알기위해 jar 파일을 다운로드 하는 동시에 해당 라이브러리의 pom파일도 같이 다운로드 한다.<br>메이븐은 위처럼 프로젝트 라이브러리 숫자가 급격히 증가하는 문제점을 해결하기 위해 라이브러리 제한이 가능하도록 의존성 전이 설정을 지원한다. </p><ul><li><p>Dependency mediation: 같은 의존성의 여러버전을 마주치게 되었을때 artifact의 어떤 버전을 사용할지 결정한다.<br>메이븐은 이때 더 가까운 의존관계에 있는 버전의 의존관계를 선택한다.<br>예를들어 다음과 같은 의존성이 있다고 가정한다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  A</span><br><span class="line">├── B</span><br><span class="line">│   └── C</span><br><span class="line">│       └── D 2.0</span><br><span class="line">└── E</span><br><span class="line">    └── D 1.0</span><br></pre></td></tr></table></figure><p>이 예에서는 A를 build할때 D 1.0이 사용된다. 왜냐하면 A -&gt; B -&gt; C -&gt; D 2.0 보다 A -&gt; E -&gt; D 1.0 이 더 가깝기 때문이다.<br>여기서 서로 depth 가 같은 상황이라면 먼저 명시된 라이브러리의 버전이 사용된다.<br>만약 project A의 pom.xml에 직접 version을 적어주면 그 버전을 사용한다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  A</span><br><span class="line">├── B</span><br><span class="line">│   └── C</span><br><span class="line">│       └── D 2.0</span><br><span class="line">├── E</span><br><span class="line">│   └── D 1.0</span><br><span class="line">│</span><br><span class="line">└── D 2.0</span><br></pre></td></tr></table></figure><p>이처럼 A에 직접 D 2.0 의 의존성을 추가하면 D 2.0 을 사용한다.  </p></li><li><p>Dependency management: 메이븐의 <code>&lt;dependencyManagement&gt;</code> element로 의존관계에 있는 artifact의 버전을 직접 명시랄 수 있다.</p></li><li>Dependency scope: 현재 빌드상태에 맞는 라이브러리만 의존관계를 포함한다.<br>즉, test scope를 가지는 경우 최종 배포산출물을 빌드하는 시점에는 포함되지 않는다.  </li><li>Excluded dependencies: 만약 A -&gt; B -&gt; C와 같이 의존성이 있으면 project A에서 명시적으로 project C에 대한 의존성을 <code>&lt;exclusion&gt;</code>태그를 사용해 명시적으로 제외시킬 수 있다.</li><li>Optional dependencies: 만약 A -&gt; B -&gt; C와 같이 의존성이 있고 project B에 C가 optional로 설정이 되어있으면 project A를 빌드할때 project C에 대한 의존관계를 가지지 않는다.  </li></ul><p>메이븐의 Dependency Transitive가 의존관계를 최대한 잘 설정해 주겠지만 pom.xml에 항상 라이브러리의 명확한 version을 명시하는게 좋다.<br>현재 프로젝트에서 의존하고있는 라이브러리의 tree를 보고싶다면 다음 plugin의 goal을 사용하면 좋다.<br><code>mvn dependency:tree</code>  </p><h2 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h2><p>pom.xml에서 발생하는 중복설정은 속성(property)를 정의하여 개선할 수 있다.<br>보통 공통된 버전관리에 많이 사용하고는 하는데 예제를 보면 바로 이해할 수 있다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;project&gt;</span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;spring.version&gt;3.0.1.RELEASE&lt;/spring.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spring-core&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;spring.version&#125;&gt;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p>속성은 <code>&lt;properties&gt;</code> element에서 <code>&lt;property.name&gt;value&lt;/property.name&gt;</code> 형태로 정의한다.<br>그리고 이렇게 정의한 내용은 pom.xml 파일내에서 <code>${property.name}</code> 으로 접근할 수 있다.<br><br> <br> </p>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/maven/">maven</category>
      
      
      <category domain="https://tk-one.github.io/tags/maven/">maven</category>
      
      
      <comments>https://tk-one.github.io/2020/03/24/maven-basic/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>인터넷 메일시스템 (SMTP)</title>
      <link>https://tk-one.github.io/2020/01/03/smtp/</link>
      <guid>https://tk-one.github.io/2020/01/03/smtp/</guid>
      <pubDate>Fri, 03 Jan 2020 10:44:29 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;이 포스트에서는 Application Layer의 &lt;strong&gt;SMTP (Simple Mail Transfer Protocol)&lt;/strong&gt; 에 대해 알아본다.&lt;br&gt;먼저, SMTP를 보기전에 인터넷 전자메일 시스템이 </description>
        
      
      
      
      <content:encoded><![CDATA[<p><br></p><p>이 포스트에서는 Application Layer의 <strong>SMTP (Simple Mail Transfer Protocol)</strong> 에 대해 알아본다.<br>먼저, SMTP를 보기전에 인터넷 전자메일 시스템이 어떤식으로 동작하는지 알고있어야 한다.  </p><h2 id="Mail-System"><a href="#Mail-System" class="headerlink" title="Mail System"></a>Mail System</h2><p>인터넷 메일 시스템은 크게 <strong>user agent, 메일서버, SMTP</strong> 이 3가지 요소로 구성되어 있다.<br>아래의 그림을 보면서 이해하면 쉽다.</p><p><img src="/images/mail_system_topology.png" alt="mail_system"></p><ul><li><p><strong>user agent</strong><br>MS의 Outlook을 생각하면 쉽다. user agent는 사용자가 메일을 읽고, 작성하고, 전송할 수 있도록 해준다.</p></li><li><p><strong>메일서버</strong><br>사용자가 메일 작성을 끝내면 user agent는 메시지를 메일서버로 보내게되고, 여기서 메시지는 메일서버의 output 메시지큐에 들어가게 된다.<br>여기서의 메일서버는 송신자의 메일서버를 의미한다.<br>송신자의 메일서버에서 수신자의 메일서버로 메시지가 전송되면, 메일들은 수신자의 메일서버안의 <strong>메일박스(mailbox)</strong>안에 저장되고 유지된다.<br>만약, 수신자의 메일서버가 다운된 상황에서 송신자가 메일을 보내면 어떻게 될까?<br>송신자가 메일을 전송하면, 먼저 메일이 송신자의 메일서버에 도착한다. 그리고 송신자의 메일서버는 메일을 수신자의 메일서버로 전송할 수 없을때, 메시지 큐(message queue)에 보관하고 있다가, 주기적으로 메일전송을 시도한다.</p></li><li><p><strong>SMTP</strong><br>SMTP는 Application layer에서 작동하는 메일전송 프로토콜이다. 위의 메일서버 설명에서, 한 메일서버에서 다른 메일서버로 메시지(메일)을 전송할때 사용하는 프로토콜이 SMTP이다.<br>이뿐만 아니라, 송신자의 user agent에서 본인의 메일서버로 메일을 전송할때도 SMTP가 사용된다.<br>SMTP는 TCP위에서 작동한다. 참고로 SMTP는 HTTP보다 훨씬 더 오래전부터 사용되었다.</p></li></ul><p>이 설명을 기반으로 메일을 전송하는 간단한 시나리오를 보자.<br>A가 B에게 메일을 전송하는 상황이다.  </p><ol><li>A가 user agent를 통해 B에게 메일 내용을 작성하고 전송버튼을 누른다.</li><li>A의 user agent는 메시지를 A의 메일서버에 보내게 되고, 메시지는 메일서버의 output message queue에 위치한다.</li><li>A의 메일서버에서 동작하는 SMTP 클라이언트는 output message queue에 쌓여있는 메시지를 B의 메일서버로 전송하기 위해 먼저 TCP연결을 맺는다.</li><li>TCP가 맺어진 후, SMTP 핸드쉐이킹을 하고 SMTP 프로토콜에 따라 B의 메일서버로 전송한다.</li><li>B의 메일서버는 메시지를 수신한 후, 그 메시지를 B의 메일박스(mailbox)에 놓는다.</li><li>B는 이후에 user agent를 실행하여 메일을 읽을 수 있다.</li></ol><p>인터넷 메일 시스템은 대충 이런식으로 작동한다.<br>(2번에서 A의 user agent가 메시지를 A의 메일서버로 보낸다고 되어있는데, 사실 여기서도 SMTP 프로토콜을 통해 전달된다.)<br>그러면 이제 SMTP를 좀 더 자세히 보도록 한다.</p><h2 id="SMTP-Simple-Mail-Transfer-Protocol"><a href="#SMTP-Simple-Mail-Transfer-Protocol" class="headerlink" title="SMTP (Simple Mail Transfer Protocol)"></a>SMTP (Simple Mail Transfer Protocol)</h2><p>대부분의 Application layer protocol 처럼 SMTP는 송신자의 메일서버에서 수행하는 클라이언트와, 수신자의 메일서버에서 수행되는 서버를 가지고 있다. 메일서버가 상대 메일서버로 전송할때는 SMTP의 클라이언트로 동작하는 것이고, 메일서버가 상대 메일서버로 부터 메일을 받을때는 SMTP 서버로 동작하는 것이다. HTTP를 떠올리면 쉽다.  </p><p>메일서버에서 상대 메일서버로 메일을 보내는 상황에서, 먼저 클라이언트 SMTP는 서버 SMTP의 25번 포트로 TCP연결을 맺는다. 만약 서버가 죽어있으면 클라이언트는 나중에 다시 시도한다.<br>TCP 연결이 맺어지면, 클라이언트와 서버는 SMTP 핸드쉐이킹을 수행한다. 이 SMTP 핸드쉐이킹 과정에서 클라이언트는 송신자와 수신자의 email 주소를 제공한다.  </p><p>핸드쉐이킹 과정을 마치면, 클라이언트는 메시지를 보낸다.  </p><p>SMTP 클라이언트와 SMTP 서버 사이의 메시지 전달과정을 예를들어 살펴보자.<br>클라이언트 호스트네임은 github.io 이고, 서버 호스트네임은 korea.ac.kr 이라고 하자.<br>C는 클라, S는 서버를 나타내고 TCP 연결 직후의 상황을 가정한다.<br>메일 내용은 “Hello, this is TK-one. Can I know the result of the interview?” 이다.</p><p><code>S: 220 korea.ac.krC: HELO github.ioS: 250 Hello github.io, pleased to meet youC: MAIL FROM: <a href="mailto:&#116;&#107;&#45;&#111;&#x6e;&#x65;&#64;&#103;&#105;&#116;&#104;&#x75;&#98;&#x2e;&#105;&#111;">&#116;&#107;&#45;&#111;&#x6e;&#x65;&#64;&#103;&#105;&#116;&#104;&#x75;&#98;&#x2e;&#105;&#111;</a>S: 250 <a href="mailto:tk-one@github.io" rel="external nofollow noopener noreferrer" target="_blank">tk-one@github.io</a> … Sender okC: RCPT TO: <a href="mailto:&#x6b;&#105;&#109;&#x40;&#x6b;&#111;&#114;&#101;&#97;&#x2e;&#x61;&#x63;&#46;&#107;&#114;">&#x6b;&#105;&#109;&#x40;&#x6b;&#111;&#114;&#101;&#97;&#x2e;&#x61;&#x63;&#46;&#107;&#114;</a>S: 250 <a href="mailto:kim@korea.ac.kr" rel="external nofollow noopener noreferrer" target="_blank">kim@korea.ac.kr</a> … Recipient okC: DATAS: 354 Enter mail, end with “.” on a line by itselfC: Hello, this is TK-one. (메일내용)C: Can I know the result of the interview? (메일내용)C: .S: 250 Message accepted for deliveryC: QUITS: 221 korea.ac.kr closing connection</code></p><p>클라이언트는 5개의 명령(<strong>HELO, MAIL FROM, RCPT TO, DATA, QUIT</strong>)을 내리며 하나의 점(.)으로 된 라인을 송신하면 이는 메시지의 끝을 의미한다.<br>서버는 각 명령에 대해 답하며, 각 응답은 응답코드와 옵션 설명을 갖고 있다.<br>그리고 주의할 점이 있는데, SMTP는 메시지의 body와 header를 포함하여 전부 7bit ASCII 코드로 작성되어야 한다.<br>HTTP는 이런 제한이 없는 반면, SMTP는 한글이나 binary 데이터 처럼 ASCII가 아닌 문자를 포함한다면 반드시 이 메시지는 전송되기 전에 7bit ASCII로 인코딩이 되어야한다.  </p><p>이렇게 SMTP를 통해 한 메일서버에서 다른 메일서버로 메시지가 전달된다.<br>그렇다면 수신자는 자신의 PC에서 user agent를 통해 자신의 메일서버에 있는 메시지들을 어떻게 얻을 수 있을까?<br>수신자의 user agent는 메일을 가져오기위해 SMTP를 사용할 수는 없다. 왜냐하면 SMTP는 푸시(push)용 프로토콜인 반면, 메시지를 가져오는 것은 풀(pull) 동작이기 때문이다.<br>여기서는 메일서버로부터 자신의 user agent로 메시지를 가져오기 위해 특별한 메일 접속 프로토콜을 사용한다. 이들중엔 <strong>POP3(Post Office Protocol - Version 3), IMAP(Internet Mail Access Protocol), HTTP</strong> 등이 있다.<br><br><br>참고: <a href="https://www.amazon.com/Computer-Networking-Top-Down-Approach-7th/dp/0133594149" rel="external nofollow noopener noreferrer" target="_blank">Computer Networking - A Top-Down approach</a></p>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/network/">network</category>
      
      
      <category domain="https://tk-one.github.io/tags/smtp/">smtp</category>
      
      
      <comments>https://tk-one.github.io/2020/01/03/smtp/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>시스템 버스(System Bus)란?</title>
      <link>https://tk-one.github.io/2019/08/09/system-bus/</link>
      <guid>https://tk-one.github.io/2019/08/09/system-bus/</guid>
      <pubDate>Fri, 09 Aug 2019 14:53:02 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;System Bus는 간단하게 digital data를 이동시키기 위한 통로이다.  &lt;/p&gt;
&lt;p&gt;Bus에는 3가지 종류가 있다. &lt;strong&gt;Control Bus&lt;/strong&gt;, &lt;strong&gt;Address Bus&lt;/strong&gt;, &lt;str</description>
        
      
      
      
      <content:encoded><![CDATA[<p>System Bus는 간단하게 digital data를 이동시키기 위한 통로이다.  </p><p>Bus에는 3가지 종류가 있다. <strong>Control Bus</strong>, <strong>Address Bus</strong>, <strong>Data Bus</strong> 이다. 이 3가지가 System Bus를 구성한다.  </p><p align="center">    <img style="max-width: 400px" alt="System Bus" src="/images/system-bus.png"/></p><p>시스템버스는 internal bus로서 프로세서와 내부 internal 하드웨어 장치들과 연결하도록 고안된 버스이다. 시스템버스는 메인보드에 존재한다.  </p><p>Bus의 종류를 하나씩 보자.  </p><h2 id="Control-Bus"><a href="#Control-Bus" class="headerlink" title="Control Bus"></a>Control Bus</h2><p>Control Bus는 CPU가 다른 internal device들과 통신하는데 사용된다. Control Bus는 이름부터 알 수 있듯이 CPU의 command를 전달하고 device의 status signal을 반환한다. Control Bus 안에는 line 이라는 것이 있는데, control bus마다 line의 개수와 종류가 각각 다르지만 공통으로 가지고 있는 line이 있다.  </p><ul><li>READ line: device가 CPU에게 읽히면 active된다.  </li><li>WRITE line: device가 CPU에게 쓰여지고 있으면 active된다.</li></ul><p>메모리에 읽고 쓸때에는 Control Bus의 Read, Write line이 활성화된다.<br>Control Bus는 양방향으로 작동한다.  </p><h2 id="Address-Bus"><a href="#Address-Bus" class="headerlink" title="Address Bus"></a>Address Bus</h2><p>Address Bus는 memory에 읽고 쓸때 memory의 physical address를 전달하는데 사용한다. Memory에 read, write를 할때 address bus에는 memory location이 담긴다. read, write 할 메모리 값 그 자체는 밑에서 볼 data bus에 실린다.<br>Address Bus는 Memory Address Register와 연결되어 있으며, 단방향이다.<br>Address bus는 중요한게 bus width(폭)가 시스템이 다룰 수 있는 address를 결정한다. 예를들어, 32bit address bus라면 시스템은 최대 2<sup>32</sup>가지의 주소공간을 다룰 수 있겠다. Byte-addressable이면 가능한 memory space는 4GB가 되겠다.<br>다만 모든 경우에 address bus의 width가 꼭 시스템이 사용하는 address와 동일하게 매칭되는 것은 아니다.<br>CPU 칩에서 pin수는 엄청난 비용이다. 따라서 예전 시스템들에서는 16 bit address bus를 사용하지만 32 bit address space를 사용할 수 있다. 다만 이 경우 16bit로 주소를 나누어 두번에 걸쳐 전송해야한다.  </p><h2 id="Data-Bus"><a href="#Data-Bus" class="headerlink" title="Data Bus"></a>Data Bus</h2><p>Data Bus는 데이터를 전송하는데 사용하는 버스이다. Data Bus는 당연히 읽고 쓸수 있어야 하므로 양방향이다.  </p><h2 id="32bit-64bit-CPU"><a href="#32bit-64bit-CPU" class="headerlink" title="32bit, 64bit CPU"></a>32bit, 64bit CPU</h2><p>32bit 운영체제, 64bit 운영체제를 결정하는 것은 무엇일까? 이는 data bus가 결정하지 않는다. 이를 결정하는것은 프로세서의 정수 register 크기이다. Data bus의 폭은 정수 register와 다를수도 있다. 예전 컴퓨터 머신들의 초기설계에는 data bus의 폭과 정수 register의 크기는 같았으나 꼭 그럴 필요는 없다.  </p><p>실제로 8080 IBM PC는 16bit CPU이나 data bus는 8bit width이다. 그러므로 프로세서의 register에서 RAM으로 전송하려면 8bit씩 두번을 전송해야 했다.  </p><p>그리고 프로세서의 정수 register와 address bus의 폭도 다를 수 있다. address bus의 폭은 이보다 더 클수도 작을 수도 있다.  </p><p>실제로 original AMD Operaton은 64bit 시스템이지만 address bus는 memory subsystem을 단순화하기위해 40bit로 설계되었다. 64bit가 주소로 전부 활용되기는 현실적으로 힘들기 때문이다.<br>지금 현재의 대부분의 64bit AMD CPU도 48bit의 address bus를 가지고있다.  </p><h2 id="Modern-designs"><a href="#Modern-designs" class="headerlink" title="Modern designs"></a>Modern designs</h2><p>현재의 디자인들은 우리가 잘 알고있는 bit 기반의 paradigm과는 맞지않는 더 복잡한 bus들을 사용한다. Modern CPU들은 매우 복잡하게 설계되어 있으며 이들은 memory 및 다른 CPU들과의 통신을 위해 특수한 bus들을 사용하기도 한다. 더이상의 단일로 존재하는 address bus나 data bus는 없으며 우리가 알고있는 bit-width 기반으로 처리하는 방식이 아닌 다른 signaling 방식으로 작동할 수도 있다.  </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://en.wikipedia.org/wiki/Control_bus">https://en.wikipedia.org/wiki/Control_bus</a></li><li><a href="https://superuser.com/questions/446395/is-it-the-address-bus-size-or-the-data-bus-size-that-determines-8-bit-16-bit">https://superuser.com/questions/446395/is-it-the-address-bus-size-or-the-data-bus-size-that-determines-8-bit-16-bit</a></li></ul><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0/">컴퓨터구조</category>
      
      
      <category domain="https://tk-one.github.io/tags/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0/">컴퓨터구조</category>
      
      
      <comments>https://tk-one.github.io/2019/08/09/system-bus/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>운영체제 9편 - 메모리</title>
      <link>https://tk-one.github.io/2019/08/08/os-memory/</link>
      <guid>https://tk-one.github.io/2019/08/08/os-memory/</guid>
      <pubDate>Thu, 08 Aug 2019 14:07:51 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.&lt;br&gt;맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 메모리에 대한 내용입니다.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.<br>맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  </p><p>이번 편은 메모리에 대한 내용입니다.  </p><span id="more"></span><h2 id="서론"><a href="#서론" class="headerlink" title="서론"></a>서론</h2><p>우리 핸드폰에서 가장 비싼 부품이 무엇일까? 첫번째는 CPU, 두번째로는 LCD 패널, 그 다음이 메모리다. CPU는 가장 비싸다. 이를 놀게만들수는 없다. 결국 범용 컴퓨터의 목적은 CPU 활용률(utilization)을 극대화 하는게 중요하다.  </p><p>이전의 작성한 운영체제편들에서 사용자에게 빠른 응답을 제공하기위한 멀티프로그래밍도 알아보았고 이를 위해 스케줄링하는 것도 알게되었다. 여러 프로그램이 concurrent하게 돌아가다 보니 동기화 라는 것도 알게되었다.  </p><p>하지만 여러 프로그램이 동시에 메모리에 적재되어 실행되면서 메모리를 공유해야할 필요성이 생겼다. 메모리가 아무리 커진다고 하더라도 메모리관리는 필요하다. 여러개의 프로세스가 동시에 뜰 수 있기 때문이다. 어떻게 이를 관리할 수 있을까?  </p><h2 id="주소공간-Address-space"><a href="#주소공간-Address-space" class="headerlink" title="주소공간(Address space)"></a>주소공간(Address space)</h2><p>주소공간은 프로세스에서 참조할 수 있는 주소들의 범위이다. 주소공간은 프로세스와 1대1로 매핑되며 스레드들은 이를 공유한다.  </p><p>주소공간의 크기는 CPU의 address bus에 의존한다. <a href="/2019/08/09/system-bus/">Address Bus에 대해 다룬 글</a>이 있으니 참고하면 좋겠다.  </p><p>예를들어 address bus가 32bit width이면 주소자체의 개수는 2<sup>32</sup>개 있고 byte-addressable 이면 주소공간의 크기는 4GB가 되겠다. 만약 word-addressable이고 word가 16bit이면 주소공간의 크기는 8GB가 된다.<br>byte-addressable 이라는 것은 최소로 읽어낼 수 있는 단위가 byte인 것이다. 즉 모든 byte 단위들로 주소를 가지고 있는 것이다.<br>word-addressable이고 word가 16bit이라면 char 타입을 저장할때 나머지 8bit는 비어있게된다. 이런면에서 memory 효율화 차이가 있지만 이런 내용들은 컴파일러가 적절하게 최적화할 수 있다.<br>요즘은 프로세서들은 byte-addressable 하지않다. 대부분 work-addressable 하므로 한번 읽으면 4byte를 한번에 읽는다.  </p><p>또 메모리는 Memory alignment 라고하여, 메모리 주소로 접근할 때 word size에 대해서 align하는 것으로 제한한다. 즉 word size가 4byte이고 byte-addressable이라면 메모리 주소로 접근할때 반드시 4의 배수로 접근해야 한다는 것이다. 그렇지 않으면 trap이 발생한다. </p><h2 id="물리주소와-가상주소"><a href="#물리주소와-가상주소" class="headerlink" title="물리주소와 가상주소"></a>물리주소와 가상주소</h2><p>물리주소라는 것은 메모리를 접근할 때 사용되는 주소다. Address Bus로 메모리에 주소가 들어갈때는 다 물리주소로 들어간다. 메모리 자체는 수동적인 device이다. 주소가 들어오면 그냥 주소를 읽어들이는 것 뿐이다.<br>가상주소는 프로세스 관점에서 사용하는 주소이다. 우리가 이미 사용하고 있는 변수접근도 이미 가상주소를 사용하여 접근하고 있다. Logical한 주소공간이기 때문에 주소공간을 의미있는 단위로 나누어 사용할 수 있다.<br>이전에 다루었던 내용에서 프로세스의 메모리구조를 보았을때 text segment, data segment, stack 등으로 공간을 나누었었다. 이들은 가상주소를 의미있는 단위로 나누어 사용한 것 뿐이다.  </p><h4 id="Compile-amp-Link-time에서의-주소"><a href="#Compile-amp-Link-time에서의-주소" class="headerlink" title="Compile &amp; Link time에서의 주소"></a>Compile &amp; Link time에서의 주소</h4><p>Compile과 linking에서 다양한 논리주소들이 생성된다. Compiler는 symbol table을 만드는 것이 핵심이다. 예를 들면, 프로그래머들은 변수로 전부 이해할 수 있는 이름을 사용한다. int a, b에서 a, b 모두 symbol이다. 각 symbol이 가리키고 있는 메모리 주소를 map으로 가지고있는게 symbol table이다.<br>다만 이 symbol table에서의 주소는 relative한 주소이다. 오직 Object file 안에서만 유효한 주소이다. 주소는 0부터 시작하고 이후 relocate할 수 있다. 상대적인 위치만 정해놓은 것이다.<br>Linking을 하게되면 하나의 바이너리가 만들어진다. Linker가 하는것은 address resolution이다. 앞에서 만들어낸 relative한 주소를 relative하지 않게 만든다. 모든 Object 파일들과 라이브러리들을 묶어서 symbol table에 의존적이지 않은 주소를 만들어 낸다. 주소를 0부터 쫙 매긴다. 각각 들어가는 모듈들을 시작하는 주소에 상대적으로 다 더해줘서 주소를 모두 만들어낸다. 여기서 만들어지는게 가상주소이다. 이것이 프로그램이 사용하는 주소가 된다.<br>프로그램의 수행을 위해 Loader는 executable을 메모리로 load 한다. 메모리에 올라가면 그때 또 프로그램이 가지고있는 주소와 물리주소간에 연결을 시켜주어야 한다. 이를 위해 binding을 한다. 프로그램이 가지고 있는 주소와 물리주소를 연결시킨다.<br>Load가 다시 진행되면 똑같은 executable이 다른 물리주소로 가게될 수 있다. 그리고 런타임시에도 밑에서 자세히 보겠지만 swapping과 paging을 통해 물리메모리에서 내려갔다가 올라갈때 주소가 바뀔 수 있다.  </p><p align="center">    <img style="max-width: 400px" alt="다양한 논리주소의 생성" src="/images/logical-address-creation.png"/></p><p>이야기가 길어졌는데 결국 하고자 하는 이야기는 Compile time, Link time, load 과정에서의 주소와, runtime 할때의 주소가 따로 있다는 것이다. General purpose 한 운영체제에서는 보통 이런방법을 사용한다.  </p><h2 id="초창기-컴퓨터에서의-주소관리"><a href="#초창기-컴퓨터에서의-주소관리" class="headerlink" title="초창기 컴퓨터에서의 주소관리"></a>초창기 컴퓨터에서의 주소관리</h2><p>초기의 시스템에서는 아주 간단한 주소변환만 했다.<br>주소에 base register에 있는 값을 더한다. 그리고 이 값을 바로 물리주소로 사용했다. base register를 프로그램마다 바꾸어 주는 방식이다. 즉, 각 시작번지를 바꾸는 방법이다.  </p><p align="center">    <img alt="초창기 시스템의 주소변환" src="/images/address-translation-old.png"/></p><p>이렇게 하는방법은 오버헤드가 적었다. 그냥 base register에 더하기만 하면 되었다. 다만 약점도 명확했는데, 프로세스가 반드시 continuous 해야 동작했다. 즉 한덩어리로 연결이 되어있어야 한다. linear 해야한다는 의미이다.<br>이를 극복하기 위해 virtual memory개념을 만들고 이를 translation을 하는 방법을 생각해냈다. 지금은 전부 이런 memory translation 을 전제로 하고있다. 가상주소를 변환하고 그것을 가지고 물리메모리에 접근한다.<br>리눅스 Kernel이 virtual translation과 physical memory 주소로의 변환을 모두 다룬다.  </p><p align="center">    <img alt="주소변환" src="/images/address-translation.png"/></p><p>여기에 DMA가 붙으면 어디에 붙어야할까?<br>DMA는 I&#x2F;O를 대신해준다. I&#x2F;O 데이터를 읽어온 것을 메모리에 써주고, 또는 I&#x2F;O에 쓸 것을 메모리에서 가져와서 대신 write 해준다.<br>DMA를 위 그림에 넣으려한다. DMA가 받아들이는 주소는 Virtual Address일까 Physical Address일까?<br>보통은 DMA는 Physical Address를 받는다. DMA중에 Virtual 주소를 받는것도 있긴하다. 이를 DVMA라고 부른다. 다만 일반적인 General purpose PC에서의 DMA에서는 물리주소를 받는다.  </p><h2 id="MMU-Memory-Management-Unit"><a href="#MMU-Memory-Management-Unit" class="headerlink" title="MMU(Memory Management Unit)"></a>MMU(Memory Management Unit)</h2><p>위에서 virtual address를 physical address로 변환하기 위해서는 translation을 해야한다고 했다. 다만 이 translation 속도가 굉장히 중요하다. 잘못하면 memory 접근속도가 반으로 떨어질 수 있다. 이 성능을 높이기 위해서는 software로는 더이상은 한계가 있다는 것을 알고 하드웨어를 도입했다. 이것이 MMU(Memory Management Unit)이다.<br>CPU는 그냥 주소를 MMU에 보낸다. MMU는 이를 physical address로 변환한다.<br>MMU는 CPU 칩안에 붙어있으며, address bus에는 physical address가 실린다.  </p><p align="center">    <img alt="MMU" src="/images/mmu.png"/></p><p>위의 구조에서 CPU 캐시는 어디에 위치할까? 캐시는 translation 하기 전에 작동한다. 그 이유는 캐시는 빨리 접근하는 것이 중요하다. 캐시 hit이 되면 이를 translate 할 필요가 없으므로 더 빠르게 값을 얻을 수 있다. 즉 캐시는 virtual address를 보고있다. 다만 virtual address는 그 유효범위가 프로세스에서만 유효하다. Context Switching이 일어나면 캐시를 전부 flush 시켜야 한다. virtual address 이기 때문에 서로 다른 프로세스가 virtual address가 같지만 physical address는 다를 수 있기때문이다. 다만 또 어떤 디자인에서는 캐시를 physical address에 붙이는 것도 존재하여 항상 그런것은 아니다.<br>이 CPU 캐시는 Kernel이 flush 해주어야 한다. 이를 flush할 수 있는 instruction이 존재한다. 다만 이 또한 연구가 계속 되고있으므로 언제든 변할 수 있다.  </p><h2 id="가상메모리-Virtual-Memory"><a href="#가상메모리-Virtual-Memory" class="headerlink" title="가상메모리(Virtual Memory)"></a>가상메모리(Virtual Memory)</h2><p>가상메모리는 실제로 존재하지 않지만 사용자에게 메모리로서 역할을 한다. 가상 메모리를 생각한 아이디어는 다음과 같다.<br>물리메모리를 가지고 어떻게 이를 관리할 것인가에 대해 초점이 맞추는게 아닌, virtual memory가 있다고 가정을 하고 사용자들에게 그냥 virtual memory를 가지고 마음껏 사용할 수 있도록 하자는 것이 핵심이다.<br>프로세스가 수행되기 위해서 프로그램의 모든 부분이 물리메모리에 있을 필요가 없다는 것이다. 결국 프로세스는 현재 실행하고있는 code&#x2F;data&#x2F;stack 부분만 물리메모리에 있으면 실행가능하다.  </p><h4 id="Paging"><a href="#Paging" class="headerlink" title="Paging"></a>Paging</h4><p>메모리 주소공간을 동일한 크기인 page 라는 단위로 나누어 관리한다. 이를 <strong>Paging</strong> 이라고 한다.<br>보통 1 page의 크기를 4KB로 한다. 보통 Page Frame이라고 하면 물리메모리를 고정된 크기로 나누었을때 하나의 블록을 의미한다. Page 라고 하면 보통 가상메모리의 블록을 의미한다. Frame과 Page의 크기는 동일하다. 이 둘을 잘 구분해서 이해할 수 있도록 하자. </p><p>Page가 하나의 Frame을 할당받으면 물리메모리에 위치하게 된다. Frame을 할당받지 못한 Page들은 외부 저장장치(Backing Store)에 저장된다. Backing Store도 Page, Frame과 같은 크기로 나누어져 있다.  </p><p>CPU가 관리하는 모든 주소는 두 부분으로 나뉜다. page 번호와 page offset이다.<br>page 번호는 각 프로세스가 가진 페이지 각각에 앞에서 부터 부여된 번호이다. 예를들어 1번 프로세스는 0부터 63번까지의 페이지를 가지고 있다.<br>Page offset은 각 페이지 안에서의 내부주소를 가리킨다. offset은 page가 4KB이니 0부터 4095까지 존재한다.<br>Page 번호와 offset으로 모든 주소를 표현할 수 있다. 예를들어 1번 프로세스의 12번 페이지(page 번호)의 34번째(offset) 데이터로 표현할 수 있다.  </p><p>잘 이해했는지 확인하기 위해 퀴즈를 풀어보자.  </p><ol><li>128MB의 물리메모리를 4KB 단위로 페이징하려면 몇개의 frame이 필요할까?  </li><li>4GB의 logical address를 페이징하려고 하면 총 몇개의 page가 필요한가? (페이지 크기는 4KB)  </li><li>page 크기가 4KB일때, 한 페이지의 메모리를 access 하기 위한 주소 bit는 몇 bit인가?</li></ol><p>해답은 다음과 같다.  </p><ol><li>2<sup>27</sup> &#x2F; 2<sup>12</sup> &#x3D; 2<sup>15</sup>개 (약 32,000개)  </li><li>2<sup>32</sup> &#x2F; 2<sup>12</sup> &#x3D; 2<sup>20</sup>개 (약 1,000,000개)</li><li>4KB 크기이므로 12bit이다.</li></ol><h4 id="가상메모리-mapping"><a href="#가상메모리-mapping" class="headerlink" title="가상메모리 mapping"></a>가상메모리 mapping</h4><p align="center">    <img alt="가상메모리 mapping" src="/images/virtual-address-mapping.png"/></p><p>실제 물리 메모리에는 이렇게 그림처럼 올라간다. 어떤 조합으로 올라갈것인가는 운영체제의 몫이다.<br>예를들면 위 그림에서 P3에 있는 page를 하나 더 물리메모리에 올리고싶다고 하자. 근데 현재는 물리메모리가 모두 사용중이다. 새로운 page를 올리기 위해 이미 올라가있던 Frame을 빼서 외부 저장장치에 임시로 저장해놓는다. 나중에 이 Page가 다시 필요하면 다시 가져와서 다시 메모리에 넣는다. 그리고 이를 반복한다.  </p><p>서로 다른시간 t1, t2가 있다고 했을때 동일한 page frame에 대하여 t1과 t2에 특정 page frame에는 서로 다른 virtual page가 올라갈 수 있다.</p><h2 id="Page-Table"><a href="#Page-Table" class="headerlink" title="Page Table"></a>Page Table</h2><p>Page Table은 virtual memory와 physical memory 사이의 매핑이다. 각 프로세스의 페이지 정보를 저장한다. 그러므로 프로세스마다 하나의 page table을 가진다.<br>테이블의 인덱스는 페이지 번호이다. table 내용으로는 물리 frame의 시작주소가 있다. 이 시작주소와 페이지 offset을 결합하여 원하는 데이터가 있는 물리 메모리 주소를 알 수 있다.<br>page 크기가 4KB이면 offset이 12bit이므로 <code>page(20 bit) + offset(12 bit)</code>가 된다.<br>그러므로 물리 frame 번호만 변환하고 뒤에 offset을 붙이면 물리메모리 주소를 알 수 있다.  </p><p align="center">    <img alt="가상메모리와 물리메모리 사이의 paging 모델" src="/images/paging-model.png"/></p><p>Page table은 Kernel이 관리하고 있는 data structure이다. Page table은 물리메모리 상에 저장이 되어있다. Page-table base register가 현재 수행중인 프로세스의 물리메모리 내 page table의 위치를 가리키고 있다. Context Switching이 일어나면 이 register의 값을 PCB에 저장한다. page table의 위치를 저장하는 것이다. 새로운 프로세스가 스케줄링되면 Kernel이 register의 값을 변경해주어야 한다.  </p><p>결국 이를 위해서는 Kernel이 프로세스의 page table 위치를 알고있어야하는데, Page Table은 프로세스가 맨 처음 만들어질때 생성된다.  </p><p align="center">    <img alt="Page table을 이용한 주소변환" src="/images/page-table-translation.png"/></p><p>그림의 p는 페이지의 인덱스이며 page 번호이다. 이 page가 매핑된 값을 앞에 20bit에 넣고 offset은 그대로 뒤에 붙여준다. 그러면 정확한 물리메모리의 address가 나온다.<br>이 작업을 MMU가 한다. MMU는 하드웨어이다. 왜 이걸 MMU가 할까?<br>page table은 Kernel data structure로 memory에 있다. memory에 있다는 것은 page table에 접근할때 memory 접근 후, 이를 읽어낸 후 다시 이를 보고 physical 주소로 변환해야한다. 그러면 메모리 접근을 10번하려면 20번을 메모리 접근해야한다. 당연 성능이 좋지않다. 그래서 이를 하드웨어로 하자는 이야기로 MMU를 사용한다. MMU는 밑에서 다시 볼 것이다.  </p><h2 id="Page-Table-Entry"><a href="#Page-Table-Entry" class="headerlink" title="Page Table Entry"></a>Page Table Entry</h2><p>page table 내부를 살펴보자. page table을 통해서 메모리 접근이 다 일어나므로 매우 중요하다. page 별로 메모리에 올리고 내리고 하는게 가능하다. 이를 위해 page table이 어떻게 구현되어있는지가 핵심적이다.<br>Page Table Entry란 Page table에 있는 하나의 record-page를 의미한다. 간단하게 PTE라고 한다. PTE는 page에 대한 접근이 있었는지, 사용이 되는지의 여부, page가 바뀌었는지의 여부를 저장할 수 있다.<br>PTE의 대략적인 필드내용은 다음과 같다.  </p><ul><li><strong>Page base Address</strong><br>해당 페이지에 할당된 프레임의 시작주소이다. 이를 통해 물리메모리에 접근할 수 있다.  </li><li><strong>Flag bits</strong><br>Accessed bit: 페이지에 대한 접근이 있었는지에 대한 bit<br>Dirty bit: 페이지 내용의 변경이 있었는지에 대한 bit<br>Present bit: 현재 페이지에 할당된 Frame이 있는지에 대한 bit<br>Read&#x2F;Write bit: 읽기&#x2F;쓰기에 대한 권한표시 bit</li></ul><p>실제로 프로세스가 작업하는 페이지는 물리메모리에 없을 수 있다. 이를 해결하기 위해 Present bit이 필요하다.<br>Dirty bit은 밑에서 볼 내용이지만 page가 page-out 되고, 다시 frame을 할당받았을때 dirty bit가 마킹이 안되어있으면 이전과 같은내용으로 다시 page-out이 될 때 I&#x2F;O로 다시 써줄 필요가 없으므로 성능상 이득을 볼 수 있다.  </p><p>Segmentaion fault의 발생은 언제 일어날까? 예를들어 <code>*p = 1</code>의 코드가 있는데 p가 NULL, 즉 0인 경우에 발생한다. 프로세스가 만들어 질때 첫번째 page는 읽어서도 안되고 써서도 안된다고 표시를 한다. 정확히는 fork 할때 page table도 copy를 하므로 똑같이 page 0번의 PTE에 Read&#x2F;Write bit을 마킹을 해준다. 그래서 NULL pointer에 접근할때 0번의 PTE에 접근시 trap이 발생하여 segmentation fault가 발생한다.  </p><p>이 Page table과 PTE를 보면 예전 프로세스에서 말했던 프로세스는 protection domain이라는 것을 이해할 수 있다. 프로세스 A는 프로세스 B의 page table에 접근할 수 없기 때문이다. 이는 MMU가 접근을 시켜주지 않는다. 다른 프로세스의 메모리에 접근할 수 없다는 것에 대해 이런 배경이 숨겨져 있다.  </p><h2 id="TLB"><a href="#TLB" class="headerlink" title="TLB"></a>TLB</h2><p>Computer Science의 magic 2가지가 있다. 하나는 caching이고 또 하나는 indirection이다.<br>성능이 안나오면 caching을 하는 경우가 많다. 그리고 뭐든지 복잡하면 layering을 한다. 그러므로 항상 computer science를 공부할때에는 여기서는 어떤 indirection을 사용하고 있는지 보면 도움이 될때가 많다.<br>캐싱을 사용하지 않으면 페이징 방법에서는 데이터로의 접근이 항상 두번의 메모리 접근을 거쳐야 한다. Page table에 한번 그리고 물리메모리에 한번이다. 이것이 메모리 접근 속도를 떨어뜨린다.<br>이를 극복하기 위해 MMU도 caching을 사용한다. 이것이 <strong>TLB</strong>(<strong>Translation Look-aside Buffers</strong>)이다.  </p><p>Page table을 이용해 변환된 주소를 TLB에 저장해둔다. 그리고 다음 접근시에 TLB에 hit이 된다면 TLB에 저장되어있는 값을 이용하여 빠르게 변환된 주소를 얻는다. TLB는 register이기 때문에 빠른 수행이 가능하다. TLB hit ratio를 높여야 전체적인 메모리 접근속도를 높일 수 있다. miss가 나면 원래처럼 page table을 물리메모리에서 다시 조회해야한다.  </p><p>TLB entry는 보통 16개에서 512개 정도이다. 이미 우리가 사용하는 PC들은 모두 MMU가 붙어있다.<br>TLB를 이용한 paging을 그림으로 나타내면 다음과 같다.  </p><p align="center">    <img alt="TLB를 이용한 페이징" src="/images/tlb-paging.png"/></p>  <h2 id="MultiLevel-Page-Table"><a href="#MultiLevel-Page-Table" class="headerlink" title="MultiLevel Page Table"></a>MultiLevel Page Table</h2><p>Page Table의 크기는 얼마일까? PTE 하나는 보통 4 byte의 크기를 가진다. 그러므로 page table의 크기는 4MB이다. 크기가 작다고 생각할 수 있겠지만 프로세스가 100개라면 400MB가 된다.<br>우리가 만든 프로그램이 있다고 해보자. 이 프로그램에서 사용하는 page의 개수는 몇개일까?<br>segment별로 생각해보자. stack은 보통 프로그램에서 32KB를 넘어가지를 않는다. text segment는 코드가 1만 라인이라면 Page 10개를 넘지 못한다. text segment도 40KB를 넘기기 힘들다는 것이다. data segment도 보통은 작은 크기를 가진다. 결국 우리가 만든 프로그램은 PTE(page table entry)가 100개를 넘는 프로그램을 만들기 힘들다는 것이다.  </p><p>Page table의 PTE는 총 2<sup>20</sup>개이다. 약 100만개이다. 그러면 보통 프로그램에서는 이 100만개의 PTE중에 100개를 사용한다는 것은 비율상 0.01%개의 PTE만 사용된다는 것이다. Kernel 메모리 낭비가 굉장히 심하다.  </p><p>이를 어떻게 메모리 사용량을 줄일수 있을까 하여 Multi-Level Page Table이 나오게 되었다. Intel은 거의다 2-level page table을 사용한다. page table 자체도 paging된 공간에 저장된다.  </p><h4 id="2-Level-Page-Table"><a href="#2-Level-Page-Table" class="headerlink" title="2-Level Page Table"></a>2-Level Page Table</h4><p>Outer page table을 한개 더 두어서 이들이 page table들을 가르키도록 한다. 여기서 말한 outer page table은 level-1 page table이 된다.<br>예를들어, 20-bit를 차지하고 있는 page number를 다시 아래와 같이 나눈다.<br><code>10-bit page 번호 + 10-bit page 주소</code><br>결국 32bit 주소의 모양은 다음과 같아진다.  </p><p align="center">    <img alt="2-Level Page Table 32bit 주소" src="/images/2-level-page-table-example.png"/></p><p>그러면 level-1 page table에는 1024개의 entry가 들어간다. 이 각각의 entry가 level-2 page table을 가리키는 구조이다. 이 level-2 page table에도 1024개의 entry가 존재하게 된다.  </p><p align="center">    <img alt="2-Level Page Table 매핑 예시" src="/images/2-level-page-mapping.png"/></p><p>결국 level-1 page table entry 중 1개가 mapping 하고 있는 virtual memory의 크기는 4MB이다.<br>만약 page table entry의 4개만 사용하고있는 프로세스는 page table에 얼마의 메모리가 필요할까?<br>level-1 page table은 4KB가 필요하고, level-2 page table은 3개를 사용하고 있다고 가정하자. 최대 4개가 사용될 수 있지만, 2개의 page는 동일한 level-2 page table 에 있다고 하자.<br>그러면 총 4개의 page table이 사용되므로 16KB가 필요하다.  </p><h4 id="Page-Walk"><a href="#Page-Walk" class="headerlink" title="Page Walk"></a>Page Walk</h4><p>MMU가 page table에 접근하는 것을 <strong>Page Walk</strong>라는 표현을 사용한다(Table Walk라고도 부른다). 처음에 outer page table을 보고 그리고 level-2 page table을 보고 그 다음 물리메모리 주소를 찾는다. 다음은 2-Level page table 에서의 page walk 예시다.  </p><p align="center">    <img alt="2-Level Page Walk" src="/images/page-walk.png"/></p><p>page walk는 모든 메모리에 접근할때마다 발생한다.<br>만약 3-Level page table을 도입하면 어떻게될까? 이는 page size들을 더 줄일 수 있겠지만 그만큼 성능에 대한 tradeoff가 존재한다. page walk가 더 길어지기 때문이다.<br>보통 시스템은 2-level 까지만 하는 경우가 많은데 이는 3-level 부터는 성능이 개선되는게 별로 크지 않기때문이라고 한다.  </p><h2 id="Inverted-Page"><a href="#Inverted-Page" class="headerlink" title="Inverted Page"></a>Inverted Page</h2><p>64bit 주소공간을 가진 시스템에서는 multi-level paging을 위한 정보의 크기는 32bit에 비해 현격하게 증가되어 문제가 있었다. 그래서 새로 연구된 paging 방법이 있는데 이것이 이번에 소개할 Inverted Page이다. 다만 이 방법은 현재는 거의 사용되고 있지는 않으므로 참고하는 정도로만 보면 좋겠다.  </p><p>64bit 주소공간에 모든 물리메모리가 매핑되어 있지는 않지만, 이의 반대로 모든 물리메모리는 가상메모리에 전부 매핑이 되어있을 확률이 높다는 것에서 출발한다.<br>원래는 page table이 virtual page 번호가 들어오면 이에 매핑되는 physical page 번호를 반환하는 거였다면 이를 반대로 physical page 번호가 들어오면 virtual page 번호를 반환하자는 것이 아이디어다.<br>결국 physical page 번호를 가지고 page table을 만드는 것이다.<br>page table entry로는 process id와 virtual page 번호를 넣는다. 즉 pid와 virtual address의 조합으로 page id를 만든다.  </p><p align="center">    <img alt="Inverted Paging" src="/images/inverted-paging.png"/></p><p>이렇게하면 얻을 수 있는 장점이 시스템 전체에서 하나의 page table만 사용하면 된다.<br>문제는 page table을 검사하는데 너무 오랜 시간이 걸렸다. 테이블 검색을 전체를 검색하지말고 Hash table을 사용하여 개선하기도 하지만 이또한 한계가 존재한다.<br>그러하여 이런 방식보다는 virtual page를 찾고 TLB와 함께 사용하여 성능을 개선한 기존 방식을 많이 사용한다.  </p><h2 id="Demand-Paging"><a href="#Demand-Paging" class="headerlink" title="Demand Paging"></a>Demand Paging</h2><p>Demand Paging은 프로세스의 실행을 위한 모든 page를 메모리에 올리지 않고, 필요한 page의 요청이 발생할 때 메모리에 올리는 paging 기법이다.<br>PTE의 valid bit을 활용하여 한 프로세스에 필요한 page를 memory와 secondary storage 간에 이동을 시킨다. 이런 방법을 통해 물리메모리 구성의 시간이 줄어든다. 그리고 프로세스 전체 이미지를 메모리에 올리지 않기때문에 실제 필요한 물리메모리의 양을 줄일 수 있다.   </p><p>page table에서 참조하려는 page가 valid한 경우에는 이 page가 실제 물리메모리에 frame이 할당되어 있기 때문에 정상적인 참조가 가능하다. 다만 참조하려는 page가 invalid하다면 이 page가 실제 물리메모리에 존재하지 않으므로 이에 대한 처리가 필요한데 이것을 <strong>Page Fault</strong>라고 한다.  </p><p align="center">    <img alt="Demand Paging" src="/images/demand-paging.png"/></p>  <h2 id="Page-Fault"><a href="#Page-Fault" class="headerlink" title="Page Fault"></a>Page Fault</h2><p>프로세스가 page를 참조하였을때 해당 page가 할당받은 frame이 없을경우 page fault가 발생한다. 그러면 page fault handler를 수행한다.<br>page fault handler는 새로운 page frame을 할당받는다. 그리고 backing store에서 해당 page의 내용을 frame에 불러들인다. 그리고 page table을 재구성한 후 프로세스의 작업을 재개한다.<br>다음 그림을 보자.  </p><p align="center">    <img alt="Page Fault Handler" src="/images/page-fault-handler.png"/></p><p>위 그림의 1번에서 페이지 참조를 하면 page fault가 일어난다. page fault는 trap이다.<br>그러면 disk로 부터 해당 page를 읽고 이를 물리메모리에 할당한다. page table을 적절하게 마킹을 표시해주고 재개한다.  </p><p>프로세스가 맨 처음 실행될때는 어떤 일이 일어날까?<br>프로세스가 처음 실행을 시작하면 page fault가 일어난다. 처음에는 page가 물리메모리에 frame이 할당되어 있지 않기때문이다. 그래서 맨처음에 제일먼저 page fault가 일어난다.<br>그러면 page fault handler가 disk에서 해당 page의 내용을 다시 frame에 불러오고 이를 재개한다.  </p><p>만약에 우리가 local variable을 접근하고 있다고하자. local variable은 stack에 존재한다. 하지만 맨 처음에는 이 접근에 page fault가 발생한다. 그럼 이 또한 disk 에서 읽어와야할까?<br>text segment와 data segment는 disk에서 읽어오는데 stack은 읽어올 내용이 없다. stack은 프로세스가 수행을 시작하고 프로세스가 끝나면 사라져야한다. 그러므로 stack 같은 경우에는 disk에 접근하는 것을 생략하고 frame만 할당한 후 page table을 업데이트하고 수행을 재개한다.  </p><p>그러면 맨처음 page fault가 일어날때 어떻게 disk에서 이 위치를 알고 가져올 수 있을까?<br>Loader가 프로그램을 시작할때 executable file을 읽고 disk에 address space를 적절하게 build 해준다. executable file 자체가 text segment와 data segment의 page file이 되도록 하는 시스템도 존재한다.  </p><p>page fault는 비싼작업이다. 일단 프로세스는 page fault가 발생하면 멈추어야한다. page fault는 trap으로 동작하고 할당가능한 frame을 찾고 필요하다면 disk에 접근하여 page file을 가져오고 다시 page table을 mapping 해주어야한다. frame을 찾는 것은 micro second 단위이지만 disk 접근은 milli second 단위이다. page fault가 최대한 발생하지 않도록 Kernel에서도 많은 노력을 하고있다.  </p><p>프로세스의 실행시간중 page fault를 처리하는 시간이 execution보다 긴 상황을 <strong>Thrashing</strong>이 일어났다고 표현한다. 이런 Thrashing 현상을 줄이기 위해 여러 노력을 하고있는데 그중 하나가 working set model이 있다.  </p><h4 id="Working-Set"><a href="#Working-Set" class="headerlink" title="Working Set"></a>Working Set</h4><p>메모리의 Locality 속성을 기반으로 프로세스가 일정시간동안 원활하게 수행되기 위해서 한꺼번에 메모리에 올라와 있어야 하는 page set을 working set이라고 한다. 특정 time window 동안에 접근된 page 들의 집합이 working set에 적용될 수 있다.<br>Kernel은 프로세스의 working set을 계산하여 이를 프로세스에게 제공하여 page fault를 최소화한다. working set에 포함된 page 수는 시스템 전반에 걸쳐 사용가능한 총 page 수에 따라 증가되기도 감소하기도 한다. 또 working set을 프로그램이 시작하면 바로 할당하면서 맨처음 수행시 page fault가 나지않도록 text, data segment에 해당하는 frame을 미리 할당하는 전략도 존재한다.  </p><h2 id="Page-Replacement"><a href="#Page-Replacement" class="headerlink" title="Page Replacement"></a>Page Replacement</h2><p>멀티 프로그래밍 시스템에서 user process가 증가하면 모든 user process가 사용하는 page 수보다 물리메모리의 frame이 부족한 상황이 발생할 수 있다. 이럴때에는 page 교체(replacement)를 진행해야 한다. page fault를 할 때 page replacement가 추가된다.<br>물리메모리에 위치한 page를 disk에 저장한다. 그 frame에는 virtual page의 내용이 저장되어 있는데 이 값을 disk에 쓰고, 이 frame에 다른 virtual page를 불러와 교체한다. 이를 <strong>Page Replacement</strong>라고 한다.<br>다음은 page replacement 과정을 간략하게 설명한 내용이다.  </p><ol><li>Disk에서 요구된 page의 위치를 찾는다.  </li><li>물리메모리에서 free frame을 찾는다. 만약 free frame이 있다면 이를 사용하고, 없다면 page replacement 알고리즘을 사용하여 교체할 frame(victim frame)을 선택한다. 교체할 frame을 disk에 저장하고 page table, frame table을 변경한다.  </li><li>요구된 page를 free frame으로 읽어들이고 해당 page table, frame table을 적절하게 변경한다.  </li><li>user process를 재개한다.</li></ol><p>비어있는 frame이 존재하지 않는다면 어떤 frame을 교체할지 적절하게 알고리즘으로 판단하여 victim을 정한다. 그리고 이 victim을 disk에 write하고 page table을 변경한다. 이때 invalid bit(present bit)를 설정한다. 이를 page out 되었다고 표현한다.  </p><p align="center">    <img alt="Page Replacement 과정" src="/images/page-replacement.png"/></p><p>user process가 page out된 page에 다시 접근하면 page fault가 발생한다. 이 말은 physical address binding이 runtime에 일어난다는 증거이다.<br>Kernel은 page fault가 날때마다 page table 업데이트를 2번 해주어야한다. victim은 page out하고 이 victim process의 page table의 invalid bit 세팅하고, 그리고 page-in 된 프로세스의 page table도 수정해야한다.  </p><h4 id="Page-Replacement-Algorithms"><a href="#Page-Replacement-Algorithms" class="headerlink" title="Page Replacement Algorithms"></a>Page Replacement Algorithms</h4><p>Page Replacement가 필요할 때 어떻게 교체할 페이지를 고를까?<br>이는 여러 알고리즘이 존재하는데 결국 핵심적인 내용은 이 알고리즘들은 모두 Page Replacement에 의한 I&#x2F;O 작업 수행 횟수를 최대한 줄이려는 목적을 가지고 있으며, 적합한 알고리즘의 사용은 시스템의 성능을 크게 좌우하는 요소이다. I&#x2F;O 작업은 매우 큰 비용을 지불해야 하기 때문이다.  </p><p>여러 알고리즘이 존재하고 이들은 매우 간단하게 몇가지만 알아볼 것이다.<br>먼저 가장 목표로 해야되는 것은 앞으로 가장 오랫동안 사용되지 않을 page를 교체해야한다. 그렇게 해야 가장 낮은 page fault 발생빈도를 가질 수 있다. 다만 미래에 어떤 page가 사용되지 않을지는 알수가 없다.  </p><h6 id="FIFO-알고리즘"><a href="#FIFO-알고리즘" class="headerlink" title="FIFO 알고리즘"></a>FIFO 알고리즘</h6><p>그냥 먼저 frame이 할당된 page를 교체한다.<br>가장 단순한 알고리즘이며 FIFO 큐를 이용해 구현가능하다.  </p><h6 id="NRU-알고리즘"><a href="#NRU-알고리즘" class="headerlink" title="NRU 알고리즘"></a>NRU 알고리즘</h6><p>NRU(Not Recently Used) 알고리즘은 이름 그대로 최근에 사용하지 않은 페이지를 교체하는 방식이다.<br>각 page마다 reference bit와 modified bit를 둔다. 이 bit들의 설정은 MMU가 한다.<br>reference bit는 최초로 frame에 로드되었을때 그리고 참조되었을때 bit를 1로 설정한다. 그리고 주기적으로 0으로 리셋한다.<br>modified bit은 최초로 frame에 로드될때는 0으로 설정하고 이후에 page의 내용이 변경되었을때 1로 설정한다.<br>그래서 page replacement가 필요한 시점에 다음 순서대로 rough 하게 교체대상을 찾는다.  </p><ol><li>reference bit &#x3D; 0, modified bit &#x3D; 0</li><li>reference bit &#x3D; 1, modified bit &#x3D; 0</li><li>reference bit &#x3D; 0, modified bit &#x3D; 1</li><li>reference bit &#x3D; 1, modified bit &#x3D; 1</li></ol><h6 id="LRU-알고리즘"><a href="#LRU-알고리즘" class="headerlink" title="LRU 알고리즘"></a>LRU 알고리즘</h6><p>LRU(Least Recently Used) 알고리즘은 가장 오랜시간 참조되지 않은 페이지를 교체하는 방식이다.<br>이는 Page의 locality를 고려하고 가장 이상적인 알고리즘에 근접해있어 가장 많이 사용하는 방법이다.<br>Counter를 사용하여 참조된 시간을 기록하는 방식으로 구현하거나, bit와 Queue를 사용하여 구현한다.  </p><h2 id="Swapping"><a href="#Swapping" class="headerlink" title="Swapping"></a>Swapping</h2><p>Page Replacement, Page out은 4KB 짜리 page를 disk로 내보내는 것이다. 하지만 물리메모리가 부족한 상황에서는 page를 하나씩 내보내는 것으로는 부족하다. 이때에는 특정 프로세스 전체를 통째로 내려야한다. 이를 <strong>Swapping</strong>이라고 한다.<br>Swap 대상이 된 프로세스 전체를 secondary storage로 보낸다.<br>이렇게 page out이나 swapping에 사용되는 secondary storage의 영역을 swap 영역이라고 부른다.<br>OS를 설치할때 맨처음 disk에 대해 swap 영역의 크기를 물어보는 부분이 나올때가 있다. 이 부분은 처음에 OS설치시 file system이 이 영역을 사용하지 않고 swap 영역으로 사용하도록 한다.  </p><p>서버에서 swap이 발생했다면 이미 상황이 좋지 않은 것이다. 왜냐하면 swap이 발생하면 Thrashing이 발생할 수 밖에 없는데 CPU가 굉장히 바쁘다. 한 프로세스가 사용하는 모든 page들을 disk로 내리고, 다른 프로세스를 swap in 시켜주어야 하기 때문이다.</p><p>그러면 이를 방지하기 위해서는 어떻게 해야할까?<br>현실적으로는 물리 메모리의 크기를 늘리거나 프로세스 숫자를 줄여야한다.  </p><p>다음은 swapping의 swap-out과 swap-in 과정을 그림으로 표현한 것이다.  </p><p align="center">    <img alt="Swapping" src="/images/swapping.png"/></p>  <h2 id="Kernel-Memory"><a href="#Kernel-Memory" class="headerlink" title="Kernel Memory"></a>Kernel Memory</h2><p>Kernel Memory 영역은 어떻게 잡히는 것일까? 이또한 위에서 살펴보았던 Virtual Memory와 연관이 있는것일까?<br>시스템이 virtual memory를 사용한다면, Kernel 또한 virtual memory를 사용한다. Windows 같은 경우는 2GB이상을 사용한다. 이는 kernel code, kernel data, process 별 page table 등을 포함하고 이들또한 disk로 page-out 될 수 있다.<br>즉 virtual memory의 특정영역이 Kernel에 사용되도록 되어있다.<br>예를들어 32bit 주소공간을 가진 시스템에서 주소가 2GB 이상부터는 Kernel 영역이라고 정할 수 있다. 그 영역을 매핑하는 page table들은 각 PTE가 kernel page들을 가리키게 된다. 이 PTE들은 user page를 매핑할 수 없다.  </p><p>page table은 프로세스마다 존재한다. context switching이 일어나면 바라보는 page table 전체가 변경된다. 하지만 전통적으로 모든 프로세스들이 공유를 하고있는 영역도 존재를 한다.<br>이런 영역같은 경우는 page table에 특별한 marking을 해놓아서 TLB에 캐시가 invalidate 되지 않도록 한다. 그리고 매우 critical 한 부분은 swap이 되지 않도록 설정하기도 한다. 다음의 virtual memory segment 그림을 보자.  </p><p align="center">    <img style="max-width: 500px" alt="Virtual Memory" src="/images/kernel-memory.png"/></p><p>Kernel virtual memory 영역은 kernel code와 kernel data를 포함한다.<br>그리고 kernel memory의 특정영역은 physical memory에 매핑이되어 모든 프로세스들이 이를 공유하기도 한다. 예를들어 모든 kernel code와 kernel data는 모든 프로세스들이 공유한다.<br>리눅스 또한 특정영역의 인접한 virtual page들을 인접한 physical page로 매핑하기도 한다. 이는 커널이 physical memory에서 특정 영역을 접근하는데 편하게 해준다.<br>예를들어 page table에 접근을 할때나 memory mapped I&#x2F;O operation을 수행해야 할때 도움을 준다.  </p><p>Kernel virtual memory 영역에서 또다른 영역은 프로세스 별로 데이터를 가지고있는 영역도 있는데, 이 영역에는 그 프로세스의 page table이나 그 프로세스의 context에서 실행되는 kernel stack, 다양한 데이터 자료구조들이 존재한다.  </p><h4 id="기타사항"><a href="#기타사항" class="headerlink" title="기타사항"></a>기타사항</h4><p>그렇다면 Virtual Memory를 사용하지 않는 운영체제도 있을까?<br>일부 임베디드 시스템들은 virtual meomory를 사용하지 않는 시스템이 있다. 예를들어 자동차 하드웨어의 일부는 virtual memory를 사용하지 않는데 MMU를 사용해야하므로 비용문제등으로 사용하지 않는 경우가 있다.  </p><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      
      <comments>https://tk-one.github.io/2019/08/08/os-memory/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>운영체제 8편 - 동기화의 고전적인 문제들</title>
      <link>https://tk-one.github.io/2019/07/25/classic-problem-of-synchronization/</link>
      <guid>https://tk-one.github.io/2019/07/25/classic-problem-of-synchronization/</guid>
      <pubDate>Thu, 25 Jul 2019 12:50:23 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.&lt;br&gt;맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 동기화(synchronization)에 이어 동기화의 고전적인 문제들에 대한 내용입니다.  &lt;/p&gt;
&lt;p&gt;동기화에 대한 고전적인 대표적인 문제들이 몇가지 있는데 이중 2가지를 알아볼 것이다.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bounded-buffer problem&lt;/li&gt;
&lt;li&gt;Readers and Writers problem&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.<br>맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  </p><p>이번 편은 동기화(synchronization)에 이어 동기화의 고전적인 문제들에 대한 내용입니다.  </p><p>동기화에 대한 고전적인 대표적인 문제들이 몇가지 있는데 이중 2가지를 알아볼 것이다.  </p><ul><li>Bounded-buffer problem</li><li>Readers and Writers problem</li></ul><span id="more"></span><h2 id="Bounded-buffer-problem"><a href="#Bounded-buffer-problem" class="headerlink" title="Bounded-buffer problem"></a>Bounded-buffer problem</h2><p>이는 간단하게 표현하면 N개의 buffer에 여러 producer와 consumer가 동시에 접근하는 문제이다.<br>몇가지 용어부터 정리해보자.  </p><ul><li><strong>생산자(Producer)</strong><br>한번에 하나의 아이템을 생성해 buffer에 저장한다. 다만 이미 buffer의 상태가 full이면 대기한다.  </li><li><strong>소비자(Consumer)</strong><br>Buffer에서 하나의 아이템을 가져온다. 다만 buffer가 비어있다면 대기한다.</li></ul><p align="center">    <img style="max-width: 550px" alt="Bounded-buffer problem" src="/images/bounded-buffer-problem.png"/></p>  <p>이 문제는 주변에서도 많이 찾아볼 수 있는데 패킷을 예로 들 수 있겠다. 패킷이 계속 들어올때 ethernet을 사용한다고 가정한다면 ethernet driver가 패킷을 하나씩 저장해준다. 이들이 buffer에 올라오면 그다음 부터는 Kernel의 역할이다. Kernel이 buffer를 읽어야한다. 이를 읽을때 어떤 것을 동기화를 해야하는가. 어떤 자료구조를 사용해야 하는가 등이 이문제를 해결하는데 중요한 요점이다.</p><p>여기에서는 Producer와 Consumer가 충돌없이 buffer에 접근할 수 있도록 해야하므로, Buffer에 접근하는 부분이 critical section이 되겠다.  </p><p>여러개의 Producer가 동시에 produce할 수 없고, Producer와 Consumer가 동시에 produce, consume 할 수 없다. 또한 여러개의 Consumer도 동시에 consume할 수 없다.<br>이를 여러개의 적절한 자료구조와 알고리즘을 통해 해결해보자.  </p><p>이는 3가지 semaphore를 사용해서 구현할 수 있다. (다른 구현방법도 많다.)<br>우선 buffer는 간단하게 N의 크기를 가진 배열로 하자. Producer는 buffer가 full이 아니라면 새로운 item을 추가한다. Consumer는 buffer가 empty가 아니라면 item을 소비한다.  </p><p>문제를 해결하기 위해 도입한 3가지 semaphore와 그들의 역할은 다음과 같다.  </p><ul><li><strong>empty</strong><br>버퍼내에 저장할 공간을 의미한다. Producer들의 진입을 관리한다. buffer의 크기가 N이므로 초기값은 N으로 하여 0이 될때까지 Producer가 진입할 수 있다.  </li><li><strong>full</strong><br>버퍼내에 소비할 아이템이 있는지를 의미한다. Consumer들의 진입을 관리한다. 초기값은 0으로 하여 비어있을때는 Consumer가 진입하지 못한다.  </li><li><strong>Mutex</strong><br>buffer의 변경을 위한 semaphore이다. 초기값은 1로하여 동시에 1개의 Producer or Consumer만 진입할 수 있다.</li></ul><p>pseudo 코드를 바로 살펴보자.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Producer:<br>    do &#123;<br>        // 버퍼에 최소 하나의 공간이 생기기를 기다린다.<br>        wait(empty);<br>        wait(mutex);<br><br>        [produce an item in buffer]<br><br>        signal(mutex);<br>        // 버퍼에 아이템이 있음을 알린다.<br>        signal(full);<br>    &#125; while (1);<br><br><br>Consumer:<br>    do &#123;<br>        // 버퍼에 적어도 하나의 아이템이 채워지기를 기다린다.<br>        wait(full); <br>        wait(mutex);<br>        <br>        [consume item from buffer]<br><br>        signal(mutex);<br>        // 버퍼에 하나의 빈 공간이 생겼음을 알린다.<br>        signal(empty);<br>    &#125; while (1);<br></code></pre></td></tr></table></figure><p>이를 자바로 구현한 코드도 살펴보자.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.concurrent.Semaphore;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Buffer</span> &#123;<br>    <span class="hljs-keyword">private</span> String[] buffer;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> index;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">Buffer</span><span class="hljs-params">(<span class="hljs-type">int</span> size)</span> &#123;<br>        <span class="hljs-built_in">this</span>.buffer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>[size];<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">add</span><span class="hljs-params">(String message)</span> &#123;<br>        <span class="hljs-keyword">if</span> (index &gt;= buffer.length) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;buffer is full&quot;</span>);<br>        &#125;<br><br>        buffer[index++] = message;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">pop</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">if</span> (index == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;buffer is empty&quot;</span>);<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> buffer[--index];<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Producer</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> counter;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Buffer buffer;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore empty;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore mutex;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore full;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">Producer</span><span class="hljs-params">(Buffer buffer,</span><br><span class="hljs-params">                    Semaphore empty,</span><br><span class="hljs-params">                    Semaphore mutex,</span><br><span class="hljs-params">                    Semaphore full)</span> &#123;<br>        <span class="hljs-built_in">this</span>.buffer = buffer;<br>        <span class="hljs-built_in">this</span>.empty = empty;<br>        <span class="hljs-built_in">this</span>.mutex = mutex;<br>        <span class="hljs-built_in">this</span>.full = full;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">produce</span><span class="hljs-params">(String message)</span> &#123;<br>        <span class="hljs-keyword">try</span> &#123;<br>            empty.acquire();<br>            mutex.acquire();<br><br>            buffer.add(message + <span class="hljs-string">&quot;-&quot;</span> + counter);<br>            counter++;<br><br>            mutex.release();<br>            full.release();<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Consumer</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Buffer buffer;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore empty;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore mutex;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore full;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">Consumer</span><span class="hljs-params">(Buffer buffer,</span><br><span class="hljs-params">                    Semaphore empty,</span><br><span class="hljs-params">                    Semaphore mutex,</span><br><span class="hljs-params">                    Semaphore full)</span> &#123;<br>        <span class="hljs-built_in">this</span>.buffer = buffer;<br>        <span class="hljs-built_in">this</span>.empty = empty;<br>        <span class="hljs-built_in">this</span>.mutex = mutex;<br>        <span class="hljs-built_in">this</span>.full = full;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">consume</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">try</span> &#123;<br>            full.acquire();<br>            mutex.acquire();<br><br>            <span class="hljs-type">String</span> <span class="hljs-variable">message</span> <span class="hljs-operator">=</span> buffer.pop();<br>            System.out.println(<span class="hljs-string">&quot;pop &quot;</span> + message);<br><br>            mutex.release();<br>            empty.release();<br><br>            <span class="hljs-keyword">return</span> message;<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Main</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String args[])</span> &#123;<br>        <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">size</span> <span class="hljs-operator">=</span> <span class="hljs-number">3</span>;<br>        <span class="hljs-type">Buffer</span> <span class="hljs-variable">buffer</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Buffer</span>(size);<br>        <span class="hljs-type">Semaphore</span> <span class="hljs-variable">emptySemaphore</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Semaphore</span>(size);<br>        <span class="hljs-type">Semaphore</span> <span class="hljs-variable">mutexSemaphore</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Semaphore</span>(<span class="hljs-number">1</span>);<br>        <span class="hljs-type">Semaphore</span> <span class="hljs-variable">fullSemaphore</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Semaphore</span>(<span class="hljs-number">0</span>);<br>        <span class="hljs-type">Producer</span> <span class="hljs-variable">producer1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Producer</span>(<br>                buffer, emptySemaphore, mutexSemaphore, fullSemaphore);<br>        <span class="hljs-type">Producer</span> <span class="hljs-variable">producer2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Producer</span>(<br>                buffer, emptySemaphore, mutexSemaphore, fullSemaphore);<br>        <span class="hljs-type">Consumer</span> <span class="hljs-variable">consumer1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Consumer</span>(<br>                buffer, emptySemaphore, mutexSemaphore, fullSemaphore);<br><br>        <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; IntStream.range(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>).forEach(i -&gt; producer1.produce(<span class="hljs-string">&quot;producerThread1&quot;</span>))).start();<br>        <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; IntStream.range(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>).forEach(i -&gt; producer2.produce(<span class="hljs-string">&quot;producerThread2&quot;</span>))).start();<br>        <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; &#123;<br>            <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>                consumer1.consume();<br>            &#125;<br>        &#125;).start();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><code>producer1</code>과 <code>producer2</code>는 각각 100번씩 buffer에 문자열 item을 채워넣는다.<br>그리고 buffer를 consumer가 계속해서 소비하도록 한다.  </p><h2 id="Readers-And-Writers-Problem"><a href="#Readers-And-Writers-Problem" class="headerlink" title="Readers And Writers Problem"></a>Readers And Writers Problem</h2><p>이는 여러개의 Reader와 Writer가 하나의 공유데이터를 읽거나 쓰기위해 접근하는 문제를 의미한다.  </p><p>몇가지 용어들을 정리해보자.  </p><ul><li><strong>Reader</strong><br>여러 Reader들은 동시에 데이터를 읽을 수 있다.  </li><li><strong>Writer</strong><br>Reader 혹은 다른 Writer가 데이터에 접근하고 있으면 write를 할 수 없다.</li></ul><p align="center">    <img alt="Readers Writers problem" src="/images/readers-writers-problem.png"/></p><p>즉 여러개의 reader는 동시에 critical section에 접근할 수 있지만, writer는 다른 reader나 writer가 접근하지 않을때 write 할 수 있다. Reader도 writer가 현재 critical section에 접근중이면 진입할 수 없다.앞에서 보았던 Bounded-buffer problem에서는 reader와 writer가 각각 한번만 들어가는 것을 허용했는데 Readers Writers problem에서는 여러 reader들의 접근을 허용한다.<br>이를 여러개의 적절한 자료구조와 알고리즘을 통해 해결해보자.  </p><p>이는 2가지 semaphore와 정수형 자료구조를 사용해서 구현할 수 있다.  </p><p>문제를 해결하기 위해 도입한 자료구조들의 역할은 다음과 같다.  </p><ul><li><strong>int readCount</strong><br>버퍼에 현재 접근하고 있는 reader들의 개수를 의미한다. reader 개수가 0이되어야 writer가 접근할 수 있다. 초기값은 0으로 설정한다.  </li><li><strong>Semaphore write</strong><br>Writer간의 동기화를 위한 semaphore이다. 초기값은 1로 두고, 1일때에만 writer가 접근할 수 있다.  </li><li><strong>Semaphore mutex</strong><br><code>readCount</code>에 대한 접근과 <code>write</code> semaphore에 대한 접근이 원자적으로 수행되기위한 semaphore이다. 초기값은 1로 설정한다.</li></ul><p>pseudo 코드를 바로 살펴보자.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Writer:<br>    wait(write);<br>    [write]<br>    signal(write);<br><br><br>Reader:<br>    wait(mutex);<br>    readCount++;<br>    // Reader가 최소 1개 진입해있을때 writer의 접근을 막는다.<br>    if (readCount == 1) &#123;<br>        wait(write);<br>    &#125;<br>    signal(mutex);<br><br>    [read]<br><br>    wait(mutex);<br>    readCount--;<br>    // 아무런 Reader 도 진입해있지 않으므로 writer의 접근을 허용한다.<br>    if (readCount == 0) &#123;<br>        signal(write);<br>    &#125;<br>    signal(mutex);<br></code></pre></td></tr></table></figure><p>이를 자바를 사용하여 구현한 코드도 살펴보자.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.concurrent.Semaphore;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Data</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> number;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getNumber</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> number;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setNumber</span><span class="hljs-params">(<span class="hljs-type">int</span> number)</span> &#123;<br>        <span class="hljs-built_in">this</span>.number = number;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Writer</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> counter;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore writer;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Data data;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">Writer</span><span class="hljs-params">(Semaphore writer, Data data)</span> &#123;<br>        <span class="hljs-built_in">this</span>.writer = writer;<br>        <span class="hljs-built_in">this</span>.data = data;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">write</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">try</span> &#123;<br>            writer.acquire();<br>            data.setNumber(++counter);<br>            System.out.println(<span class="hljs-string">&quot;Write data &quot;</span> + counter + <span class="hljs-string">&quot; in thread &quot;</span> + Thread.currentThread().getName());<br>            writer.release();<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Reader</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-type">int</span> readCount;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore writer;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Semaphore mutex;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Data data;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">Reader</span><span class="hljs-params">(Semaphore writer, Semaphore mutex, Data data)</span> &#123;<br>        <span class="hljs-built_in">this</span>.writer = writer;<br>        <span class="hljs-built_in">this</span>.mutex = mutex;<br>        <span class="hljs-built_in">this</span>.data = data;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">read</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">try</span> &#123;<br>            mutex.acquire();<br>            readCount++;<br>            <span class="hljs-keyword">if</span> (readCount == <span class="hljs-number">1</span>) &#123;<br>                writer.acquire();<br>            &#125;<br>            mutex.release();<br><br>            <span class="hljs-type">int</span> <span class="hljs-variable">num</span> <span class="hljs-operator">=</span> data.getNumber();<br>            System.out.println(<span class="hljs-string">&quot;Read data &quot;</span> + num + <span class="hljs-string">&quot; in thread &quot;</span> + Thread.currentThread().getName());<br><br>            mutex.acquire();<br>            readCount--;<br>            <span class="hljs-keyword">if</span> (readCount == <span class="hljs-number">0</span>) &#123;<br>                writer.release();<br>            &#125;<br>            mutex.release();<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Main</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">sleep</span><span class="hljs-params">(<span class="hljs-type">long</span> timeMillis)</span> &#123;<br>        <span class="hljs-keyword">try</span> &#123;<br>            Thread.sleep(timeMillis);<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e);<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String args[])</span> &#123;<br>        <span class="hljs-type">Data</span> <span class="hljs-variable">data</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Data</span>();<br>        <span class="hljs-type">Semaphore</span> <span class="hljs-variable">writer</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Semaphore</span>(<span class="hljs-number">1</span>);<br>        <span class="hljs-type">Semaphore</span> <span class="hljs-variable">mutex</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Semaphore</span>(<span class="hljs-number">1</span>);<br><br>        <span class="hljs-type">Reader</span> <span class="hljs-variable">reader1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Reader</span>(writer, mutex, data);<br>        <span class="hljs-type">Reader</span> <span class="hljs-variable">reader2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Reader</span>(writer, mutex, data);<br>        <span class="hljs-type">Writer</span> <span class="hljs-variable">writer1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Writer</span>(writer, data);<br><br>        <span class="hljs-type">Thread</span> <span class="hljs-variable">readerThread1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; IntStream.range(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>).forEach(i -&gt; &#123;<br>            reader1.read();<br>            sleep(<span class="hljs-number">5</span>);<br>        &#125;));<br>        readerThread1.setName(<span class="hljs-string">&quot;readerThread-0&quot;</span>);<br>        <span class="hljs-type">Thread</span> <span class="hljs-variable">readerThread2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; IntStream.range(<span class="hljs-number">0</span> ,<span class="hljs-number">20</span>).forEach(i -&gt; &#123;<br>            reader2. read();<br>            sleep(<span class="hljs-number">5</span>);<br>        &#125;));<br>        readerThread2.setName(<span class="hljs-string">&quot;readerThread-1&quot;</span>);<br>        <span class="hljs-type">Thread</span> <span class="hljs-variable">writerThread1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; IntStream.range(<span class="hljs-number">0</span>, <span class="hljs-number">50</span>).forEach(i -&gt; &#123;<br>            writer1.write();<br>            sleep(<span class="hljs-number">10</span>);<br>        &#125;));<br>        writerThread1.setName(<span class="hljs-string">&quot;writerThread-0&quot;</span>);<br>        readerThread1.start();<br>        readerThread2.start();<br>        writerThread1.start();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>이에 대한 출력은 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Read data 0 in thread readerThread-1<br>Read data 0 in thread readerThread-0<br>Write data 1 in thread writerThread-0<br>Read data 1 in thread readerThread-1<br>Read data 1 in thread readerThread-0<br>Read data 1 in thread readerThread-1<br>Read data 1 in thread readerThread-0<br>Write data 2 in thread writerThread-0<br>Read data 2 in thread readerThread-1<br>Read data 2 in thread readerThread-0<br>Read data 2 in thread readerThread-1<br>Read data 2 in thread readerThread-0<br>Write data 3 in thread writerThread-0<br>Read data 3 in thread readerThread-1<br>...<br>...<br></code></pre></td></tr></table></figure><p>다만 이와같이 구현을 하게되면 Writer가 starve할 수 있는 문제가 있다. 왜냐하면 readCount가 0일때에만 writer가 접근할 수 있으므로 여러 reader들이 계속 접근을 하게되면 writer는 접근할 수 있는 기회를 얻지 못하게 된다.  </p><h2 id="동기화-알고리즘의-설계"><a href="#동기화-알고리즘의-설계" class="headerlink" title="동기화 알고리즘의 설계"></a>동기화 알고리즘의 설계</h2><p>실제 실무에서도 동기화 문제에 부딪혀 이를 해결해야 하는 경우가 빈번하다.<br>이를 해결하기 위해 다음과 같은 사항을 고려하며 생각해보면 도움이 될 수 있겠다.  </p><p>먼저 동기화 문제를 동작조건과 함께 서술해보자. 어떤 조건이 있어야하는지? 어떤 접근형태를 가지는지를 먼저 정리해보자.<br>그 다음에 사용할 semaphore를 설계한다. 몇개를 사용할지? 이 semaphore가 왜 필요할지, 어떤 대상을 protect할지 사고해보고 결정한다. 또한 각 semaphore에 알맞는 초기값을 설계해야한다.<br>그리고 알고리즘을 설계한 후 검토한다. 다음 사항을 검토해보자.  </p><ul><li>Data consistency가 확보되는지</li><li>Deadlock이 발생하는지</li><li>Starvation 가능성이 있는지</li><li>Concurrency를 얼마나 제공하는지</li></ul><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      
      <comments>https://tk-one.github.io/2019/07/25/classic-problem-of-synchronization/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>운영체제 7편 - 동기화(Synchronization)</title>
      <link>https://tk-one.github.io/2019/07/24/os-synchronization/</link>
      <guid>https://tk-one.github.io/2019/07/24/os-synchronization/</guid>
      <pubDate>Wed, 24 Jul 2019 11:51:23 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.&lt;br&gt;맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 동기화(synchronization)에 대한 내용입니다.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.<br>맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  </p><p>이번 편은 동기화(synchronization)에 대한 내용입니다.  </p><span id="more"></span><h2 id="동기화-Synchronization"><a href="#동기화-Synchronization" class="headerlink" title="동기화(Synchronization)"></a>동기화(Synchronization)</h2><p>공유하고 있는 데이터에 동시에 접근할 때 동기화를 적절하게 해주지 않으면 데이터에 대한 일관성(consistency)를 보장할 수 없다. 공유데이터에 대해 여러 프로세스가 동시에 접근하여 read, write를 함으로써 데이터의 일관성이 보장되지 않는 상황을 <strong>race condition</strong>이라고 한다.<br>데이터의 일관성을 유지하기 위해 수행하는 프로세스들이 순차적으로 데이터를 접근하게 하면 일관성을 보장해줄 수 있는데 이를 <strong>동기화</strong>라고 한다. 한번에 하나의 프로세스만 그 값을 write할 수 있게 하는 것이다.  </p><h6 id="Race-condition-예시"><a href="#Race-condition-예시" class="headerlink" title="Race condition 예시"></a>Race condition 예시</h6><p>흔한 잔고 출금에 대한 예시를 들어보자.<br>process A는 <code>balance</code> 변수를 읽고 <code>balance = balance - 500</code>을 하고 process B는 <code>balance</code> 변수를 읽고 <code>balance = balance + 500</code>을 하면 process A와 process B가 concurrent하게 수행이 될때 언제 context switching이 일어날지 모르기때문에 이들을 적절하게 동기화를 해주지않으면 결과를 정확이 보장할 수 없다.  </p><p>이 말고도 우리가 평소에도 흔하게 사용하는 Thread 들에서도 data section을 서로 공유하기 때문에 race condition이 발생할 수 있고, Kernel 내부에서도 PCB(Process Control Block)이 공유되고 이 외에도 page table 같은 Kernel data structure 들에서도 race condition이 발생한다.  </p><h2 id="Critical-Section"><a href="#Critical-Section" class="headerlink" title="Critical Section"></a>Critical Section</h2><p>여러 프로세스들이 공유하는 데이터에 접근을 하는 코드의 영역을 <strong>critical section</strong>이라고 한다. 데이터 일관성을 해결하기 위해서는 오직 하나의 process만이 critical section에 접근할 수 있도록 해야한다.  </p><p>동기화 문제를 critical section을 사용하여 모델링하면 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">entry section<br>    [critical section]<br>exit section<br>    [remainder section]<br></code></pre></td></tr></table></figure><p>Critical section에 작성한 코드들이 제대로 동작되기 위해서는 오직 하나의 프로세스만 접근을 허용해야한다. 이를 해결하기 위한 알고리즘은 다음 3가지 조건을 만족해야 한다.  </p><ul><li><strong>Mutual Exclusion</strong><br>process A가 critical section에 진입해있다면, 다른 프로세스는 진입할 수 없어야 한다.  </li><li><strong>Progress</strong><br>critical section에 진입하려는 프로세스가 있다면, 그중 한개의 프로세스는 진입할 수 있어야 한다. 그리고 진입을 시도하는 다른 프로세스는 critical section에 진입하기 위해 영원히 대기할 수 없다. (즉, critical section 안에서 deadlock이 발생하면 안된다)  </li><li><strong>Bounded Waiting</strong><br>어떤 프로세스가 critical section에 들어가기 위해서는 특정 시도횟수 안에 critical section에 진입할 수 있어야 한다. 즉 starve 상태인 프로세스가 없어야 한다. 현재 critical section에 진입해있는 프로세스를 제외하고 critical section에 진입을 원하는 다른 프로세스도 critical section에 진입할 수 있는 기회가 보장되어야 한다는 것이다.</li></ul><h2 id="동기화-알고리즘"><a href="#동기화-알고리즘" class="headerlink" title="동기화 알고리즘"></a>동기화 알고리즘</h2><p>동기화를 해결하기 위한 알고리즘 몇가지를 살펴보자.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Shared Variable:<br>    /*<br>     * turn = 0 일때 critical section에 접근가능하다.<br>     */<br>    int turn = 0;<br><br><br>Process A:<br>    while (turn != 0);<br>        [critical section]<br>    turn = 1;<br>        [remainder section]<br><br><br>Process B:<br>    while (turn != 1);<br>        [critical section]<br>    turn = 0;<br>        [remainder section]<br></code></pre></td></tr></table></figure><p>위의 방식은 알고리즘 조건의 Progress와 Bounded waiting을 만족하지 못한다.<br>Process B를 시작하지 않으면 Process A는 아예 critical section을 진입하지 못하므로 Progress 조건을 만족하지 못하고, 무한정 busy waiting을 하기때문에 Bounded waiting 조건도 만족하지 못한다.  </p><p>이를 개선하기 위하여 두개의  flag 변수를 활용하는 방법이 있다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Shared Variable:<br>    /*<br>     * flag[0] = flag[1] = false 로 초기화<br>     */<br>    boolean flag[2];<br><br><br>Process A:<br>    flag[0] = true;<br>    while (flag[1]);<br>        [critical section]<br>    flag[0] = false;<br>        [remainder section]<br><br><br>Process B:<br>    flag[1] = true;<br>    while (flag[0]);<br>        [critical section]<br>    flag[1] = false;<br>        [remainder section]<br></code></pre></td></tr></table></figure><p>이 방법도 mutual exclusion 조건은 만족하지만 그전에 보았던 방식과 동일하게 다른 2개의 조건은 만족하지 못한다. 경우에 따라 flag[0]과 flag[1]이 모두 true가 될 수 있기 때문이다. </p><p>이를 해결하기 위해 <strong>Peterson Solution</strong>이라고 불리는 방법이 있다.<br>위와 동일한 방식에 turn이라는 변수를 한가지 추가한다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Shared Variable:<br>    /*<br>     * flag[0] = flag[1] = false 로 초기화<br>     */<br>    int turn = 0;<br>    boolean flag[2];<br><br><br>Process A:<br>    flag[0] = true;<br>    turn = 1;<br>    while (flag[1] &amp;&amp; turn == 1);<br>        [critical section]<br>    flag[0] = false;<br>        [remainder section]<br><br><br>Process B:<br>    flag[1] = true;<br>    turn = 0;<br>    while (flag[0] &amp;&amp; turn == 0);<br>        [critical section]<br>    flag[1] = false;<br>        [remainder section]<br></code></pre></td></tr></table></figure><p>이 알고리즘은 flag 변수도 쓰고 turn 변수도 사용한다.<br>알고리즘을 보면 flag[0]과 flag[1]이 둘다 true인 경우가 있을 수 있지만 turn 변수는 반드시 0 혹은 1 이여야 하므로 두개의 프로세스 모두 critical section에 들어가는 경우는 존재하지 않는다. 또한 두개의 프로세스 모두 critical section에 들어가지 못하므로 flag[0], flag[1] 모두 false인 경우도 발생하지 않는다.<br>그러므로 알고리즘의 조건인 Mutual Exclusion, Progress, Bounded Waiting을 모두 만족한다.<br>다만 Peterson Solution은 확장성에 한계가 존재한다.<br>만약 2개의 프로세스가 아닌 3개 이상의 프로세스에 대해 지원하려면 매우 복잡해지고 구현하기가 쉽지 않다.  </p><h2 id="해결책"><a href="#해결책" class="headerlink" title="해결책"></a>해결책</h2><h6 id="초기-해결책"><a href="#초기-해결책" class="headerlink" title="초기 해결책"></a>초기 해결책</h6><p>kernel에서도 critical section에 대한 동기화가 필요했다. 초기에는 kernel mode로 들어갈때 kernel code 자체를 critical section으로 만들어 kernel mode 일때에는 아예 interrupt 자체를 막아버리는 방식으로 구현을 했었다. (현재는 이렇게 구현되어 있지 않다.) 그래서 kernel mode에서 빠져나와 user mode가 되는 순간 interrupt가 발생하는 방식이였다. interrupt가 disable 되어있으니 context switching은 발생하지 않는다. kernel 전체가 커다란 critical section으로 동작하는 것이다. 다만 kernel 전체가 프로세스가 많아질때 오직 한개의 프로세스만kernel에 진입할 수 있었으므로 대기시간이 길었고, Kernel 레벨에서 multi-threading 을 지원하기 힘들었다.  </p><h6 id="하드웨어-해결책"><a href="#하드웨어-해결책" class="headerlink" title="하드웨어 해결책"></a>하드웨어 해결책</h6><p>interrupt로 동기화를 해결하려다 보니 쉽지않아 hardware instruction으로 해결하자는 이야기가 등장했다. instruction으로 처리하면 알고리즘이 매우 간단하기 때문이다. 흐름은 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">acquire lock<br>    [critical section]<br>release lock<br>    [remainder section]<br></code></pre></td></tr></table></figure><p>critical section에 진입하기 전에 lock을 잡고 lock 획득에 실패한 프로세스들은 모두 lock을 대기한다. 오직 한개의 프로세스만 lock을 잡고 critical section에 들어갈 수 있다. 이런 방식을 <strong>Mutex Lock</strong> 이라고도 부른다.<br>lock은 동기화 instruction(Synchronization instruction)을 사용하여 구현할 수 있다.  </p><h6 id="Synchronization-instruction"><a href="#Synchronization-instruction" class="headerlink" title="Synchronization instruction"></a>Synchronization instruction</h6><p>동기화 instruction은 hardware에서 제공해주는 instruction으로 CPU에서 원자적으로(atomically) 수행되는 것을 보장한다. instruction 사이에는 당연히 interrupt가 발생할 수 없다.  </p><p>동기화 instruction의 예시들을 몇가지 살펴보자.  </p><h6 id="testAndSet"><a href="#testAndSet" class="headerlink" title="testAndSet"></a>testAndSet</h6><p>testAndSet의 semantics는 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">boolean testAndSet(boolean *target) &#123;<br>    boolean rv = *target;<br>    *target = true;<br>    return rv;<br>&#125;<br></code></pre></td></tr></table></figure><p>여기의 testAndSet의 semantics가 그대로 instruction으로 구현이 되어 있는 것이다. boolean 변수를 받아 true로 설정하고 그 전에 설정되어 있던 값을 반환한다. 다만 testAndSet은 느린 instruction이다. 보통 memory access에 대한 instruction은 8cycle 정도 걸리는데 testAndSet instruction은 최소 16 cycle은 걸린다. 다만 이 instruction이 수행되는 동안은 interrupt를 받지않는다. 이를 활용한 방식은 process가 여러개여도 동작한다. 이를 사용하여 동기화를 구현하면 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Shared Variable:<br>    boolean lock = false;<br><br>All Process:<br>do &#123;<br>    while (testAndSet(&amp;lock));<br>        [critical section]<br>    lock = false;<br>        [remainder section]<br>&#125;<br></code></pre></td></tr></table></figure><p>testAndSet 수행결과가 false인 경우만 critical section에 진입할 수 있다. 그리고 instruction이 atomic 하므로 mutual exclusion을 만족한다.<br>다만 위 예제처럼 실제로는 while loop을 통한 busy waiting으로는 잘 구현하지 않는다. testAndSet instruction을 수행할 때마다 memory access가 일어나는데 이를 busy waiting을 하니 Bus가 엄청난 트래픽을 받게되어 성능이 많이 떨어지게 된다. 뒤에 보겠지만 이를 극복하는 방법이 있다.  </p><h6 id="Swap"><a href="#Swap" class="headerlink" title="Swap"></a>Swap</h6><p>Swap이라는 동기화 instruction도 존재하는데 이 또한 testAndSet의 일종이라고 생각해도 된다. 다음과 같은 swap을 atomic하게 수행되도록 해주는 hardware instruction이다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">void swap(boolean *a, boolean *b) &#123;<br>    boolean temp = *a;<br>    *a = *b;<br>    *b = temp;<br>&#125;<br></code></pre></td></tr></table></figure><p>ARM의 swap instruction 설명을 보자.  </p><blockquote><p>SWP (Swap) and SWPB (Swap Byte) provide a method for software synchronization that does not require disabling interrupts. This is achieved by performing a special type of memory access, reading a value into a processor register and writing a value to a memory location as an atomic operation.</p></blockquote><p>즉 interrupt disable을 따로 할 필요없이 instruction level에서 atomic 수행을 보장해준다.  </p><p>이런 testAndSet 이나 swap 같은 동기화 instruction을 사용할 수 있도록 OS나 각 언어별 API에서 제공을 해준다.  </p><h6 id="Synchronization-instruction-한계"><a href="#Synchronization-instruction-한계" class="headerlink" title="Synchronization instruction 한계"></a>Synchronization instruction 한계</h6><p>이처럼 동기화 instruction을 사용하면 mutual exclusion은 해결할 수 있으나, bounded waiting 같은 조건은 software 프로그램에서 제공을 해야한다. 사용자 software에서 이런 부담을 지우기 위해 동기화 primitive를 따로 지원하는데 이렇게 해서 나온 것이 세마포어(semaphore)이다.  </p><h2 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h2><p>Semaphore는 두개의 원자적 연산을 가지는 변수이다.<br>여기서 말하는 원자적인 연산은 다음과 같다.  </p><ul><li>wait() or P()</li><li>signal() or V()</li></ul><p>Semaphore는 그냥 변수이다. critical section에 들어가기 전에 P를 수행하고 critical section에 나오면 V를 수행한다. Semaphore는 P를 통과하면 decrement하고 V를 통과하면 increment 한다.<br>P와 V 연산은 서로 독립적이고, 원자적으로 수행된다. P, V 연산 모두 원자적으로 수행되는 것은 맞지만 P, V 모두 반드시 같은 프로세스에서 진행될 필요는 없다.  </p><p>Semaphore는 2가지로 보통 나누게 된다. Counting Semaphore와 Binary Semaphore이다.  </p><ul><li><strong>Counting Semaphore</strong><br>Semaphore의 값은 한계가 따로 없으며, 초기 값은 가능한 자원의 수로 정해진다.  </li><li><strong>Binary Semaphore</strong><br>Semaphore가 가질 수 있는 값은 오직 0과 1이다.</li></ul><h4 id="Original-Semaphore"><a href="#Original-Semaphore" class="headerlink" title="Original Semaphore"></a>Original Semaphore</h4><p>Original Semaphore는 busy waiting을 사용한다.<br>Busy waiting을 이용한 방법은 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Semaphore as S<br>P(S):<br>    while (S &lt;= 0);<br>        S = S - 1;<br><br>V(S):<br>    S = S + 1;<br></code></pre></td></tr></table></figure><p>위처럼 Busy waiting은 critical section에 진입할 조건이 될 때까지 loop을 돌며 기다린다. 그러므로 CPU cycle을 낭비할 수 있고, 대기중인 프로세스에서 어떤 프로세스가 critical section에 진입할지 알 수 없다.  </p><p>어떻게 busy waiting을 하지 않을 수 있을까? 기다리고 있는 프로세스들은 sleep을 하도록 만들 수 있겠다.  </p><h4 id="Semaphore-with-sleep-queue"><a href="#Semaphore-with-sleep-queue" class="headerlink" title="Semaphore with sleep queue"></a>Semaphore with sleep queue</h4><p>Busy waiting 방식의 CPU cycle을 낭비하는 문제를 해결하기 위해 Semaphore의 자료구조에 sleep queue를 추가하여 대기중인 프로세스를 관리할 수 있겠다.<br>다른 프로세스가 이미 critical section에 진입해있으면 진입을 시도하려는 프로세스를 sleep을 하고 sleep queue에 넣는다. 그리고 Semaphore의 값이 양수가 되어 critical section에 진입이 가능하게 된다면 sleep queue에서 대기중인 프로세스를 깨워 실행시킨다.<br>Sleep을 한다는 것에 대해 조금 생각해 볼 필요가 있다. 예전에는 프로세스가 sleep을 하기 위해서는 I&#x2F;O를 요청하는 경우였다. Time quantum을 전부 다 소진한 프로세스는 다시 ready queue로 들어가게 된다. 이제는 이런 경우가 아닌 새롭게 voluntary(자발적인) sleep의 개념이 생긴 것이다.<br>스케줄링과 조금 엮어서 생각을 해보면, P를 통해 critical section에 진입을 하지 못한 프로세스들은 sleep을 하게되는데 V 호출이 일어나게 되면 sleep 하고 있는 프로세스들에게 critical section에 다시 진입하도록 깨워주어야한다. 실제 구현체에서는 V 호출이 일어나면 현재 semaphore로 인해 sleep 하고있는 모든 프로세스들을 다 깨운다. 그리고 다음 critical section에 대한 진입은 scheduler에게 맡긴다.  </p><h4 id="Mutex-vs-Semaphore"><a href="#Mutex-vs-Semaphore" class="headerlink" title="Mutex vs Semaphore"></a>Mutex vs Semaphore</h4><p>Semaphore는 서로 다른 프로세스들에서 P와 V를 각각 호출할 수 있다. 즉 process A에서는 P를 호출하고 process B에서는 V를 호출할 수 있다.<br>이러한 이유로 Semaphore에는 여러가지 발생할 수 있는 문제들이 존재하는데 예를들어 P를 호출하지 않았는데 V를 호출한다던가(Accidental Release), P를 호출한 프로세스가 다시 P를 호출하는 재귀적 deadlock(Recursive Deadlock) 등이 있다.<br>다만 Mutex의 경우는 lock을 획득한 주체만이 unlock할 수 있다. 만약 lock을 획득한 프로세스가 아니라 다른 프로세스에서 unlock을 시도하게되면 unlock이 불가능하다. 이것이 Semaphore와의 가장 큰 차이인데 이를 <strong>The principle of ownership</strong>이라고 한다. Mutex에서는 본인이 lock을 들고있어야 하므로 위에서 설명한 Accidental Release가 발생할 수 없고 Recursive Deadlock도 쉽게 해결할 수 있다.  </p><p>Mutex는 concurrent하게 실행되는 code에 대한 protecting에 초점을 맞춘다면, Semaphore는 한개의 스레드가 다른 스레드에게 signal을 보내는 의미가 강하다고 생각할 수 있다.  </p><h4 id="Deadlock-in-Semaphore"><a href="#Deadlock-in-Semaphore" class="headerlink" title="Deadlock in Semaphore"></a>Deadlock in Semaphore</h4><p>Semaphore에 Deadlock을 해결하기 위한 간단한 방법이 있다.<br>Semaphore간의 partial order를 정해놓고 그 order에 맞춰서 semaphore를 잡으면 deadlock을 막을 수 있다. 여러개의 프로세스들에서 Semaphore의 P를 잡고 들어가고 V를 호출하는 순서들을 정확히 맞추는 방법이다.  </p><p>애초에 이런 문제를 해결하기 위해 맨처음 진입시 전체적인 global state를 관리할 수 있는 큰 semaphore 1개를 잡고 할 수 있지않느냐 라고 질문을 할 수 있다. 이러면 deadlock은 발생하지 않는다. 다만 concurrency를 높게 살리기 힘들다.   </p><h2 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h2><p>위에서 살펴보았듯이 Semaphore에는 발생할 수 있는 여러가지 문제점이 있다.(Accidental Release, Recursive Deadlock)<br>그래서 이를 조금 더 high level에서 해결하려는 요구를 하게되었다. High-level 언어에서 procedure를 호출하는 것만으로도 동기화를 해결할 수 있도록 하는 것이다.<br>그래서 Application level에서는 P, V 연산을 호출할 필요없이 지원하는 procedure만 호출하도록 한다. 이를 <strong>Monitor</strong>라고 한다.<br>예시로는 자바에는 synchronized keyword를 지원한다. synchronized block을 지정하여 동기화되는 영역을 지정할 수 있다.  </p><h4 id="Monitor-in-Java"><a href="#Monitor-in-Java" class="headerlink" title="Monitor in Java"></a>Monitor in Java</h4><p>내부적으로 Entry queue를 만들고 synchronized block에는 한번에 한개의 스레드만 진입할 수 있도록 한다.  </p><p align="center">    <img alt="Synchronized in Java" src="/images/synchronized.png"/></p>  <h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://developer.arm.com/documentation/dht0008/a?lang=en">ARM Synchronization Primitives Development Article</a></li><li><a href="https://blog.feabhas.com/2009/09/mutex-vs-semaphores-%E2%80%93-part-1-semaphores/">Semaphores</a></li><li><a href="https://blog.feabhas.com/2009/09/mutex-vs-semaphores-%e2%80%93-part-2-the-mutex/">The Mutex</a></li></ul><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      
      <comments>https://tk-one.github.io/2019/07/24/os-synchronization/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>운영체제 5편 - CPU 스케줄링</title>
      <link>https://tk-one.github.io/2019/07/23/os-scheduling/</link>
      <guid>https://tk-one.github.io/2019/07/23/os-scheduling/</guid>
      <pubDate>Tue, 23 Jul 2019 07:31:06 GMT</pubDate>
      
      <description>&lt;p&gt;이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.&lt;br&gt;맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  &lt;/p&gt;
&lt;p&gt;이번 편은 CPU 스케줄링에 대한 내용입니다.  &lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.<br>맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다.  </p><p>이번 편은 CPU 스케줄링에 대한 내용입니다.  </p><span id="more"></span><h2 id="CPU-Scheduling"><a href="#CPU-Scheduling" class="headerlink" title="CPU Scheduling"></a>CPU Scheduling</h2><p>CPU 스케줄링은 어떻게 프로세스들에게 CPU의 사용을 분배할 것인가에 대한 내용이다. 메모리 내에 ready queue에 있는 실행이 준비된 상태인 프로세스들 가운데 하나를 선택하여 CPU를 할당한다.<br>여기서 어떤 프로세스를 다음차례로 선택할 것인가에 대해 많은 알고리즘이 존재하는데 밑에서 살짝만 보고 넘어갈 것이다.<br>결국 CPU의 idel 시간을 최소로하고 CPU를 최대한 효율적으로 사용하기 위해서 CPU 스케줄링이 존재하는 것이다.  </p><p>CPU 스케줄링의 결정은 다음 2가지 상태변화일때 일어난다.  </p><ul><li>I&#x2F;O로 인해 Process가 Running 상태에서 Waiting 상태로 가는경우</li><li>Time quantum을 전부 소진하여 Process가 Running 상태에서 Ready 상태로 가는경우</li></ul><p>스케줄링은 크게 <strong>비선점형 스케줄링</strong>(<strong>Non-preemptive scheduling</strong>)과 <strong>선점형 스케줄링</strong>(<strong>Preemptive scheduling</strong>)으로 나뉜다.  </p><ul><li><strong>비선점형 스케줄링</strong><br>OS가 강제로 프로세스의 CPU사용을 중단시키지 않는다. 프로세스 스스로 CPU 사용을 중단하거나 I&#x2F;O를 하는 상황에서만 스케줄링 된다.  </li><li><strong>선점형 스케줄링</strong><br>OS가 현재 수행중인 프로세스를 강제로 중단시킨다. 보통 프로세스의 Time quantum을 다 소진한 상황에서 스케줄링을 한다.</li></ul><p>각 스케줄링들의 특성을 비교하기 위해서는 기준이 필요하다. 여기서의 기준을 몇가지 소개하겠다.  </p><ul><li><strong>CPU Utilization</strong><br>전체 CPU 사용중에서 user process들이 작업을 처리하는 시간을 CPU Utilization이라고 한다.  </li><li><strong>Throughput</strong><br>단위시간동안 처리하는 프로세스 개수이다. 즉 얼마나 많은 양을 처리할 수 있는가이다.  </li><li><strong>Response Time</strong><br>하나의 프로세스 관점에서 입출력을 시작해서 첫 결과가 나오기까지 나오는 시간이다.  </li><li><strong>Waiting Time</strong><br>프로세스가 얼만큼 queue에서 대기를 하고있느냐이다.  </li><li><strong>Turnaround Time</strong><br>프로세스가 시작해서 끝날때까지의 시간이다.</li></ul><p>만약 Dos(Denial Of Service)공격을 받으면 패킷도착에 대한 interrupt가 미친듯이 발생한다. 그러면 대부분의 CPU 시간은 Kernel에서 ISR을 처리하는데 보내게 된다. 그러면 CPU utilization이 극도로 낮아지게 된다.<br>가장 이상적인 스케줄링은 CPU utilization이 높을수록 좋고, Throughput은 높을수록 좋고 Response Time은 낮을수록 좋다. 하지만 이런 이상적인 스케줄러를 설계하는 것은 현실상 불가능하다.<br>그래서 각 컴퓨터의 사용종류에 따라 특정 기준을 극대화하는 방식을 사용하는데 예를들어 슈퍼컴퓨터의 경우에는 CPU utilization을 극대화 한다. 그리고 우리가 보통 사용하는 서버를 포함한 일반 컴퓨터는 Response Time을 줄이는 것에 초점을 두게된다.  </p><p>일반적인 프로세스는 CPU Burst(CPU로 연산하는 시간)과 I&#x2F;O Burst(I&#x2F;O 처리를 위해 기다리는 시간)을 번갈아 가며 수행한다.<br>이 두가지 특성에 따라 긴 CPU burst를 가진 프로세스를 <strong>CPU-bound 프로세스</strong>라고 하고 짧은 CPU burst를 가지고 I&#x2F;O 시간이 많은 프로세스를 <strong>I&#x2F;O-bound 프로세스</strong>라고 한다.<br>어떤 종류의 프로세스들이 많은지에 따라 CPU 스케줄링 기법의 효율성이 달라진다.  </p><h2 id="선점형-Preemptive-스케줄링"><a href="#선점형-Preemptive-스케줄링" class="headerlink" title="선점형(Preemptive) 스케줄링"></a>선점형(Preemptive) 스케줄링</h2><p>선점형 스케줄링은 어떻게 구현할 수 있을까?<br>어떻게 preemption이 일어날 수 있을까? user program을 돌리고 있을때 kernel이 다시 제어권을 얻어야지만 현재 수행중인 프로세스를 멈추고 스케줄링을 할 수 있을 것이다. 현재 수행중인 프로세스를 멈추고 kernel이 제어권을 가지고 와야한다.<br>어떻게 하면 kernel이 제어권을 가져올 수 있을까? 두가지가 있다. 바로 <strong>interrupt</strong>와 <strong>시스템콜</strong>이다.<br>timer interrupt가 발생하면 그에맞는 ISR이 수행되면서 수행되고 있는 프로세스를 멈추고 다음 프로세스로 스케줄링을 해줄 수 있을 것이다.  </p><p>다만 timer interrupt가 발생한다고 해서 그때마다 스케줄링을 하는 것은 아니다. 커널에서는 interrupt를 받지않도록 구간을 코드로 설정할 수가 있다. 예전의 커널에서 멀티스레드를 지원하지는 않았을때에는 현재 수행중인 프로세스가 시스템 콜을 통해 Kernel에 들어가게 되면 그때에는 아예 interrupt를 disable 시켰다. 그러므로 Kernel에서 나와 user mode로 나오는 순간 그때 모두 처리되었다.<br>지금은 이렇게 동작하지는 않고 <strong>preemption point</strong>라고 하여 나중에 동기화 쪽에서도 살펴보겠지만 kernel 에서는 많은 state를 관리하는데 동기화의 문제로 코드의 특정 구간에서는 scheduling이 되지 않도록 설정할 수 있다. 이 경우에도 interrupt 자체는 발생한다. 이것이 무슨말이냐면 해당 코드 구간에 들어갈때에는 preemptive disable flag를 ON 해놓고 들어가게되면, timer interrupt가 발생했을때 그에 맞는 ISR을 수행하게 되는데 이때 preemptive disable flag가 설정이 되어있다면 스케줄링 하지 않는다.  </p><h2 id="CPU-Scheduling-알고리즘"><a href="#CPU-Scheduling-알고리즘" class="headerlink" title="CPU Scheduling 알고리즘"></a>CPU Scheduling 알고리즘</h2><p>여러가지 CPU Scheduling 알고리즘이 존재한다. 어떤 알고리즘들이 있는지만 간단하게 짚고 넘어가겠다.  </p><h4 id="FCFS-First-Come-First-Serve-Scheduling"><a href="#FCFS-First-Come-First-Serve-Scheduling" class="headerlink" title="FCFS (First-Come First-Serve) Scheduling"></a>FCFS (First-Come First-Serve) Scheduling</h4><p>FCFS는 ready 큐에 있는 순서대로 CPU를 할당한다. FIFO 큐로 간단하게 구현이 가능하다.  </p><h4 id="Shortest-Job-First-Scheduling"><a href="#Shortest-Job-First-Scheduling" class="headerlink" title="Shortest Job First Scheduling"></a>Shortest Job First Scheduling</h4><p>CPU Burst 시간이 가장 짧은 프로세스에게 CPU를 할당한다. 그러므로 최소의 평균대기시간을 제공한다.<br>다만 프로세스들의 CPU Burst 시간을 미리 알 수 없기때문에 대략적으로 CPU Burst 시간이 짧은 프로세스를 선택하여 time quantum 만큼 수행하는 방식으로 구현하기도 한다.  </p><h4 id="Priority-Scheduling"><a href="#Priority-Scheduling" class="headerlink" title="Priority Scheduling"></a>Priority Scheduling</h4><p>프로세스에 priority를 두고 priority에 따라 CPU를 할당한다. 예를들어 time quantum마다 현재 프로세스보다 높은 priority를 가진 프로세스를 찾아 다음에 수행하는 방식으로 구현할 수 있다.<br>다만 priority가 낮은 프로세스들에게 기아현상(starvation)이 발생할 수 있고 이를 극복하기 위해 waiting 시간에 따라 프로세스의 priority를 높여주는 Aging 기법을 같이 사용하기도 한다.  </p><h4 id="RoundRobin-Scheduling"><a href="#RoundRobin-Scheduling" class="headerlink" title="RoundRobin Scheduling"></a>RoundRobin Scheduling</h4><p>선점형 스케줄링 방식으로 RoundRobin으로 동작한다. 보통 time quantum을 10ms 밑으로 두고 time quantum이 소진된 프로세스는 ready queue의 맨 끝에 들어가 다시 CPU 할당을 기다린다.<br>이 방식은 Response Time이 짧은 이점이 있다.  </p><p>RoundRobin 방식과 context switching을 연결하여 조금 더 살펴보도록 하자.<br>Time quantum이 소진되면 다음 ready queue의 헤드에 있는 프로세스를 다음에 실행을 시키게 될텐데 time quantum은 어떻게 측정할 수 있을까?<br>Kernel은 time quantum을 하나하나 정확하게 측정해서 scheduling을 하지 않는다. 그리고 Linux 또한 고정된 Time quantum을 상수로 가지지 않고 매 프로세스마다 다르도록 varaible 한 값을 가지고 있다. timer interrupt가 발생하면 대략적으로만 시간을 approximate 하여 스케줄링 한다.  </p><p>RoundRobin 방식에서는 time quantum이 너무 작으면 너무 많은 Context switching이 일어날 수 있는데 이렇게 CPU는 바쁜데 user process는 CPU를 할당받지 못한 상황을 <strong>Thrashing 현상</strong>이라고 한다.<br>나중에도 다루겠지만 Virtual Memory에서도 메모리가 너무 작아지면 Thrashing 현상이 일어날 수 있다.  </p><h4 id="Multi-Level-queue-Scheduling"><a href="#Multi-Level-queue-Scheduling" class="headerlink" title="Multi-Level queue Scheduling"></a>Multi-Level queue Scheduling</h4><p>큐를 한개만 두지말고 여러개를 두는 방식이다.<br>예를들어, Foreground queue와 Background queue를 두고 Foreground queue는 roundRobin 방식으로 Background queue는 FCFS 스케줄링 방식으로 구현할 수 있겠다. 그래서 Realtime process는 foreground queue에 할당하고, batch job process는 background queue에 할당할 수 있다.  </p><h2 id="Multi-core-Scheduling"><a href="#Multi-core-Scheduling" class="headerlink" title="Multi core Scheduling"></a>Multi core Scheduling</h2><p>여러개의 코어를 사용하는 시스템의 경우 스케줄링은 더욱 복잡해진다.<br>여러개의 코어를 사용하는 경우 ready queue를 어떻게 설계하는 것이 좋을까?  </p><ul><li>global queue 한개를 두고 이 queue를 바라보고 모든 코어들에게 프로세스를 할당해주는 방식</li><li>각 코어마다 queue를 두고 프로세스를 할당해주는 방식</li></ul><p>위의 두가지가 대표적인 설계방식일 수 있겠다.<br>다만 전자의 global queue를 두는 방식은 scalable 하지 않아 사용하지 않는다. 왜냐하면 queue라는 것은 kernel의 메모리에 있는 data structure이다. 커널에서 한개의 큐를 바라보고 여러개의 코어에 할당하는 방식으로 구현하게 되면 동기화 이슈를 해결하기 힘들기 때문이다. 그리고 후자의 코어마다 queue를 두게되면 얻게되는 이점이 있다.<br>각 코어마다 queue를 두게되면 프로세스는 계속 같은 코어에 할당이 되는데 CPU Cache의 hit rate를 높일 수 있기때문에 더 효율적으로 작동할 수 있다.</p><p>Affinity 라고 특정 코어에 프로세스를 할당할 수 있는 방법이 있다.  </p><h2 id="In-current-linux"><a href="#In-current-linux" class="headerlink" title="In current linux"></a>In current linux</h2><p>현재 Linux Kernel 에서는 2.6.23 버전부터 CFS(Completely Fair Scheduler) 라고 불리는 CPU scheduling 알고리즘을 기본값으로 사용한다.<br>이는 각 task마다 고정된 CPU time을 가지는 것이 아닌,  각 task의 weight 정도에 따라 서로 다른 CPU time을 할당한다.  </p><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/OS/">OS</category>
      
      
      <category domain="https://tk-one.github.io/tags/OS/">OS</category>
      
      
      <comments>https://tk-one.github.io/2019/07/23/os-scheduling/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Memory False Sharing 이란?</title>
      <link>https://tk-one.github.io/2019/07/16/false-sharing/</link>
      <guid>https://tk-one.github.io/2019/07/16/false-sharing/</guid>
      <pubDate>Tue, 16 Jul 2019 07:23:28 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;Memory False Sharing이란 무엇일까? 말 그대로 직역하면 메모리 거짓 공유이다. 이게 무엇을 뜻하는지 알아보자.  &lt;/p&gt;
&lt;h2 id=&quot;Cache-Coherence&quot;&gt;&lt;a href=&quot;#Cache-Coherence&quot; class=&quot;h</description>
        
      
      
      
      <content:encoded><![CDATA[<p>Memory False Sharing이란 무엇일까? 말 그대로 직역하면 메모리 거짓 공유이다. 이게 무엇을 뜻하는지 알아보자.  </p><h2 id="Cache-Coherence"><a href="#Cache-Coherence" class="headerlink" title="Cache Coherence"></a>Cache Coherence</h2><p>먼저 Cache Coherence를 알아야 한다. 멀티코어 환경에서 코어마다 cache가 각각 존재한다. 흔히 말하는 캐시의 개념으로 자주 사용하는 데이터들을 메모리보다 더 빠른 캐시에 저장함으로서 메모리에서 읽지 않고 바로 캐시에서 가져옴으로 성능적으로 큰 이득을 볼 수 있다. 이런 멀티코어 환경에서 각 코어들에 있는 cache들의 일관성을 유지하는 것을 Cache Coherence라고 말한다.<br>만약에 Core 1에서 메모리 주소 <code>X</code>에 있는 값을 읽기 위해 먼저 메모리에서 읽고 이를 Core 1 캐시에 저장하였다. 다음으로 Core 2에서 메모리 주소 <code>X</code>에 있는 값을 읽기 위해 메모리에서 이를 읽고 이를 Core 2의 캐시에 저장하였다. 만약 Core 1에서 add 연산으로 해당 변수를 원래 값인 1에서 5로 증가시켰다고 해보자. 그러면 Core 1의 캐시는 5로 업데이트 된다. 여기서 Core 2가 이 변수를 읽으면 무슨 값이 반환되어야 할까? 1일까 5일까?<br>Cache Coherence는 캐시에서 공유하고 있는 데이터의 값의 변경사항이 적시에 시스템 전체에 전파될 수 있도록 하는 원칙이다.<br>Cache Coherence는 다음 2가지가 필요하다.  </p><ul><li><strong>Write Propagation(쓰기 전파)</strong><br>어떠한 캐시에 데이터가 변경이 되면 이 cache line을 공유하고 있는 다른 캐시에도 이 변경사항이 전파되어야 한다.</li><li><strong>Transaction Serialization</strong><br>특정 메모리 주소로의 read&#x2F;write은 모든 프로세서에게 같은 순서로 보여야 한다.</li></ul><p>두번째의 Transaction Serialization은 다음 예를 보면 이해하기 쉽다.<br>Core 1,2,3,4 가 있을때 이들 모두 초기값이 0인 변수 <code>S</code>의 캐시된 복사본을 각 캐시에 가지고있다. 프로세서 P1은 이 <code>S</code>의 값을 10으로 변경한다. 그리고 프로세서 P2가 이어서 이 <code>S</code>의 값을 20으로 변경한다. 위의 Write Propagation를 보장한다면 P3와 P4가 이 변경사항을 볼 수 있다. 다만 프로세서 P3는 P2의 변경사항을 본 후, P1의 변경사항을 봐서 변수 <code>S</code>의 값으로 10을 반환받는다. 그리고 프로세서 P4는 원래의 순서에 따라 P1의 변경사항을 보고, P2의 변경사항을 그 다음으로 봐서 20을 반환받는다. 결국 프로세서 P3, P4는 캐시의 일관성을 보장할 수 없는 상태가 되었는데 이처럼 Write Propagation 하나만으로는 Cache Coherence가 보장이 안된다.<br>이를 위해 변수 <code>S</code>에 대한 Write는 반드시 순서가 지정이 되어야한다. Transaction Serialization이 보장이 된다면 <code>S</code>는 위의 예제에서 10으로 write하고 그리고 20으로 write 했기 때문에, 절대 변수 <code>S</code>에 대해 값 20으로 읽고 그다음 값 10으로 읽을 수가 없다. 반드시 값 10으로 읽고 그 다음 20으로 읽는다.    </p><p>이처럼 Cache Coherence를 유지하기 위해서는 다른 프로세서에서 갱신한 캐시 값을 곧바로 반영을 하든 지연을 하든 해서 다른 프로세서에서 사용할 수 있도록 해주어야 한다. 캐시 일관성을 유지하기 위한 다양한 프로토콜들이 존재하며 대표적으로 <a href="https://en.wikipedia.org/wiki/MESI_protocol">MESI</a> 프로토콜이 있다.  </p><p>Cache Coherence에 대한 내용은 여기까지만 보도록 하고 cache line 이라는 것을 알아보자.</p><h2 id="Cache-Line"><a href="#Cache-Line" class="headerlink" title="Cache Line"></a>Cache Line</h2><p>메인 메모리의 내용을 읽고 캐시에 이를 저장하는 과정에서 메모리를 읽을때에 이를 읽어들이는 최소 단위를 <strong>Cache Line</strong>이라고 한다. 메모리 I&#x2F;O의 효율성을 위해서이며 spatial locality(공간 지역성)을 위해서이다. 보통의 cache line은 64byte 혹은 128byte로 이루어져 있으며 위에서 설명한 Cache Coherence도 cache line의 단위로 작동한다. 이렇게 cache line으로 읽어들인 데이터들로 캐시의 data block을 구성하게 된다.<br>또 cache line은 고정된 주소단위(보통은 64byte)로 접근하고 가져온다. 예를들면 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">+---------+-------+-------+<br>| address | x1000 | int a |<br>+---------+-------+-------+<br>| address | x1004 | int b |<br>+---------+-------+-------+<br>| address | x1008 | int c |<br>+---------+-------+-------+<br>| address | x100B | int d |<br>+---------+-------+-------+<br>| ....... | ..... | ..... |<br>+---------+-------+-------+<br></code></pre></td></tr></table></figure><p>위와 같은 메모리 구조가 있다고 할때 변수 a를 읽을때는 주소 x1000부터 cache line의 크기인 64byte만큼 가져오고, 변수 c를 읽을때에는 주소 x1008부터 64byte를 읽는게 아니다. 고정된 주소단위로 변수 c를 읽을때에도, write를 할때에도 주소 x1000으로 읽는다는 의미이다.  </p><p>이제 cache line을 알았으니 다시 Memory False Sharing으로 돌아가자.  </p><h2 id="Memory-False-Sharing"><a href="#Memory-False-Sharing" class="headerlink" title="Memory False Sharing"></a>Memory False Sharing</h2><p>Memory False Sharing은 동일한 cache line을 공유할때 Cache Coherence로 인해 성능이 느려지는 안티패턴을 의미한다. 위에서 본 예제로 다시 이해해 보자.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">+---------+-------+-------+<br>| address | x1000 | int a |<br>+---------+-------+-------+<br>| address | x1004 | int b |<br>+---------+-------+-------+<br>| address | x1008 | int c |<br>+---------+-------+-------+<br>| address | x100B | int d |<br>+---------+-------+-------+<br>| ....... | ..... | ..... |<br>+---------+-------+-------+<br></code></pre></td></tr></table></figure><p>메모리 구조가 위와 같을때 스레드 2개가 있고 스레드 1은 int 변수 a를 1씩 계속 더하는 일을 하고, 스레드 2는 int 변수 c를 1씩 계속 더하는 일을 한다고 해보자.  </p><ul><li>Thread 1: while (true) { a++ }</li><li>Thread 2: while (true) { c++ }</li></ul><p>더하기를 시작하기 전 이미 해당 cache line이 캐시에 올라와있다면 CPU 캐시의 상태는 다음과 같을 것이다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Core 1 Cache<br>+-------------+---------+----------------------+<br>| mem address | invalid | data block (64 byte) |<br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br>|    x1000    |  false  | a | b | c | d | .... | <br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br><br>Core 2 Cache<br>+-------------+---------+----------------------+<br>| mem address | invalid | data block (64 byte) |<br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br>|    x1000    |  false  | a | b | c | d | .... | <br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br></code></pre></td></tr></table></figure><p>상황을 쉽게 이해하기 위해 두 스레드는 서로 다른 프로세서에서 실행되지만 시간상으로 볼 때 서로 사이좋게 번갈아 가며 add를 한다고 해보자.  </p><ol><li>Thread 1: a++</li><li>Thread 2: c++</li><li>Thread 1: a++</li><li>Thread 2: c++이런 순서대로 실행이 된다고 하자. 먼저 1번의 a++가 발생했을때는 Core 1의 cache에서 a에 해당하는 부분이 1을 증가시킨 값으로 write가 일어나게 된다. 하지만 여기서 문제가 발생한다. 바로 다음 2번이 실행되기를 원하지만 그 사이에는 많은 일이 발생한다. 1번을 실행하였을때 Core 1 Cache의 data block 값이 변하였고, Cache Coherence protocol에 의하여 2가 실행되기 전에 하드웨어 병목이 생긴다.  MESI protocol에 의해 Core 2의 해당 cache line의 상태가 invalid 상태로 바뀌고 Core 2가 다시 데이터를 읽으려면해당 cache line이 invalid 이기 때문에 Core 1에서 읽거나 해야한다.<br>즉 cache line단위로 관리되기 때문에 Thread 2는 변수 a와는 전혀 상관이 없는 작업임에도 불구하고 변수 a에 대한 변경때문에 성능저하가 급격하게 나타나게 된다.<br>두 변수 a와 c가 서로는 전혀 상관이 없는 데이터임에도 불구하고 같은 cache line에 있기때문에 CPU는 특정 변수가 변경될때마다 캐시 일관성을 맞추기 위해 작업을 하게된다. 이는 성능하락으로 이어진다.</li></ol><h2 id="어떻게-해결할-수-있을까"><a href="#어떻게-해결할-수-있을까" class="headerlink" title="어떻게 해결할 수 있을까?"></a>어떻게 해결할 수 있을까?</h2><p>어떻게하면 이를 해결할 수 있을까?<br>일종의 cache line size에 맞추어 padding을 넣어 서로 다른 cache line에 속하게할 수 있다. 예는 다음과 같다.  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Core 1 Cache<br>+-------------+---------+----------------------+<br>| mem address | invalid | data block (64 byte) |<br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br>|    x1000    |  false  | a |     padding      | <br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br><br>Core 2 Cache<br>+-------------+---------+----------------------+<br>| mem address | invalid | data block (64 byte) |<br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br>|    x1040    |  false  | c |     padding      | <br>+-------------+---------+----------------------+<br>|    .....    |  .....  | .................... | <br>+-------------+---------+----------------------+<br></code></pre></td></tr></table></figure><p>이처럼 변수뒤에 padding을 붙여줌으로서 서로 다른 cache line에 속하게 하면 위 같은 False Sharing 문제를 해결할 수 있다.<br>C++에서는 <code>alignas</code> 함수를 사용하여 padding을 넣어줄 수 있다.  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">alignas</span>(<span class="hljs-number">64</span>) <span class="hljs-type">int</span> a = <span class="hljs-number">0</span>;<br><span class="hljs-built_in">alignas</span>(<span class="hljs-number">64</span>) <span class="hljs-type">int</span> c = <span class="hljs-number">0</span>;<br></code></pre></td></tr></table></figure><p>자바도 이와 비슷한 방법으로 자바 8부터 @jdk.internal.vm.annotation.Contended 라는 어노테이션을 지원한다.<br>먼저 클래스 내부필드에 어노테이션을 적용하는 방법을 알아보자. 클래스 내부 필드에 이를 적용하게 되면 해당 필드는 앞뒤로 empty bytes로 패딩을 추가함으로서 object 안의 다른 필드들과 다른 cache line을 사용하도록 해준다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Counter1</span> &#123;<br>    <span class="hljs-meta">@jdk</span>.internal.vm.annotation.Contended<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count1</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count2</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>@Contended에는 group tag라는 것도 지원하는데 이 group tag는 필드단위에 적용되었을때에만 작동한다. Group은 서로 다른 모든 그룹과 독립된 cache line을 가지게 된다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Counter1</span> &#123;<br>    <span class="hljs-meta">@jdk</span>.internal.vm.annotation.Contended(<span class="hljs-string">&quot;group1&quot;</span>)<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count1</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br><br>    <span class="hljs-meta">@jdk</span>.internal.vm.annotation.Contended(<span class="hljs-string">&quot;group1&quot;</span>);<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count2</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br><br>    <span class="hljs-meta">@jdk</span>.internal.vm.annotation.Contended(<span class="hljs-string">&quot;group2&quot;</span>);<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count3</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>위의 예처럼 group tag를 지정해주면, count1 변수와 count2 변수는 같은 그룹으로 지정이 되어있고 count3는 다른 그룹으로 지정되어있다.<br>이런 경우 count1과 count2는 count3과는 다른 cache line을 가지게 되며 count1과 count2는 그룹이 같으므로 같은 cache line으로 될 수 있다.  </p><p>Contended 어노테이션은 클래스에도 적용할 수 있는데, 클래스에 적용하게되면 모든 field들이 같은 group tag를 가지는 것과 동일하다. 하지만 JVM 구현체에 따라서 다른 isolation 방법을 사용할 수 있다. 전체 object를 isolate 기준으로 할수도 있고 각 field 들을 isolate 기준으로 할수도 있다. (HotSpot JVM 기준으로는 class에 Contended 어노테이션이 적용되어있다면 모든 field 앞에 padding을 적용하는 것 같다. <a href="https://github.com/openjdk/jdk/blob/319b4e71e1400f8a482f0ab42377d40056c6f0ac/src/hotspot/share/classfile/classFileParser.cpp#L4236">implementation in HotSpotJVM</a>)  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@jdk</span>.internal.vm.annotation.Contended<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Counter1</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count1</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">volatile</span> <span class="hljs-type">long</span> <span class="hljs-variable">count2</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>Contended 어노테이션은 이 용도에 맞게 각 object들이 서로 다른 스레드에서 접근하는 상황일때 사용하면 성능향상을 가져올 수 있을것이다.<br>실제 Contended 어노테이션은 ConcurrentHashMap 구현이나 ForkJoinPool.WorkQueue 등에서 사용하고 있다.  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://en.wikipedia.org/wiki/False_sharing">https://en.wikipedia.org/wiki/False_sharing</a></li><li><a href="http://shumin.co.kr/comp-arch-false-sharing/">http://shumin.co.kr/comp-arch-false-sharing/</a></li><li><a href="https://www.baeldung.com/java-false-sharing-contended">https://www.baeldung.com/java-false-sharing-contended</a></li></ul><br/><br/><br/><br/>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0/">컴퓨터구조</category>
      
      
      <category domain="https://tk-one.github.io/tags/java/">java</category>
      
      <category domain="https://tk-one.github.io/tags/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0/">컴퓨터구조</category>
      
      
      <comments>https://tk-one.github.io/2019/07/16/false-sharing/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>자바 ForkJoin Framework(포크조인)</title>
      <link>https://tk-one.github.io/2019/07/15/fork-join-pool/</link>
      <guid>https://tk-one.github.io/2019/07/15/fork-join-pool/</guid>
      <pubDate>Mon, 15 Jul 2019 12:36:19 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;이번 글은 자바 7에 도입된 Fork&amp;#x2F;Join Framework에 대한 내용입니다.  &lt;/p&gt;
&lt;h2 id=&quot;Fork-x2F-Join-Framework&quot;&gt;&lt;a href=&quot;#Fork-x2F-Join-Framework&quot; class=&quot;head</description>
        
      
      
      
      <content:encoded><![CDATA[<p>이번 글은 자바 7에 도입된 Fork&#x2F;Join Framework에 대한 내용입니다.  </p><h2 id="Fork-x2F-Join-Framework"><a href="#Fork-x2F-Join-Framework" class="headerlink" title="Fork&#x2F;Join Framework"></a>Fork&#x2F;Join Framework</h2><p>자바 7에는 Fork&#x2F;Join Framework가 도입되었는데 이는 <code>ExecutorService</code>의 구현체로서 이를 활용하면 작업들을 멀티코어를 사용하도록 작업할 수 있습니다. 기본적으로 Fork&#x2F;Join은 하나의 병렬화할 수 있는 작업을 재귀적으로 여러개의 작은 작업들로 분할하고 각 subtask들의 결과를 합쳐서 전체 결과를 반환합니다.<br>Fork&#x2F;Join은 divide-and-conquer 알고리즘과 굉장히 비슷하다. 다만 Fork&#x2F;Join Framework는 한가지 중요한 개념이 있는데 이상적으로는 worker thread가 노는경우가 없다. 왜냐하면 Fork&#x2F;Join Framework에서는 work stealing이라는 기법을 사용해서 바쁜 worker thread로 부터 작업을 steal, 즉 작업을 훔쳐온다.<br>먼저 ForkJoin의 thread pool에 있는 모든 thread를 공정하게 분할한다. 각각의 스레드는 자신에게 할당된 task를 포함하는 double linked list를 참조하면서 작업이 끝날때마다 queue의 헤드에서 다른 task를 가져와서 처리한다. 다만 아무리 공정하게 태스크들을 분할한다고 해도 특정 한 스레드는 다른 스레드보다 자신에게 할당된 태스크들을 더 빠르게 처리 할 수 있는데, 이렇게 자신에게 주어진 태스크들을 다 처리해서 할일이 없어진 스레드는 다른 스레드의 queue의 tail에서 작업을 훔쳐(steal)온다. 모든 태스크가 다 끝날때까지 이 과정을 반복하여 스레드간의 작업부하를 균등하게 맞출 수 있다.</p><h2 id="ForkJoinPool"><a href="#ForkJoinPool" class="headerlink" title="ForkJoinPool"></a>ForkJoinPool</h2><p><code>java.util.concurrent.ForkJoinPool</code>은 위에서 설명한 work stealing 방식으로 동작하는 <code>ExecutorService</code>의 구현체이다. 우리는 ForkJoinPool의 생성자로 작업에 사용할 processor number를 넘겨줌으로서 병렬화 레벨을 정할 수 있다. 기본값은 <code>Runtime.getRunTime().availableProcessors()</code> 결과로 결정된다. 또 다른 특징으로는 <code>ExecutorService</code>들의 구현체와는 다르게 ForkJoinPool은 모든 워커 스레드가 데몬스레드로 명시적으로 program을 exit할 때 shutdown을 호출할 필요가 없다. ForkJoinPool의 내부에서 workerthread를 등록하는 과정에서 daemon 스레드로 설정한다.  </p><h2 id="ForkJoinTask"><a href="#ForkJoinTask" class="headerlink" title="ForkJoinTask"></a>ForkJoinTask</h2><p><code>java.util.concurrent.ForkJoinTask</code>는 ForkJoinPool에서 실행되는 task의 abstract class이다. <code>ForkJoinTask&lt;V&gt;</code>는 <code>Future&lt;V&gt;</code>를 구현한다. <code>ForkJoinTask</code>는 일종의 light 한 스레드라고 생각하면 쉽다. 여러개의 task 들이 생성되면 이들은 ForkJoinPool의 설정된 스레드들에 의해 실행되게 된다.<br><code>RecursiveAction</code>와 <code>RecursiveTask&lt;R&gt;</code>가 ForkJoinTask의 서브클래스들인데 이들 또한 abstract class들이다. 그래서 이들을 구현한 서브클래스를 만들어서 사용한다. <code>RecursiveAction</code>과 <code>RecursiveTask&lt;R&gt;</code>의 차이점은 <code>RecursiveAction</code>은 태스크가 생성하는 결과가 없을때 사용하고 결과가 있을때에는 <code>RecursiveTask&lt;R&gt;</code>을 사용한다. 두 클래스 모두 abstract method인 <code>compute()</code> 를 구현해야한다.  </p><p>ForkJoinTask는 현재 실행상태를 확인하기 위한 몇가지 메서드를 제공한다.<br><code>isDone()</code>은 태스크가 완료되었는지의 여부를 반환한다. <code>isCompletedNormally()</code>는 태스크가 cancellation이나 exception 없이 완료되었는지의 여부를 반환하고 이외에도 <code>isCancelled()</code>, <code>isCompletedAbnormally()</code> 등의 메서드를 제공한다.  </p><h2 id="RecursiveTask-활용"><a href="#RecursiveTask-활용" class="headerlink" title="RecursiveTask 활용"></a>RecursiveTask 활용</h2><p>스레드 풀을 이용하기위해 <code>RecursiveTask&lt;R&gt;</code>의 서브클래스를 만들어보자. parameter type R은 결과 형식을 의미한다. 우리는 RecursiveTask의 compute 메서드를 구현해야 한다.<br><code>protected abstract R compute();</code><br>compute 메서드는 태스크를 서브태스크로 분할하는 로직과 더이상 분할할 수 없을때 개별 서브태스크의 결과를 생산할 알고리즘을 정의한다. 따라서 대부분의 compute 메서드의 구현은 다음과 같다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">if</span> (태스크가 충분히 작거나 분할할 수 없으면) &#123;<br>    태스크 계산<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    태스크를 두 서브태스크로 분할한다.<br>    태스크가 다시 서브태스크로 분할되도록 이 메서드를 재귀적으로 호출한다.<br>    모든 서브태스크의 연산이 완료될때까지 기다린다.<br>    각 서브태스크의 결과를 합친다.<br>&#125;<br></code></pre></td></tr></table></figure><p>그렇다면 1부터 N까지의 합을 구하는 프로그램을 Fork&#x2F;Join Framework를 사용하여 작성해보자. 코드는 다음과 같다.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.concurrent.RecursiveTask;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ForkJoinSumCalculator</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">RecursiveTask</span>&lt;Long&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span>[] numbers;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> start;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> end;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">THRESHOLD</span> <span class="hljs-operator">=</span> <span class="hljs-number">10_000</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">ForkJoinSumCalculator</span><span class="hljs-params">(<span class="hljs-type">long</span>[] numbers)</span> &#123;<br>        <span class="hljs-built_in">this</span>(numbers, <span class="hljs-number">0</span>, numbers.length);<br>    &#125;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">ForkJoinSumCalculator</span><span class="hljs-params">(<span class="hljs-type">long</span>[] numbers, <span class="hljs-type">int</span> start, <span class="hljs-type">int</span> end)</span> &#123;<br>        <span class="hljs-built_in">this</span>.numbers = numbers;<br>        <span class="hljs-built_in">this</span>.start = start;<br>        <span class="hljs-built_in">this</span>.end = end;<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> Long <span class="hljs-title function_">compute</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-type">int</span> <span class="hljs-variable">size</span> <span class="hljs-operator">=</span> end - start;<br>        <span class="hljs-keyword">if</span> (size &lt;= THRESHOLD) &#123;<br>            <span class="hljs-keyword">return</span> computeSequentially();<br>        &#125;<br><br>        <span class="hljs-type">ForkJoinSumCalculator</span> <span class="hljs-variable">leftTask</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ForkJoinSumCalculator</span>(<br>                numbers, start, start + size / <span class="hljs-number">2</span>);<br>        leftTask.fork();<br>        <span class="hljs-type">ForkJoinSumCalculator</span> <span class="hljs-variable">rightTask</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ForkJoinSumCalculator</span>(<br>                numbers, start + size / <span class="hljs-number">2</span>, end);<br>        <span class="hljs-type">long</span> <span class="hljs-variable">rightResult</span> <span class="hljs-operator">=</span> rightTask.compute();<br>        <span class="hljs-type">long</span> <span class="hljs-variable">leftResult</span> <span class="hljs-operator">=</span> leftTask.join();<br>        <span class="hljs-keyword">return</span> leftResult + rightResult;<br>    &#125;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> <span class="hljs-title function_">computeSequentially</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-type">long</span> <span class="hljs-variable">sum</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> start; i &lt; end; i++) &#123;<br>            sum += numbers[i];<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> sum;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>그리고 다음과 같이 ForkJoinPool의 invoke 메서드를 사용해 실행시켜보자.  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.concurrent.ForkJoinPool;<br><span class="hljs-keyword">import</span> java.util.concurrent.ForkJoinTask;<br><span class="hljs-keyword">import</span> java.util.stream.LongStream;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Main</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> <span class="hljs-variable">N</span> <span class="hljs-operator">=</span> <span class="hljs-number">30_000_000L</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String args[])</span> &#123;<br>        <span class="hljs-type">long</span>[] numbers = LongStream.rangeClosed(<span class="hljs-number">1</span>, N).toArray();<br>        ForkJoinTask&lt;Long&gt; task = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ForkJoinSumCalculator</span>(numbers);<br>        <span class="hljs-type">long</span> <span class="hljs-variable">sum</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ForkJoinPool</span>().invoke(task);<br>        System.out.println(sum);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>Fork&#x2F;Join을 사용할 때 왼쪽 작업과 오른쪽 작업에 모두 fork를 호출하는게 자연스러운것 처럼 보이지만 한쪽에는 fork를 호출하는 것 보다 compute를 호출하는게 더 효율적이다. 한 태스크에는 이 Fork&#x2F;Join 스레드를 실행시킨 스레드를 재사용할 수 있으므로 불필요한 태스크를 다른 스레드에 할당하는 오버헤드를 피할 수 있다.<br>또 멀티코어에 Fork&#x2F;Join을 사용하는게 무조건 순차처리보다 빠르지 않다. 각 서브태스크의 실행시간이 새로운 태스크를 forking하는데 드는 시간보다 충분히 길수록 좋다.</p><p>위의 예제에서는 덧셈을 수행할 숫자가 만개 이하면 분할을 더이상 하지 않고 계산했다. 그러면 현재는 태스크가 3천개가 생성되는데 어차피 코어의 수는 정해져있으므로 코어가 3개라면 각 코어마다 1천만개씩 덧셈을 수행하면 딱 알맞게 효율적으로 동작하지 않을까?<br>그렇지는 않다. 실제로는 코어 개수와 관계없이 적절하게 작은 크기로 분할된 많은 태스크를 forking 하는것이 바람직하다. 1천만개씩 덧셈을 수행하도록 한다고 해도 각 3개의 코어에서 이루어지는 작업이 동시에 끝나지는 않는다. 각 태스크에서 예상치못하게 지연이 생길 수 있어 작업완료시간이 크게 달라질 수 있다. 다만 Fork&#x2F;Join Framework는 work-stealing 기법으로 idel한 스레드는 다른 스레드의 workQueue로 부터 작업을 훔쳐오기 때문에 모든 스레들에게 작업을 거의 공정하게 분할할 수 있다. 그러므로 태스크의크기를 작게 나누어야 스레드 간의 작업부하 수준을 비슷하게 맞출 수 있다.  </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://howtodoinjava.com/java7/forkjoin-framework-tutorial-forkjoinpool-example/">https://howtodoinjava.com/java7/forkjoin-framework-tutorial-forkjoinpool-example/</a></li><li><a href="https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html">https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://tk-one.github.io/categories/java/">java</category>
      
      
      <category domain="https://tk-one.github.io/tags/java/">java</category>
      
      <category domain="https://tk-one.github.io/tags/thread/">thread</category>
      
      
      <comments>https://tk-one.github.io/2019/07/15/fork-join-pool/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
