{"posts":[{"title":"코로나 확진후기","text":"결론부터 말하자면 2022년 3월 3일 코로나19 양성판정을 받았다.계속 언제, 어디서부터 감염되었는지 생각해봤는데 아무리 생각해도 잘 모르겠다.간 곳이라고는 집앞에 있는 스터디카페밖에 없었고 그 스터디카페는 정말 큼지막해서 사람간의 동선도 거의 겹치지 않는다. 확진받은 날인 3월 3일 아침에 일어났을때 평소보다 목이 조금 잠겨있었다.이거 코로나인가..? 라고 생각은 들지 않았을만큼 아주 미세한 차이였고 그래도 뭔가 찝찝해서 아침에 집에 있는 키트를 사용하여 진단해본 결과 음성이 나왔다.그래서 내가 확진되었다는 생각은 전혀 하지 못했지만, 오후에 갑자기 엄마가 확진판정을 받았다고 했다.그 말을 듣자마자 나도 바로 PCR검사를 받으러 갔고 결과는 다음과 같다. 흑흑흑..처음에 카톡으로 알림이 왔을때 나한테 잘못온줄 알았다.아니 내가 걸린다고?심지어 당일 아침에 자가진단키트로는 음성이 나왔어서 더 예상을 못했던 것 같다. 기가막히게 확진을 받은 다음날 새벽부터 오한이왔고 심한 목감기처럼 통증이 심해졌다.회사에도 현 상황을 공유해드리고 아픈건 맞지만 어차피 재택근무이고 업무하는데에는 무리가 없다는 개인적인 판단하에 계속해서 재택근무를 하기로 결정했다.감사하게도 회사에서 상비약을 마련해서 바로 퀵으로 보내주셨고 정말 많은 도움이 되었다.양도 굉장히 많아서 우리가족 다같이 나눠먹었다. 이제 자가격리가 3일 남았는데 얼른 끝났으면 좋겠다…공부해야될게 산더미인데 스터디카페를 못간다니..","link":"/2022/03/07/coronavirus-confirmed/"},{"title":"Node.js의 Buffer를 제대로 이해해보자","text":"이 글은 Daajust의 Do you want a better understanding of Buffer in Node.js? Check this out. 글을 번역 한 글입니다. 모든 저작권과 권리는 Daajust에게 있습니다. 곳곳에 의역이 들어가있는 점 양해부탁 드립니다 :) Node.js에서 Buffer, Stream, Binary Data 를 마주치게 될때면, 이것들을 아예 모르는 사람들도 있고, 늘 이 개념에 대해 두려워하는 사람도 있으며, 자기 자신이 이를 정확하게 이해 하고있는지도 종잡을 수 없는 사람도 많습니다. 이 내용들은 nodejs 시니어나 package 모듈 개발자정도는 되어야 다룰 수 있을 것 같은 기분이 듭니다. 실제로, Node.js 자체의 소스코드나 유명라이브러리들의 소스코드를 까보시면, Buffer을 안마주칠 확률보다 마주칠확률이 더 높습니다. 특히 컴퓨터공학과정을 거치지 않은사람이, nodejs로 개발 할때에 저런 단어들을 마주칠때면 더욱 두려움을 느끼곤 합니다. 안타깝게도, 시중에 있는 많은 Node.js 개발서적들은 Node.js의 핵심기능들이나 그 기능구현의 이유에 대해 설명하기도 전에, 바로 Node.js 패키지들을 이용해서 Web Application을 구현하는 방법에 대해 설명합니다. 그리고 어떤사람들은 뻔뻔하게도, Buffer, Stream, Binary Data 같은 것들은 어차피 다룰 일이 없을 것이기 때문에, 알 필요도 없다고 주장합니다. 물론 스스로 그저그런 Node.js 개발자로 살기로 선택한 사람이라면, 이런것들을 마주칠 일은 없을 수도 있습니다. 그게아니라, Node.js에 대해 더 깊이 이해하고 싶거나, 도대체 Buffer 같은 Node.js 의 핵심기능에 대해 궁금하고 더 알고싶은 호기심이 들면, 바로 이것이 제가 글을 쓰는 이유입니다 :) 이 글에선 Node.js의 핵심기능중 일부를 설명하고, 이 글을 읽게되면 Node.js에 대한 이해가 한 차원 더 높아질 것입니다. Node.js 공식문서에서는 Buffer를 다음과 같이 정의합니다. 바이너리 데이터들의 스트림을 읽거나, 조작하는 매커니즘.이 Buffer클래스는 Node.js의 일부로 도입되어 TCP 스트림이나 파일시스템같은 작업에서의 octet 스트림과의 상호작용을 가능하기 위해 만들어졌습니다.* octet Stream은 일반적으로 8bit 형식으로 된 데이터를 의미합니다. 음.. 정의를 읽어보니 약간 말이 복잡합니다.이 말을 쉽게 풀어쓰면 즉, Buffer클래스는 바이너리 데이터들의 스트림을 직접 다루기 위해 Node.js API에 추가되었습니다.라고 볼 수 있습니다. 그래도 말이 좀 간단해 졌죠 ? 하지만 아직까지는 Buffer, Stream, Binary Data .. 이런 단어들이 확 와닿지는 않습니다.이제부터 처음부터 끝까지 이것들을 살펴봅시다. Binary Data ? 이게 뭔가요?우리는 컴퓨터가 이진수로 데이터를 저장하고 표현한다는걸 이미 알고있습니다. 이진수는 단순히 1과 0의 집합입니다. 예를들어, 다음은 서로 다른 이진수 5개이며, 이 이진수들은 서로다른 1과 0의 집합입니다. 10, 01, 001, 1110, 00101011 각각 이진수에서 1 혹은 0으로 되어있는 자리를 비트(bit)라고 합니다. 이는 Binary digIT의 약자입니다. 컴퓨터가 어떤 데이터를 저장하거나 표현하기 위해서는, 컴퓨터는 해당 데이터를 이진수로 변환해야합니다.예를들어, 숫자 12를 변환하려면 컴퓨터는 12를 이진수인 1100로 변환해야합니다. 하지만 우리가 다루는 데이터중에는 숫자만 있는것은 아닙니다. 우리가 다루는 데이터중에는 문자열도 있고 이미지, 비디오형식도 있습니다. 컴퓨터는 모든 유형의 데이터를 이진수로 변환하는 법을 알고 있습니다.문자열을 예시로 들어봅시다.어떻게 컴퓨터가 문자열 ‘L’을 이진수로 나타낼까요 ? 어떤 문자를 이진수로 나타내기 위해서는 첫번째로 그 문자를 숫자로 변환해야 합니다. 숫자로 변환하면 그 숫자를 이진수로 변환하면 끝나기 때문이죠. 컴퓨터는 문자열 ‘L’에 대해서 먼저 ‘L’을 나타내는 숫자로 변환합니다. 어떻게 변환할까요?웹브라우저 콘솔을 열어 이 코드를 실행시켜보세요 : 'L'.charCodeAt(0)숫자 76이 출력되나요? 이 숫자는 문자 ‘L’을 나타내는 숫자 값입니다. 이는 Character Code 혹은 Code Point 라고 불립니다. 그렇다면 컴퓨터는 어떻게 이 문자를 보고 바로 매칭해서 숫자를 알려줄 수 있었을까요? 어떻게 문자 ‘L’을 보고 숫자 76을 알려줄 수 있었을까요? Character Set (문자 집합)Character Set은 각각문자를 숫자로 나타낼 수 있도록 정의해놓은 규칙입니다. 위에 보았듯이 ‘L’을 76으로 매칭할 수 있게 각각문자에 해당하는 숫자를 표로 정리해놓은 것이라고 보시면 됩니다. 그렇다고 한가지 Character Set 만 있는것은 아니고 여러가지 Set들이 있습니다. 이중에서 유명한 것은 우리가 자주 들어본 유니코드, 아스키코드 가 있습니다. 자바스크립트는 유니코드와 아주 궁합이 잘맞습니다. 사실은 브라우저에서 ‘L’을 76으로 표현한 것은 유니코드입니다. 이렇게 우리는 어떻게 컴퓨터가 문자를 숫자로 표현하는지 보았습니다. 이제는, 컴퓨터가 숫자 76을 이진표현로 변환할 것입니다. 우리는 그냥 단지 숫자 76을 이진수로 변환하면 될 것이라고 생각합니다. 과연 그럴까요? Character Encoding (문자인코딩)문자를 숫자로 나타내는 것에 규칙이 있는 것 처럼, 숫자를 바이너리 데이터로 나타내는 데에도 규칙이 있습니다. 정확히 말하면, 숫자를 몇 bit로 나타낼 것인가를 정하는 것입니다. 이것을 Character Encoding 이라고 부릅니다.Character Encoding의 정의중 하나가 UTF-8 입니다. UTF-8은 문자가 바이트단위로 인코딩 되어야 합니다. 1바이트는 8개 비트의 집합을 의미합니다. 즉 8개의 1 또는 0을 의미합니다. 그래서 문자를 바이너리로 나타내는데에는 8개의 1과 0으로된 집합이 사용됩니다. 위에 언급한 내용을 다시 이어나가자면, 숫자 12을 이진수로 나타내면 1100 입니다. UTF-8 명세에 따르면 숫자 12는 8bit로 구성되어야 합니다. 8bit로 구성하려면 12의 실제 이진수 표현의 왼쪽에 더 많은 bit를 추가해서 바이트로 만들면 됩니다.그래서 12는 00001100 으로 저장될 것입니다.이것이, 컴퓨터가 숫자 혹은 문자를 바이너리 데이터로 저장하는 방식입니다.이것과 비슷하게, 이미지나 비디오데이터 또한 바이너리 데이터로 저장하는 방식도 따로 정해져 있습니다. 이중에 핵심은, 컴퓨터는 모든 데이터를 바이너리 이진 데이터로 저장한다는 것입니다. Character Encoding 에 대해 더 자세히 알고싶으시다면, Character Encoding에 대해 자세히 알아보기를 한번 보시는걸 추천합니다. 우리는 이제 바이너리 데이터가 무엇인지를 알았습니다. 그렇다면 바이너리 데이터의 Stream은 무엇일까요 ? StreamNode.js 에서의 스트림은 간단하게 한 지점에서 다른 지점으로 이동하는 일련의 데이터를 의미합니다. 전체적인 의미로는, 만약 우리가 어떤 방대한 양의 데이터를 처리해야 할때, 모든 데이터가 전부다 사용가능 할때까지 기다리지 않아도 된다는 것입니다. 기본적으로 큰 데이터는 청크단위로 세분화되어 전송됩니다. 이말은, 처음 설명했던 Buffer의 정의에 따르면, 파일시스템에서 바이너리 데이터들이 이동한다는걸 의미합니다. 예를들어, file1.txt의 텍스트를 file2.txt로 옮기는 걸 의미합니다. 하지만, Streaming 하는동안에 buffer라는 것이 어떻게 바이너리 데이터를 다룰 수 있게 도와준다는 것일까요? 정확히 buffer는 무엇일까요? Buffer우리는 데이터들의 스트림이란 일련의 데이터들이 한지점에서 다른 지점으로 이동하는 것이라는 걸 배웠습니다. 하지만, 데이터들이 정확하게 어떻게 이동한다는 것일까요?일반적으로 데이터의 이동은 그 데이터를 가지고 작업을 하거나, 그 데이터를 읽거나, 무언가를 하기 위해 일어납니다. 하지만 한 작업이 특정시간동안 데이터를 받을 수 있는 데이터의 최소량과 최대량이 존재합니다. 그래서 만약에 한 작업이 데이터를 처리하는 시간보다 데이터가 도착하는 게 더 빠르다면, 초과된 데이터는 어디에선가 처리되기를 기다리고 있어야 합니다. 데이터를 처리하는 시간보다 훨씬빠르게 계속해서 새로운 데이터가 도착하면 어딘가에는 도착한 데이터들이 미친듯이 쌓일것이기 때문이죠. 반면에, 한 작업이 데이터를 처리하는 시간이 데이터가 도착하는 시간보다 더 빠르다면, 먼저 도착한 데이터는 처리되기 전에 어느정도의 데이터량이 쌓일때까지 기다려야 합니다. 바로 그 기다리는 영역이 buffer 입니다! 컴퓨터에서 일반적으로 RAM이라고 불리는 영역에서 streaming 중에 데이터가 일시적으로 모이고, 기다리며 결국에는 데이터가 처리되기위해 내보내어 집니다. Streaming과 Buffer의 과정을 버스정류장에 빗대어 설명할 수 있습니다. 어떤 버스정류장에서는, 어느정도 이상의 승객이 모이지 않거나, 출발시간 전일때에는 출발하지 않습니다. 그리고, 승객들은 버스정류장에 도착하는 속도도 도착하는 시간도 각각 다릅니다. 승객 자기자신도 버스도 그 어느 누구도 버스정류장에 승객이 도착하는걸 마음대로 제어할 수 없습니다. (승객이 버스정류장에 도착하는 것도 결국에는 승객 스스로 제어하는게 아니라 환경적 요인에 달려있다는 말입니다.)어쨌든 일찍 도착한 승객은 버스가 출발하기 전까지는 버스정류장에서 기다려야 합니다. 반대로 승객이 도착했을때, 버스가 이미 문을닫고 출발하기 직전상태 이거나 버스가 이미 출발한 상황일때에는 다음 버스를 기다려야 합니다. 어떤 경우든지, 승객이 기다리는 위치가 있습니다. 바로 버스정류장입니다. 바로 그게 Node.js에서의 Buffer입니다! Node.js는 데이터가 도착하는 시간이나 전송되는 속도를 제어할 수는 없습니다. Node.js가 결정할 수 있는건 언제 데이터를 내보내느냐 입니다. 버스를 언제 출발시킬 수 있는 제어권이 있는 것과 동일합니다. 아직 데이터를 내보낼 때가 아니면, Node.js는 데이터들을 일종의 대기영역인 RAM에 작은 영역인 buffer에 데이터를 넣어놓습니다. 일상생활에서 버퍼작동을 볼 수 있는 예로는 온라인 영상을 시청할 때 입니다. 유튜브를 보는 순간을 상상해보세요. 우리의 인터넷 연결상태가 매우 좋을때에는 영상 스트리밍이 끝날때까지 버퍼를 채우고 데이터가 처리될 수 있게 빠르게 내보내고, 다시 버퍼를 채우고 빠르게 내보내고를 반복합니다. 그러나 인터넷 연결상태가 좋지 못할때에는 첫번째 데이터셋을 처리하고 나서, 영상플레이어는 로딩 아이콘을 띠우면서 ‘buffering’이라는 텍스트를 보여줄 것입니다. 이것은 데이터가 더 모이고 도착할때 까지 기다린다는 의미입니다. 만약에 버퍼가 채워지고 데이터가 처리되면, 영상이 다시 보여지게 될 것입니다. 영상을 보여주는 동안에도, 계속해서 다음 데이터가 도착하고, 버퍼에 채워질 것입니다. 이것이 바로 버퍼입니다. 아까 보았듯이 버퍼에 대한 설명에서, 데이터가 버퍼에 있는동안 우리가 streaming되는 바이너리 데이터들을 조작하고 다룰 수 있다고 했습니다. 우리가 이런 raw한 바이너리 데이터들을 이용해서 어떤 종류의 작업을 할 수 있을까요? Node.js에서 구현한 Buffer 문서에는 우리가 다룰 수 있는 작업 내용들을 리스트로 정리해서 보여줍니다. 그 중에 몇가지를 살펴봅시다. 버퍼다루기우리는 직접 Buffer를 만들 수도 있습니다. Node.js는 Streaming 하는동안에 자동으로 Buffer를 만드는데요. 이것말고도 우리가 직접 Buffer를 만들고 직접 다룰 수 있습니다. 흥미롭죠? 한번 저희가 직접 만들어봅시다! 버퍼를 만드는데는 여러가지 방법이 있습니다. 한번 보시죠. 12345678// size가 10인 빈 buffer를 만듭니다. // 이 버퍼는 오직 10 byte만 담을 수 있습니다.const buf1 = Buffer.alloc(10);// buffer 에 데이터를 담아 만듭니다.const buf2 = Buffer.from(&quot;hello buffer&quot;); 버퍼를 만들었으면, 이제 마음대로 조작할 수 있습니다. 123456789101112131415161718192021222324252627282930// 버퍼구조를 조사합니다.buf1.toJSON();// { type: &apos;Buffer&apos;, data: [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ] }// 빈 버퍼입니다buf2.toJSON();// { type: &apos;Buffer&apos;, data: [ 104, 101, 108, 108, 111, 32, 98, 117, 102, 102, 101, 114 ] }// toJSON 메서드는 데이터를 Unicode Code Points 로 표현합니다.// 버퍼의 크기를 조사합니다.buf1.length // 10buf2.length // 12. 파라미터로 넣어주었던 content에 따라 자동으로 크기가 할당됩니다.// 버퍼에 쓰기buf1.write(&quot;Buffer really rocks!&quot;);// 버퍼를 Decoding 합니다.buf1.toString(); // &apos;Buffer rea&apos;// buf1 은 10 byte 밖에 담을 수 없기 때문에, 나머지 문자들은 할당할 수 없습니다. 우리는 버퍼로 많은 것들을 할 수 있습니다. 이들은 nodejs 공식문서에서 전부 확인할 수 있습니다. 마지막으로, 과제를 하나 남기고 가겠습니다. Node.js 핵심 라이브러리들중 하나인 zlib.js의 소스코드를 분석하여, 어떻게 버퍼를 활용하여 바이너리 데이터 스트림을 다루었는지 확인해보세요. 우리는 버퍼를 배웠기때문에 이제 두려울게 없습니다. 이건 사실 gzip을 구현한 것입니다. 코드분석을 하면서, 새로 알게된 것들을 글로도 한번 적어보시고, 댓글로도 같이한번 의견을 나눠보았으면 좋겠습니다 :) 이 글이 Node.js 의 Buffer를 이해하는데 도움이 되었으면 좋겠습니다. 혹시, 이 글이 도움이 되었고, 남들에게도 이 글을 알려드리고 싶으시다면 원본 Medium article에 클랩한번 눌러주세요! 감사합니다.","link":"/2018/08/28/nodejs-buffer/"},{"title":"마크다운(Markdown) 사용법","text":"이번 포스트는 markdown에 대해 알아보는 포스트입니다.자주 사용하는 markdown 키워드들 위주로 정리하였습니다. Italics and Bolds Italics1this is _italic_ this is italic Bolds1this is **bold** this is bold Italics and Bolds1this is **_italics and bolds_** this is italics and bolds Headers Header 1부터 6까지 지원한다.123456# header 1## header 2### header 3#### header 4##### header 5###### header 6 header 1header 2header 3header 4header 5header 6Linkinline 링크와 reference 링크가 있다. inline link1[블로그 바로가기](https://tk-one.github.io) 블로그 바로가기 reference link123Here&apos;s [블로그 바로가기][my github].[my github]: https://tk-one.github.io Here’s 블로그 바로가기. Image이미지도 링크와 비슷하게 inline 이미지와 reference 이미지가 있다. inline image1![alt text](url) reference image123![alt text][markdown image][markdown image]: /images/markdown.png 이미지의 width와 height를 직접 지정해야 하는 상황이면 HTML의 img tag를 직접 사용한다. 1&lt;img src=&quot;/path&quot; width=&quot;500px&quot; height=&quot;300px&quot; /&gt; BlockQuotes BlockQuotes를 만들기 위해서는 앞에 caret(&gt;)을 넣어준다.1&gt; this is quotes. this is quotes. BlockQuotes안에서도 다른 markdown 문법을 사용할 수 있다.1&gt; this is _italic_. this is italic. Lists unordered list 1234* list 1* list 2* list 3* list 4 list 1 list 2 list 3 list 4 ordered list12341. list 12. list 23. list 34. list 4 list 1 list 2 list 3 list 4 nested list 1234567* parent 1 * child 1 * child 2* parent 2 * child 1 * child 2 * child 3 parent 1 child 1 child 2 parent 2 child 1 child 2 child 3 child’s child 1 nested list 에서도 indent를 맞추어 주고 싶을때는 다음과 같이 한다.1234567891. this is for indent.same indent. same indent 2.2. this is for number 2. same indent. &gt; do blockquotes. this is for indent. same indent. same indent 2. this is for number 2. same indent. do blockquotes. hr(수평선)수평선은 다음과 같이 나타낸다. 1234* * *********- - - Paragraphs다음과 같은 문단을 작성한다고 해보자. 123this is paragraph.this is sentence 1.this is sentence 2. 하지만 이를 markdown으로 렌더링하면 줄바꿈이 되지 않고 다음과 같이 보인다. this is paragraph. this is sentence 1. this is sentence 2. 줄바꿈을 하려면 여러가지 방법이 있는데 첫번째는 hard break 라고 불리는 방법을 사용하는 것이다. 12345this is paragraph.this is sentence 1.this is sentence 2. this is paragraph. this is sentence 1. this is sentence 2. soft break 방식은 다음과 같다. 123this is paragraph.··this is sentence 1.··this is sentence 2.·· (여기서의 ·은 space를 의미한다.)즉, 줄의 마지막에 space 2개를 붙여주면 된다. this is paragraph.this is sentence 1.this is sentence 2.","link":"/2022/03/05/how-to-use-markdown/"},{"title":"Maven(메이븐) 이란?","text":"이 글은 박재성님이 쓰신 자바세상의 빌드를 이끄는 메이븐이라는 책과 메이븐 공식문서를 보고 정리한 글입니다.책이 있으신분은 메이븐 개념을 한번 머릿속에 정리하고 싶을때 읽으시면 매우 좋습니다. 책에 스토리로 이끌어가는 부분이 있어 매우 재밌게 읽을 수 있습니다.다만, 책이 절판되어 책을 구하고싶으신 분들은 아마 알라딘이나 다른 곳에서 중고서적으로 구매를 하셔야 합니다. Maven메이븐은 자바기반 프로젝트를 빌드하고 관리하기 위한 툴이다.요즘은 Gradle이 많이 쓰이지만 아직 maven을 사용하고 있는 프로젝트도 많다.메이븐은 빌드 프로세스를 최대한 쉽게 하는것을 목표로 하고 이 뿐만 아니라 프로젝트에 질높은 정보를 제공하고, 단일 빌드시스템을 제공하는 것을 목표로 한다. 다른 build tool 없이 자바프로젝트를 개발하게 되면 의존성관리 등 신경써야할게 한두가지가 아니다. 메이븐이 이를 도와준다.메이븐의 장점은 다음과 같다. 편리한 의존관계 관리를 지원한다. 모든 프로젝트가 일관된 프로젝트 디렉토리 구조, 빌드 프로세스를 유지할 수 있다. 다양한 메이븐의 플러그인을 활용할 수 있다. 프로젝트의 template을 만들수있다. 메이븐은 저장소를 지원해서 메이븐만 설치하면 프로젝트 build에 필요한 라이브러리, plugin을 저장소에서 우리의 PC로 자동으로 다운로드한다. 다운로드한 라이브러리들은 특정 디렉터리에 위치하게 되는데 이를 localRepository(로컬저장소)라고 부른다. 기본적으로는 ~/.m2/repository 에 위치하고 settings.xml로 설정을 변경할 수도 있다.또 메이븐은 처음 생성하는 프로젝트 종류에 따라 기반이 되는 template을 제공한다. 이를 이용해서 메이븐 기반 프로젝트를 생성할 수 있는데, 그러면 프로젝트의 기본적인 뼈대를 자동으로 생성할 수 있다. 메이븐의 이 같은 기능을 archetype이라고 한다. 메이븐 공식문서의 getting started에서 처음 메이븐 프로젝트를 만들게 될때도 archetype을 사용한다.다음 명령어로 메이븐 프로젝트를 생성해보자.1mvn -B archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 이를 실행하면 my-app 프로젝트의 디렉터리가 만들어지고 그 안에 pom.xml 파일이 생성된다.메이븐은 source code와 test code를 분리해서 관리하는데 source code는 src/main/java 에 위치하고, test code는 src/test/java 에 위치한다.여기서 사용한 groupId, artifactId는 뒷부분에서 다루겠다. 메이븐 기본명령어메이븐 명령어는 다음과 같은 형태를 가진다.mvn [options] [goal] [phase] 위의 명령어에서도 사용했던 -D 옵션들은 메이븐 설정파일(pom.xml)에 인자를 전달한다.예를 들어 단위테스트를 실행하지 않으려면 mvn -Dmaven.test.skip=true [&lt;phase&gt;]와 같이 실행할 수 있다. 메이븐에는 phase와 goal 개념이 있는데 이들을 이용하며 빌드를 실행할 수 있고, 빌드를 실행할 때 여러개의 phase와 goal을 실행할 수 있다. 예를들어 다음과 같이 다양한 형태로 실행이 가능하다.mvn clean test: clean phase와 test phase를 실행한다.mvn clean compiler:compile: clean phase와 compiler plugin의 compile goal을 실행한다.phase와 goal은 밑에서 자세히 다루겠다. Pom.xml위의 메이븐 archetype:generate 명령어로 메이븐 프로젝트를 생성했으면 pom.xml 파일이 생성된다. POM은 Project Object Model을 의미한다. 그러면 pom.xml의 각 element 들을 살펴본다. 먼저 생성된 pom.xml은 다음과 같다.12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;maven-app&lt;/name&gt; &lt;!-- FIXME change it to the project&apos;s website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; .. 이하생략 .. project: pom.xml의 최상위 element modelVersion: POM version. 최근버전이 4.0.0이다. artifactId: project를 식별하는 id를 의미한다. groupId 안에서 여러개의 project가 있을 수 있다. groupId: project를 생성하는 조직의 고유 id를 결정한다. 도메인 이름을 많이 사용하는데 꼭 그럴필요는 없다.groupId + artifactId는 값이 유일해야한다. 그렇지 않으면 중앙저장소에서 충돌한다. packaging: 어떤 방식으로 패키징할지 결정한다. jar, war 등을 설정가능하다. version: project의 현재 버전을 의미한다. 프로젝트 개발중에는 SNAPSHOT을 suffix로 사용가능하다.SNAPSHOT은 maven의 예약어이며 SNAPSHOT을 사용하면 라이브러리를 다른방식으로 관리한다. name: project 이름이다. url: project url이 있다면 이를 기입한다. dependencies: 프로젝트와 의존관계에 있는 라이브러리들을 관리한다. 각 프로젝트의 pom.xml은 기본적으로 최상위 POM이라고 불리는 설정을 상속한다. 그래서 pom.xml의 설정내용이 단순하더라도 메이븐의 기본 규약들을 전부다 따르는 것이 가능하다. 실제 정의된 설정들을 보려면 다음 명령어를 사용하면 된다. mvn help:effective-pom 설정되어있는 repository 정보를 담고있는 repositories 태그, plugin 설정정보를 담는 태그등 기존 pom.xml에 보이지 않았던 태그들을 볼 수 있다. 이 내용들이 기본적으로 최상위 POM에 존재했기 때문에 우리가 만든 project의 pom.xml은 단순하게 가져갈 수 있다. Lifecycle메이븐은 모든 빌드 단위가 이미 정의되어 있으며 이는 개발자가 임의로 변경할 수가 없다. 여기서 말하는 빌드 단위란 compile, test, package, deploy 등을 말한다.메이븐은 이와같이 미리 정의되어 있는 빌드 순서를 lifecycle이라고 하며 메이븐은 3개의 lifecycle을 제공한다. compile, test, package, deploy를 담당하는 기본 lifecycle 빌드 결과물 제거를 위한 clean lifecycle project document site를 생성하는 site lifecycle이다. 메이븐은 기본적으로 빌드후의 모든 산출물을 target 디렉터리에서 관리한다. target 디렉터리에 생성되는 하위디렉터리는 다음과 같다. target/classes: src/main/java의 소스코드가 컴파일된 class 파일들과 src/main/resources 디렉터리의 자원이 복사된다. target/test-classes: src/test/java의 소스코드가 컴파일된 class 파일들과 src/test/resources 디렉터리의 자원이 복사된다. target/surefire-reports: report 문서들이 위치한다. 기본 Lifecycle기본 lifecycle을 활용해 source code를 compile, test 등을 할 수 있는데 각 phase들을 살펴보면 다음과 같다. process-resources: src/main/resources의 모든 자원을 target/classes 로 복사한다. compile: src/main/java에 있는 source code를 compile한다. process-test-resources: src/test/resources의 모든 자원을 target/test-classes 로 복사한다. test-compile: src/test/java에 있는 source code를 compile한다. test: Junit 같은 unit test framework로 test를 진행하고 test가 실패하면 빌드실패로 간주한다.결과물을 target/surefire-reports 디렉터리에 생성한다. package: pom.xml의 packaging 값에 따라 압축한다.(jar, war) install: local repository에 압축한 파일을 배포한다. deploy: 원격저장소에 압축한 파일을 배포한다. 이처럼 maven 기본 lifecycle은 여러개의 phase로 구성되어 있으며, 각 phase는 의존관계를 가진다.process-resources ← compile ← process-test-resources ← test-compile ← test ← package이 순으로 의존관계를 가지고 있어서 package phase를 실행(mvn package)하면 의존관계에 있는 test phase가 먼저 실행되고, test phase는 compile phase에 의존관계가 있기때문에 compile phase가 먼저 실행된다.따라서 package phase를 실행하면 process-resources → compile → process-test-resources → test-compile → test → package 순으로 빌드가 진행된다. process-resources phase는 src/main/resources 에 있는 모든 자원을 test/classes 디렉터리로 복사하는데, 만약 다른 디렉터리에도 자원이 존재한다면 pom.xml에 따로 설정할 수 있다. package phase는 jar나 war형태로 압축하여 target 디렉터리에 위치시킨다.&lt;build&gt;/&lt;finalName&gt;에 값이 설정되어 있으면 {finalName}.{packaging} 형태로 압축파일이 생기고,값이 설정 안되어있다면 {artifactId}-{version}.{packaging} 형태로 된다. clean phase는 빌드한 결과물들을 제거하는 phase인데 이는 다른 phase와 관련이 없다.clean phase를 실행하지 않고 다른 phase를 실행할 때 불필요한 산출물들 때문에 오류가 날 수 있으므로 clean을 실행하고 빌드하는 습관을 가지면 좋다. Clean Lifecycleclean lifecycle은 빌드를 통해 나온 산출물을 모두 삭제한다.target directory를 삭제하는 것과 동일하다. Site Lifecyclesite lifecycle은 사용안하는 경우가 많은데 핵심만 짚고 넘어가자면,site, site-deploy phase를 사용해 실행가능하다. site lifecycle은 메이븐에 설정되어 있는 기본설정, 플러그인 설정에 따라 target/site directory에 문서 사이트를 생성한다. site-deploy는 이를 배포한다. Plugin메이븐에서 제공하는 모든 기능은 plugin을 기반으로 동작한다.메이븐 phase 또한 메이븐 plugin을 통하여 실질적인 작업이 실행된다. 따라서 phase가 실행되는 과정을 이해하려면 maven plugin을 먼저 이해해야 한다. 사용하고자 하는 maven plugin이 있다면 pom.xml에 다음과 같이 설정한다. 1234567891011&lt;project&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 이와 같이 사용하고자 하는 plugin의 groupId, artifactId, version을 명시하면 된다.version을 생략하면 가장 최신버전의 plugin이 설치된다.메이븐 plugin은 하나의 plugin에서 여러 작업을 실행할 수 있도록 지원하는데 여기서 각각 실행할 수 있는 작업을 goal이라고 정의한다.위의 compiler plugin은 하나지만 이 플러그인이 지원하는 goal은 compile(source directory의 compile), testCompile(test directory의 compile) 등이 있다. plugin은 다음과 같이 실행할 수 있다.mvn groupId:artifactId:version:goal 예를들어 앞의 compiler plugin의 compile goal은 다음과 같이 실행한다.mvn org.apache.maven.plugins:maven-compiler-plugin:2.1:compile 만약 settings.xml에 pluginGroup이 설정이 되어있다면 groupId를 생략이 가능하고,version을 생략하면 local repository에 있는 가장 최신 버전의 플러그인을 사용하며,plugin 이름이 maven-$name-plugin 이나 $name-maven-plugin 형식을 따른다면 $name 값만 명시할 수 있다.앞에서 실행했던 compile 플러그인을 다음과 같이 실행할 수 있다.mvn compiler:compile 앞부분에서 실행했던 mvn archetype:generate 명령도 mvn org.apache.maven.plugins:maven-archetype-plugin:generate를 축약한 것이다. 메이븐은 매우 많은 플러그인들을 활용할 수 있는게 큰 장점이다. 다양한 플러그인을 제공하고 있어서 원하는 개발환경을 얼마든지 만들어 나갈 수 있다. Phase와 Goal메이븐에서 phase는 build lifecycle에서 각 단계와 순서를 정의하는 개념으로 실제로 빌드작업을 하지는 않는다.실제 빌드작업은 해당 phase와 연결되어 있는 plugin의 goal에서 진행한다.mvn compile은 compile phase를 실행한 것인데 이는 compile phase와 연결되어 있는 compiler plugin의 compile goal이 실행되면서 컴파일 작업을 진행한다.기본 lifecycle에서 phase를 실행할 때 기본으로 연결된 plugin의 goal을 실행하는 구조이다.기본 lifecycle에서 phase에 연결되어 있는 plugin을 실행할 때에는 자동으로 메이븐 중앙저장소에서 plugin을 다운로드 한다. phase와 goal과의 관계를 보여주는 그림이다. 각 핵심 phase 별로 구체적인 내용을 알아보자. mvn compilecompile phase를 실행하면 먼저 의존관계에 있는 process-resources phase가 먼저 실행된다. process-resources phase는 src/main/resources 디렉터리에 있는 모든 자원을 target/classes 디렉터리로 복사한다.만약 src/main/java 안에서도 소스와 같은 패키지로 관리하는 리소스들이 있고 이들또한 target/classes에 복사되기를 원한다면 다음과 같은 설정을 하면된다.12345678910111213&lt;project&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/project&gt; 그러면 compile phase 실행시 src/main/java에 있는 *.java 파일을 제외한 모든 설정파일을 target/classes 로 복사한다.resource plugin과 compiler plugin 에 대한 자세한 정보는 다음 공식문서에서 확인할 수 있다.resources plugin : resources-plugincompiler plugin : compiler-plugin mvn testtest phase를 실행하면 process-test-resources phase가 먼저 실행되면서 src/test/resources 디렉터리의 자원복사를 먼저 진행한다.그리고 test-compile phase에서 src/test/java 디렉터리의 test code들을 컴파일한다.test phase는 target/test-classes 에 컴파일한 단위 테스트 클래스를 실행하고 그 결과물을 target/surefire-reports 디렉터리에 생성한다.기본적으로 test phase는 target/test-classes 에 있는 모든 단위 테스트 클래스를 실행하는데 특정 테스트 suite 별로 실행할 필요가 있다면 test option을 사용할 수 있다. mvn -Dtest=MyUnitTest test 이와 같이 특정 테스트 클래스만 실행할 수 있고 여러개의 test 클래스 들을 실행하고 싶다면 쉼표로 여러개를 정의하면된다. mvn packagepackage phase는 compile, test-compile, test, package 순으로 실행된 후 jar, war 파일이 target 디렉터리 하위에 생성된다.&lt;build&gt;/&lt;fileName&gt; 에 값이 설정되어 있고 jar로 패키징을 하게되면 {finalName}.jar 형태로 jar 파일이 생성된다. 만약 finalName element가 설정되어 있지 않다면 {artifactId}-{version}.{packaging} 이 압축파일 그리고 디렉터리 이름이 된다.예를들어, finalName element가 설정되어있지 않고 pom.xml 설정이 다음과 같다면 123456&lt;project&gt; &lt;groupId&gt;io.github.tk-one&lt;/groupId&gt; &lt;artifactId&gt;myapp&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; 파일은 다음 위치에 생성된다.target/myapp-1.0-SNAPSHOT/myapp-1.0-SNAPSHOT.war mvn installinstall phase는 package phase와 의존관계에 있기때문에 package phase를 먼저 실행한다. package phase에서 jar or war 파일로 압축을 완료하면 이를 local repository에 배포한다. mvn deploydeploy phase는 jar or war 파일을 원격저장소에 등록한다. 라이브러리 의존관계메이븐은 의존관계에 있는 라이브러리를 관리하기 위해 의존 라이브러리 관리기능을 제공한다. 이는 메이븐의 lifecycle과 더불어 메이븐의 핵심기능이기 때문에 반드시 이해하는게 좋다. 메이븐 저장소는 로컬저장소와 원격저장소로 나뉜다. 로컬저장소로컬저장소는 개발자 PC에 있는 저장소로 메이븐을 빌드할때 다운로드하는 라이브러리나 플러그인을 관리 및 저장한다.로컬저장소는 기본값으로는 ~/.m2/repository 에 위치한다. 원격저장소원격저장소는 외부에 위치하는 저장소로 사내에서 사용하는 저장소도 있고 중앙저장소라고 불리는 오픈소스 라이브러리나, 메이븐 플러그인 등을 저장하고 있는 저장소도 있다. 중앙저장소는 원격저장소 중 하나라고 생각하면 된다. 메이븐은 빌드를 할때 로컬저장소에 이미 다운로드한 라이브러리가 있으면 원격저장소에서 다운로드 하지않고 로컬저장소에 있는 라이브러리르 사용한다. 메이븐이 다운로드 하고자 하는 저장소는 repositories 태그로 설정할 수 있다.기본적으로 우리가 repositories 태그로 설정을 안하고 다운로드할 수 있는 이유는 최상위 POM에 이미 정의가 되어있기 때문이다.123456789&lt;project&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/project&gt; 여러개의 repository들을 추가할 수 있는데 그러면 메이븐은 라이브러리를 다운로드할때 repositories 태그에 있는 저장소 순서대로 다운로드를 시도한다. 위에서 생성한 myapp에서는 다음과 같이 dependencies 태그로 라이브러리를 관리한다.12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 이와 같이 설정하고 빌드하면 메이븐은 먼저 로컬저장소에 해당 라이브러리가 있는지 확인한다.없다면 메이븐은 중앙저장소에서 junit 4.11 버전이 있는지 확인하고 있다면 jar 파일을 로컬저장소에 다운로드한다.중앙저장소에 해당 라이브러리와 버전이 존재하는지 확인할 때에는 위에 repository 설정에 적힌 url을 바라보고https://repo.maven.apache.org/maven2/junit/junit/4.11/junit-4.11.jar 파일이 있는지를 파악한다.이와 같이 설정하고 빌드하면 메이븐은 중앙저장소에서 junit 4.11 버전의 jar 파일을 로컬저장소에 다운로드한다.로컬저장소에 다운로드 받는 위치는 기본적으로 다음과 같다.~/.m2/repository/junit/junit/4.11/junit-4.11.jar 그리고 메이븐은 로컬저장소에 다운로드한 라이브러리를 활용해 src/main/java 그리고 src/test/java 에 있는 source code들을 컴파일한다. version을 LATEST 혹은 RELEASE로 설정할 수도 있는데 그러면 항상 가장 최신버전의 라이브러리와 의존관계를 갖게된다.또, 한번 로컬저장소에 다운로드한 라이브러리는 다시 원격저장소에서 다운로드하지 않는데 이부분에서 애플리케이션이 개발단계에 있어 코드가 지속적으로 변경되는 상황이라면 SNAPSHOT을 활용하자.version 정보에 SNAPSHOT을 포함하게되면 빌드할 때마다 가장 최근에 배포한 라이브러리가 있는지 확인하고 로컬저장소에 있는것보다 최신일경우 이를 다운로드한다. scope메이븐에서는 사용하는 라이브러리의 성격에 따라 scope를 지정할 수 있다.JUnit 라이브러리의 경우 실제 배포할때는 필요없고 테스트를 진행할때만 필요하다. 이런경우 scope를 test로 주면된다. scope는 6가지 종류가 있다. compile: default scope이다. compile 및 deploy시 같이 제공해야하는 라이브러리이다. provided: compile 시점엔 필요하지만 deploy에 포함할 필요는 없는경우 사용한다. runtime: compile에는 필요없지만 runtime에는 필요한 경우 사용한다. test: test 시점에만 사용하는 라이브러리에 설정한다. system: provided scope와 비슷한데 로컬저장소에서 관리되는 jar파일이 아닌 우리가 직접 jar 파일을 제공해야한다. import: 다른 pom.xml 에 정의되어있는 의존관계설정을 가져온다. Dependency MechanismDependency Mechanism은 메이븐의 핵심중 하나이다.그러므로 메이븐이 어떻게 의존성을 관리하는지는 꼭 이해하고 넘어가는게 좋다. Dependency Transitive(의존성 전이)프로젝트에서 의존하는 라이브러리들의 숫자는 제한이 없지만 의존성 cycle이 있으면 문제가 발생한다.프로젝트에 외부 라이브러리를 하나씩 추가할때마다 그 라이브러리가 또 의존하고있는 라이브러리를 또 추가해야 하므로 의존관계에 있는 라이브러리 숫자가 증가한다.예를들어, project A가 B, C에 의존하고있다면 B가 의존하고있는 D, E, F 라이브러리가 또 필요할 것이고 또 D 가 의존하는 G 라이브러리도 필요할 것이다. 연쇄작용으로 의존하는 프로젝트는 점점 커진다.참고로 메이븐은 의존성이 있는 라이브러리가 또 어떤 라이브러리에 의존성을 가지고있는지 알기위해 jar 파일을 다운로드 하는 동시에 해당 라이브러리의 pom파일도 같이 다운로드 한다.메이븐은 위처럼 프로젝트 라이브러리 숫자가 급격히 증가하는 문제점을 해결하기 위해 라이브러리 제한이 가능하도록 의존성 전이 설정을 지원한다. Dependency mediation: 같은 의존성의 여러버전을 마주치게 되었을때 artifact의 어떤 버전을 사용할지 결정한다.메이븐은 이때 더 가까운 의존관계에 있는 버전의 의존관계를 선택한다.예를들어 다음과 같은 의존성이 있다고 가정한다. 123456 A├── B│ └── C│ └── D 2.0└── E └── D 1.0 이 예에서는 A를 build할때 D 1.0이 사용된다. 왜냐하면 A -&gt; B -&gt; C -&gt; D 2.0 보다 A -&gt; E -&gt; D 1.0 이 더 가깝기 때문이다.여기서 서로 depth 가 같은 상황이라면 먼저 명시된 라이브러리의 버전이 사용된다.만약 project A의 pom.xml에 직접 version을 적어주면 그 버전을 사용한다. 12345678 A├── B│ └── C│ └── D 2.0├── E│ └── D 1.0│└── D 2.0 이처럼 A에 직접 D 2.0 의 의존성을 추가하면 D 2.0 을 사용한다. Dependency management: 메이븐의 &lt;dependencyManagement&gt; element로 의존관계에 있는 artifact의 버전을 직접 명시랄 수 있다. Dependency scope: 현재 빌드상태에 맞는 라이브러리만 의존관계를 포함한다.즉, test scope를 가지는 경우 최종 배포산출물을 빌드하는 시점에는 포함되지 않는다. Excluded dependencies: 만약 A -&gt; B -&gt; C와 같이 의존성이 있으면 project A에서 명시적으로 project C에 대한 의존성을 &lt;exclusion&gt;태그를 사용해 명시적으로 제외시킬 수 있다. Optional dependencies: 만약 A -&gt; B -&gt; C와 같이 의존성이 있고 project B에 C가 optional로 설정이 되어있으면 project A를 빌드할때 project C에 대한 의존관계를 가지지 않는다. 메이븐의 Dependency Transitive가 의존관계를 최대한 잘 설정해 주겠지만 pom.xml에 항상 라이브러리의 명확한 version을 명시하는게 좋다.현재 프로젝트에서 의존하고있는 라이브러리의 tree를 보고싶다면 다음 plugin의 goal을 사용하면 좋다.mvn dependency:tree Propertypom.xml에서 발생하는 중복설정은 속성(property)를 정의하여 개선할 수 있다.보통 공통된 버전관리에 많이 사용하고는 하는데 예제를 보면 바로 이해할 수 있다.12345678910111213&lt;project&gt; &lt;properties&gt; &lt;spring.version&gt;3.0.1.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;${spring.version}&gt;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 속성은 &lt;properties&gt; element에서 &lt;property.name&gt;value&lt;/property.name&gt; 형태로 정의한다.그리고 이렇게 정의한 내용은 pom.xml 파일내에서 ${property.name} 으로 접근할 수 있다.","link":"/2020/03/24/maven-basic/"},{"title":"Node.js 이벤트루프 제대로 이해하기","text":"이 글은 Daniel Khan의 What you should know to really understand the Node.js Event Loop 글을 번역 한 글입니다. 모든 저작권과 권리는 Daniel Khan에게 있습니다. 곳곳에 의역이 들어가있는 점 양해부탁 드립니다 :) Node.js 는 이벤트 기반의 플랫폼입니다. 이 말은 즉슨, 노드에서 일어나는 모든 일은 어떤 이벤트에 대한 반응이라는 말과 같습니다. Node에서 일어나는 모든 처리는 전부 일련의 콜백내용들입니다. libuv 라는 추상화된 라이브러리가 이벤트루프라는 기능을 제공합니다. 이 이벤트루프는 아마 오해하기 가장 쉬운 개념일 것입니다. 저는 성능 모니터링 제작업체인 Dynatrace에서 일하며, 이곳에서 이벤트루프 모니터링에 대한 작업을 할때, 실제로 우리가 무엇을 측정하고 있는지 제대로 이해하기 위해서 많은 노력을 기울였습니다. 이 글에서는, 이벤트루프가 실제로 동작하는 방식과 어떻게 이를 올바르게 모니터링 하는 방법에 대해 다룰 예정입니다. 이벤트루프에 대한 오해Libuv는 Node.js에 이벤트루프를 제공하는 라이브러리 입니다. libuv의 코어 개발자였던 Bert Belder가 발표했던 Node 동작방식에 대한 강연에서, 우리가 이벤트루프를 구글에 검색했을때 나오는 이미지들과 그 이미지에서 설명하는 이벤트루프 동작방식들에 대해 이는 대부분이 틀렸으며 실제로 이렇게 동작하지 않는다고 설명합니다. 이제 우리가 할 수 있는 가장 흔한 오해들 부터 설명해보겠습니다. 오해 1: 이벤트루프는 우리가 실제로 작성한 코드와는 별개로 별도의 스레드에서 실행됩니다.오해우리가 작성한 자바스크립트를 코드를 실행시켜주는 main 스레드가 존재하고, 이벤트루프를 실행하는 또다른 쓰레드가 존재합니다. 비동기 작업이 실행 될 때마다, main 스레드는 이벤트루프 스레드에게 작업을 넘겨주고, 작업이 완료되면 이벤트루프 스레드가 main 스레드에게 신호를 보내 콜백을 실행합니다. 실제자바스크립트를 실행시키는 스레드는 단 하나뿐이며, 이 스레드가 바로 이벤트루프가 실행되는 스레드입니다. Node.js 어플리케이션에서 실행되는 사용자의 코드는 전부 콜백이라고 볼 수 있습니다. 이 콜백함수의 실행은, 이벤트루프에 의해 수행됩니다. 이 내용은 조금 뒤에서 깊이 다룰 예정입니다. 오해 2: 모든 비동기 작업은 스레드 풀에서 처리합니다.오해파일I/O나 외부와 통신하는 HTTP통신이나 데이터베이스통신같은 비동기 작업들은 항상 libuv가 제공하는 스레드 풀에서 수행됩니다. 실제Libuv는 기본적으로 비동기작업을 수행하기 위해 4개의 스레드를 스레드 풀에 할당하여 놓습니다. 현대의 OS들은 많은 I/O 작업들을 위해 비동기 Interface들을 제공합니다. 이 예로는 Linux의 AIO가 있습니다. Libuv는 가능하다면, 직접 OS의 비동기 Interface를 사용하며, 스레드풀에 작업을 넘겨주지 않습니다. 데이터베이스 같은 서드파티시스템에도 동일합니다. DB 드라이버를 이용해 스레드풀을 활용하지 않고, 비동기 인터페이스를 직접 사용합니다.즉, 다른 방법이 없는 경우에만, libuv는 비동기 작업들을 스레드 풀에 할당합니다. 오해 3: 이벤트 루프는 스택 or 큐 같은 것입니다.오해비동기 작업들은 FIFO(선입선출 큐)에 쌓이고, 이벤트루프는 이 큐를 계속 돌면서, 태스크가 완료되었을 때 콜백을 실행합니다. 실제이벤트루프에 큐 형식의 자료구조가 포함되있는건 맞습니다. 하지만 이벤트루프는 이 큐를 돌면서 실행하지 않고, 스택을 처리합니다. 이벤트루프는 round-robin 방식으로 차례차례 돌면서 처리되는 특정 작업들의 단계들로 이루어져 있습니다.약간 말이 어려운데요. 밑에서 자세히 다뤄보겠습니다. 이벤트루프 작업 이해하기이벤트루프를 이해하기 위해서는, 각 단계에 어떤 작업들이 수행되는지를 알아야 합니다.이벤트루프가 어떻게 실행되는 지는 다음과 같습니다. 여기서 초록색 박스는 이벤트루프의 각 단계들을 의미합니다.이 단계들을 하나씩 살펴보겠습니다. 자세한 설명은 Node.js 공식문서에 한국어로 아주 잘 설명되어있으니, 읽어보시면 도움이 될 것입니다. TimerssetTimeout() 또는 setInterval()에서 스케줄링된 모든 것은 여기서 처리됩니다. 하지만 그 타이머 안에 등록한 콜백함수의 실행은 Polling 단계의 가장 앞부분에서 이루어집니다. IO Callbacks대부분의 콜백들이 여기서 처리됩니다. 사용자가 작성한 Node.js의 모든 코드는 기본적으로 콜백입니다.(예: HTTP 요청에 의한 콜백함수는 다시 여러개의 콜백함수를 실행시킵니다.)Node.js 공식문서에서 설명하는 이벤트루프의 각 단계별 역할에 대한 내용이 이 글의 원본에 적혀있는 내용과 달라 정정합니다.Timers단계의 다음 단계인 Pending Callbacks 라고 불리는 단계에서는, TCP오류 같은 시스템 작업의 콜백을 반환합니다. 예를들어, TCP 소켓이 연결을 시도하다가 ECONNREFUSED 같은 메시지를 받으면, 이에 대한 오류보고를 위해 이에 대한 콜백함수가 IO Callbacks (pending callbacks) 단계의 큐에 추가됩니다. IO Polling이벤트루프가 Poll 단계에 진입하면,먼저 Timers 단계에 스케줄링 되었던 콜백들 중에, 시간이 지난 타이머의 콜백을 실행합니다.그 다음, Poll 단계의 큐에 있는 이벤트들을 처리합니다. 대부분의 콜백들은 이 Poll 단계에서 처리됩니다. Set Immediate (=Check)setImmediate()를 통해 등록된 모든 콜백을 실행됩니다. Close‘close’ 이벤트에 대한 모든 콜백이 실행됩니다. 이벤트루프 모니터링Node.js 어플리케이션에서 진행되는 모든 작업은 이벤트루프를 통해 실행됩니다. 이 말은 즉슨, 이 이벤트루프에 대한 메트릭을 측정할 수만 있다면, 이 정보들을 이용해 Node.js 어플리케이션에 대한 전반적인 상태나 성능에 대한 중요한 정보들을 알아낼 수 있습니다. 이벤트루프에서 런타임때 메트릭 정보들을 가져올 수 있는 API가 없기때문에, 각 모니터링 툴들이 직접 자기들만의 메트릭을 구축하여 이벤트루프에 대한 모니터링을 제공합니다. 여기서 우리가 이를 어떻게 해결했는지 한번 보겠습니다. Tick Frequency시간마다 tick 수입니다. Tick Duration1번의 tick에 걸리는 시간을 의미합니다. 실상황에서 Tick 빈도수와 Tick 소요시간 지표우리가 서로 다른레벨의 부하테스트를 줘봤을 때, 결과는 놀라웠습니다. 예제를 보여드리겠습니다. 다음 시나리오에서는 다른 HTTP 서버로 외부요청을 보내는 express.js 노드 프로그램을 호출합니다. 네가지 시나리오가 있습니다. Idle들어오는 요청이 없습니다. ab -c 5Apache bench를 이용하여 한번에 5개의 요청을 생성합니다. ab -c 10한번에 10개의 요청을 생성합니다. ab -c 10 (느린 백엔드)서버가 요청보내는 외부 HTTP 서버는 요청을 받으면 1초 후에 응답을 반환하도록 하여, 느린 백엔드를 만들었습니다. 이렇게 하면, 노드서버가 HTTP 서버로부터 응답을 기다리면서 흔히 말하는 back pressure가 발생합니다. 이 결과 차트를 보면, 재밌는 내용을 발견 할 수 있습니다. 이벤트루프 tick 수행시간과 빈도는 동적으로 조정됩니다. 노드 어플리케이션에 아무런 요청도 없는 상태인 경우, (타이머, 콜백 등) 대기중인 작업이 없는 상태로서, 위에서 설명했던 모든 단계들(timers, I/O callbacks, polling 등)을 미친듯이 계속 도는게 아니라 이벤트루프가 이에 적응하고 polling 단계에서 새로운 콜백이 등록되기를 잠시 기다립니다. 즉, 요청이 없을때에는 이벤트루프의 tick 빈도는 적고 tick의 duration은 긴 상황입니다. 이 상황에서의 지표는 이후에 진행한 느린 백엔드에서 진행했던 지표와 유사합니다. 이 노드 어플리케이션은 5개의 요청을 한번에 요청했던 시나리오에서 최고로 좋은 성능을 냅니다. 결과적으로, tick 빈도나 tick duration을 보고 분석할 때에는 노드 서버의 초당요청 수를 기준삼아 보아야합니다. 이 지표들은 우리에게 의미있는 내용들을 제공해주긴 했지만, 아직까지는 실제로 어느 단계(phase)에서 시간이 소비되었는지 알지 못하기 때문에 두가지 측정항목이 더 필요합니다. 작업 처리 지연시간작업처리 지연시간은 비동기 작업이 스레드 풀에서 처리 될 때까지 걸리는 시간을 의미합니다. 여기서 지연시간이 길다면, 이는 libuv 스레드 풀의 사용량이 많고 굉장히 바쁘다는 것과 같습니다. 이 지연되는 시간을 측정하기 위해, express.js 노드 어플리케이션에 Sharp 라이브러리를 이용하여 이미지를 가공하거나 처리해주는 API를 추가했습니다. 이미지처리는 부하가 있는 작업이기에, Sharp 라이브러리는 스레드 풀을 사용합니다. 그래프를 보면, Apache bench를 이용하여 5개의 요청을 동시에 보냈을때 이미지처리가 있는 요청과 그 전에서 진행했던 일반적인 HTTP서버 처리가 있는 요청은 확실하게 구별됩니다. 이벤트루프 지연시간이벤트루프 지연시간은 setTimeout(X)로 스케줄링 된 작업이 실제로 처리될 때 까지 추가로 소요되는 시간을 의미합니다. 이벤트루프 지연시간이 높다는 것은, 이벤트루프가 콜백을 처리하기 엄청 바쁜 상태라는 것과 같습니다. 이 이벤트루프 지연시간 측정을 위해, 이번에는 굉장히 비효율적인 알고리즘으로 피보나치를 계산하는 API를 추가했습니다. Apache bench를 이용해 피보나치를 계산하는 API로 부하를 주면 콜백 큐가 바쁘다는 걸 알 수 있습니다. 우리는 이러한 4가지 주요 메트릭(Tick Frequency, Tick duration, 작업처리 지연시간, 이벤트루프 지연시간)으로 Node.js 내부동작을 더 잘 이해할 수 있게 되었고, 꽤 중요한 내용들도 알게되었습니다. 이벤트루프 튜닝물론, 이 메트릭 정보들만으로 문제를 해결 할 수는 없습니다. 실제 이벤트루프가 고갈되었을때, 어떻게 대응 해야하는지 알아봅시다. 모든 CPU 활용Node.js 어플리케이션은 싱글 스레드로 작동합니다. 멀티코어 환경에서 1개의 Node.js 어플리케이션은 효율적으로 작동하지 않습니다. 낭비되는 CPU가 있기 때문입니다. Cluster Module을 사용하면, CPU 마다 child 프로세스를 쉽게 만들 수 있습니다. 각각의 child 프로세스는 각자 자신만의 이벤트루프가 존재하고 master 프로세스는 모든 자식들에게 요청을 분산시켜 줍니다. 스레드 풀 조정앞서 언급했듯이, libuv는 스레드 4개로 스레드 풀을 생성합니다. 스레드 풀의 기본 크기는 UV_THREADPOOL_SIZE 환경변수를 설정해서 수정할 수 있습니다. 이 방법은 I/O 작업이 많은 어플리케이션에서 도움이 될 수 있겠지만, 큰 스레드 풀은 메모리나 CPU를 고갈시킬 수 있음을 기억해야 합니다. 작업을 다른서비스에 맡기기만약 Node.js가 CPU사용이 과도하게 필요한 작업에서 사용된다면, 이 특정 작업에 더 잘맞는 다른 언어를 선택해서 그 쪽으로 처리를 옮겨 작업량을 줄이는 것이 가능한 방법일 수 있습니다. 요약 이벤트루프는 Node.js를 실행시켜주는 것이다. 이벤트루프의 기능을 잘못 알고 있는 경우가 많습니다 - 몇가지 단계들을 차례대로 반복하면서 순서대로 각 단계들의 작업들을 처리해나갑니다. 이벤트루프 자체에서 제공해주는 메트릭이 없으므로, 수집된 메트릭들은 APM 제품들마다 다릅니다. 이벤트루프 메트릭들은 병목현상에 대해 중요한 정보들을 제공해주지만, 이벤트루프와 실행중인 코드에 대한 깊은 이해가 가장 중요합니다. 감사합니다.","link":"/2019/02/07/nodejs-event-loop/"},{"title":"인터넷 메일시스템 (SMTP)","text":"이 포스트에서는 Application Layer의 SMTP (Simple Mail Transfer Protocol) 에 대해 알아본다.먼저, SMTP를 보기전에 인터넷 전자메일 시스템이 어떤식으로 동작하는지 알고있어야 한다. Mail System인터넷 메일 시스템은 크게 user agent, 메일서버, SMTP 이 3가지 요소로 구성되어 있다.아래의 그림을 보면서 이해하면 쉽다. user agentMS의 Outlook을 생각하면 쉽다. user agent는 사용자가 메일을 읽고, 작성하고, 전송할 수 있도록 해준다. 메일서버사용자가 메일 작성을 끝내면 user agent는 메시지를 메일서버로 보내게되고, 여기서 메시지는 메일서버의 output 메시지큐에 들어가게 된다.여기서의 메일서버는 송신자의 메일서버를 의미한다.송신자의 메일서버에서 수신자의 메일서버로 메시지가 전송되면, 메일들은 수신자의 메일서버안의 메일박스(mailbox)안에 저장되고 유지된다.만약, 수신자의 메일서버가 다운된 상황에서 송신자가 메일을 보내면 어떻게 될까?송신자가 메일을 전송하면, 먼저 메일이 송신자의 메일서버에 도착한다. 그리고 송신자의 메일서버는 메일을 수신자의 메일서버로 전송할 수 없을때, 메시지 큐(message queue)에 보관하고 있다가, 주기적으로 메일전송을 시도한다. SMTPSMTP는 Application layer에서 작동하는 메일전송 프로토콜이다. 위의 메일서버 설명에서, 한 메일서버에서 다른 메일서버로 메시지(메일)을 전송할때 사용하는 프로토콜이 SMTP이다.이뿐만 아니라, 송신자의 user agent에서 본인의 메일서버로 메일을 전송할때도 SMTP가 사용된다.SMTP는 TCP위에서 작동한다. 참고로 SMTP는 HTTP보다 훨씬 더 오래전부터 사용되었다. 이 설명을 기반으로 메일을 전송하는 간단한 시나리오를 보자.A가 B에게 메일을 전송하는 상황이다. A가 user agent를 통해 B에게 메일 내용을 작성하고 전송버튼을 누른다. A의 user agent는 메시지를 A의 메일서버에 보내게 되고, 메시지는 메일서버의 output message queue에 위치한다. A의 메일서버에서 동작하는 SMTP 클라이언트는 output message queue에 쌓여있는 메시지를 B의 메일서버로 전송하기 위해 먼저 TCP연결을 맺는다. TCP가 맺어진 후, SMTP 핸드쉐이킹을 하고 SMTP 프로토콜에 따라 B의 메일서버로 전송한다. B의 메일서버는 메시지를 수신한 후, 그 메시지를 B의 메일박스(mailbox)에 놓는다. B는 이후에 user agent를 실행하여 메일을 읽을 수 있다. 인터넷 메일 시스템은 대충 이런식으로 작동한다.(2번에서 A의 user agent가 메시지를 A의 메일서버로 보낸다고 되어있는데, 사실 여기서도 SMTP 프로토콜을 통해 전달된다.)그러면 이제 SMTP를 좀 더 자세히 보도록 한다. SMTP (Simple Mail Transfer Protocol)대부분의 Application layer protocol 처럼 SMTP는 송신자의 메일서버에서 수행하는 클라이언트와, 수신자의 메일서버에서 수행되는 서버를 가지고 있다. 메일서버가 상대 메일서버로 전송할때는 SMTP의 클라이언트로 동작하는 것이고, 메일서버가 상대 메일서버로 부터 메일을 받을때는 SMTP 서버로 동작하는 것이다. HTTP를 떠올리면 쉽다. 메일서버에서 상대 메일서버로 메일을 보내는 상황에서, 먼저 클라이언트 SMTP는 서버 SMTP의 25번 포트로 TCP연결을 맺는다. 만약 서버가 죽어있으면 클라이언트는 나중에 다시 시도한다.TCP 연결이 맺어지면, 클라이언트와 서버는 SMTP 핸드쉐이킹을 수행한다. 이 SMTP 핸드쉐이킹 과정에서 클라이언트는 송신자와 수신자의 email 주소를 제공한다. 핸드쉐이킹 과정을 마치면, 클라이언트는 메시지를 보낸다. SMTP 클라이언트와 SMTP 서버 사이의 메시지 전달과정을 예를들어 살펴보자.클라이언트 호스트네임은 github.io 이고, 서버 호스트네임은 korea.ac.kr 이라고 하자.C는 클라, S는 서버를 나타내고 TCP 연결 직후의 상황을 가정한다.메일 내용은 “Hello, this is TK-one. Can I know the result of the interview?” 이다. S: 220 korea.ac.kr C: HELO github.io S: 250 Hello github.io, pleased to meet you C: MAIL FROM: tk-one@github.io S: 250 tk-one@github.io … Sender ok C: RCPT TO: kim@korea.ac.kr S: 250 kim@korea.ac.kr … Recipient ok C: DATA S: 354 Enter mail, end with “.” on a line by itself C: Hello, this is TK-one. (메일내용) C: Can I know the result of the interview? (메일내용) C: . S: 250 Message accepted for delivery C: QUIT S: 221 korea.ac.kr closing connection 클라이언트는 5개의 명령(HELO, MAIL FROM, RCPT TO, DATA, QUIT)을 내리며 하나의 점(.)으로 된 라인을 송신하면 이는 메시지의 끝을 의미한다.서버는 각 명령에 대해 답하며, 각 응답은 응답코드와 옵션 설명을 갖고 있다.그리고 주의할 점이 있는데, SMTP는 메시지의 body와 header를 포함하여 전부 7bit ASCII 코드로 작성되어야 한다.HTTP는 이런 제한이 없는 반면, SMTP는 한글이나 binary 데이터 처럼 ASCII가 아닌 문자를 포함한다면 반드시 이 메시지는 전송되기 전에 7bit ASCII로 인코딩이 되어야한다. 이렇게 SMTP를 통해 한 메일서버에서 다른 메일서버로 메시지가 전달된다.그렇다면 수신자는 자신의 PC에서 user agent를 통해 자신의 메일서버에 있는 메시지들을 어떻게 얻을 수 있을까?수신자의 user agent는 메일을 가져오기위해 SMTP를 사용할 수는 없다. 왜냐하면 SMTP는 푸시(push)용 프로토콜인 반면, 메시지를 가져오는 것은 풀(pull) 동작이기 때문이다.여기서는 메일서버로부터 자신의 user agent로 메시지를 가져오기 위해 특별한 메일 접속 프로토콜을 사용한다. 이들중엔 POP3(Post Office Protocol - Version 3), IMAP(Internet Mail Access Protocol), HTTP 등이 있다. 참고: Computer Networking - A Top-Down approach","link":"/2020/01/03/smtp/"},{"title":"운영체제 8편 - 동기화의 고전적인 문제들","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번 편은 동기화(synchronization)에 이어 동기화의 고전적인 문제들에 대한 내용입니다. 동기화에 대한 고전적인 대표적인 문제들이 몇가지 있는데 이중 2가지를 알아볼 것이다. Bounded-buffer problem Readers and Writers problem Bounded-buffer problem이는 간단하게 표현하면 N개의 buffer에 여러 producer와 consumer가 동시에 접근하는 문제이다.몇가지 용어부터 정리해보자. 생산자(Producer)한번에 하나의 아이템을 생성해 buffer에 저장한다. 다만 이미 buffer의 상태가 full이면 대기한다. 소비자(Consumer)Buffer에서 하나의 아이템을 가져온다. 다만 buffer가 비어있다면 대기한다. 이 문제는 주변에서도 많이 찾아볼 수 있는데 패킷을 예로 들 수 있겠다. 패킷이 계속 들어올때 ethernet을 사용한다고 가정한다면 ethernet driver가 패킷을 하나씩 저장해준다. 이들이 buffer에 올라오면 그다음 부터는 Kernel의 역할이다. Kernel이 buffer를 읽어야한다. 이를 읽을때 어떤 것을 동기화를 해야하는가. 어떤 자료구조를 사용해야 하는가 등이 이문제를 해결하는데 중요한 요점이다. 여기에서는 Producer와 Consumer가 충돌없이 buffer에 접근할 수 있도록 해야하므로, Buffer에 접근하는 부분이 critical section이 되겠다. 여러개의 Producer가 동시에 produce할 수 없고, Producer와 Consumer가 동시에 produce, consume 할 수 없다. 또한 여러개의 Consumer도 동시에 consume할 수 없다.이를 여러개의 적절한 자료구조와 알고리즘을 통해 해결해보자. 이는 3가지 semaphore를 사용해서 구현할 수 있다. (다른 구현방법도 많다.)우선 buffer는 간단하게 N의 크기를 가진 배열로 하자. Producer는 buffer가 full이 아니라면 새로운 item을 추가한다. Consumer는 buffer가 empty가 아니라면 item을 소비한다. 문제를 해결하기 위해 도입한 3가지 semaphore와 그들의 역할은 다음과 같다. empty버퍼내에 저장할 공간을 의미한다. Producer들의 진입을 관리한다. buffer의 크기가 N이므로 초기값은 N으로 하여 0이 될때까지 Producer가 진입할 수 있다. full버퍼내에 소비할 아이템이 있는지를 의미한다. Consumer들의 진입을 관리한다. 초기값은 0으로 하여 비어있을때는 Consumer가 진입하지 못한다. Mutexbuffer의 변경을 위한 semaphore이다. 초기값은 1로하여 동시에 1개의 Producer or Consumer만 진입할 수 있다. pseudo 코드를 바로 살펴보자. 1234567891011121314151617181920212223242526Producer: do { // 버퍼에 최소 하나의 공간이 생기기를 기다린다. wait(empty); wait(mutex); [produce an item in buffer] signal(mutex); // 버퍼에 아이템이 있음을 알린다. signal(full); } while (1);Consumer: do { // 버퍼에 적어도 하나의 아이템이 채워지기를 기다린다. wait(full); wait(mutex); [consume item from buffer] signal(mutex); // 버퍼에 하나의 빈 공간이 생겼음을 알린다. signal(empty); } while (1); 이를 자바로 구현한 코드도 살펴보자. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import java.util.concurrent.Semaphore;public class Buffer { private String[] buffer; private int index; public Buffer(int size) { this.buffer = new String[size]; } public void add(String message) { if (index &gt;= buffer.length) { throw new RuntimeException(&quot;buffer is full&quot;); } buffer[index++] = message; } public String pop() { if (index == 0) { throw new RuntimeException(&quot;buffer is empty&quot;); } return buffer[--index]; }}public class Producer { private int counter; private final Buffer buffer; private final Semaphore empty; private final Semaphore mutex; private final Semaphore full; public Producer(Buffer buffer, Semaphore empty, Semaphore mutex, Semaphore full) { this.buffer = buffer; this.empty = empty; this.mutex = mutex; this.full = full; } public void produce(String message) { try { empty.acquire(); mutex.acquire(); buffer.add(message + &quot;-&quot; + counter); counter++; mutex.release(); full.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } }}public class Consumer { private final Buffer buffer; private final Semaphore empty; private final Semaphore mutex; private final Semaphore full; public Consumer(Buffer buffer, Semaphore empty, Semaphore mutex, Semaphore full) { this.buffer = buffer; this.empty = empty; this.mutex = mutex; this.full = full; } public String consume() { try { full.acquire(); mutex.acquire(); String message = buffer.pop(); System.out.println(&quot;pop &quot; + message); mutex.release(); empty.release(); return message; } catch (InterruptedException e) { throw new RuntimeException(e); } }}public class Main { public static void main(String args[]) { final int size = 3; Buffer buffer = new Buffer(size); Semaphore emptySemaphore = new Semaphore(size); Semaphore mutexSemaphore = new Semaphore(1); Semaphore fullSemaphore = new Semaphore(0); Producer producer1 = new Producer( buffer, emptySemaphore, mutexSemaphore, fullSemaphore); Producer producer2 = new Producer( buffer, emptySemaphore, mutexSemaphore, fullSemaphore); Consumer consumer1 = new Consumer( buffer, emptySemaphore, mutexSemaphore, fullSemaphore); new Thread(() -&gt; IntStream.range(0, 100).forEach(i -&gt; producer1.produce(&quot;producerThread1&quot;))).start(); new Thread(() -&gt; IntStream.range(0, 100).forEach(i -&gt; producer2.produce(&quot;producerThread2&quot;))).start(); new Thread(() -&gt; { while (true) { consumer1.consume(); } }).start(); }} producer1과 producer2는 각각 100번씩 buffer에 문자열 item을 채워넣는다.그리고 buffer를 consumer가 계속해서 소비하도록 한다. Readers And Writers Problem이는 여러개의 Reader와 Writer가 하나의 공유데이터를 읽거나 쓰기위해 접근하는 문제를 의미한다. 몇가지 용어들을 정리해보자. Reader여러 Reader들은 동시에 데이터를 읽을 수 있다. WriterReader 혹은 다른 Writer가 데이터에 접근하고 있으면 write를 할 수 없다. 즉 여러개의 reader는 동시에 critical section에 접근할 수 있지만, writer는 다른 reader나 writer가 접근하지 않을때 write 할 수 있다. Reader도 writer가 현재 critical section에 접근중이면 진입할 수 없다. 앞에서 보았던 Bounded-buffer problem에서는 reader와 writer가 각각 한번만 들어가는 것을 허용했는데 Readers Writers problem에서는 여러 reader들의 접근을 허용한다.이를 여러개의 적절한 자료구조와 알고리즘을 통해 해결해보자. 이는 2가지 semaphore와 정수형 자료구조를 사용해서 구현할 수 있다. 문제를 해결하기 위해 도입한 자료구조들의 역할은 다음과 같다. int readCount버퍼에 현재 접근하고 있는 reader들의 개수를 의미한다. reader 개수가 0이되어야 writer가 접근할 수 있다. 초기값은 0으로 설정한다. Semaphore writeWriter간의 동기화를 위한 semaphore이다. 초기값은 1로 두고, 1일때에만 writer가 접근할 수 있다. Semaphore mutexreadCount에 대한 접근과 write semaphore에 대한 접근이 원자적으로 수행되기위한 semaphore이다. 초기값은 1로 설정한다. pseudo 코드를 바로 살펴보자. 123456789101112131415161718192021222324Writer: wait(write); [write] signal(write);Reader: wait(mutex); readCount++; // Reader가 최소 1개 진입해있을때 writer의 접근을 막는다. if (readCount == 1) { wait(write); } signal(mutex); [read] wait(mutex); readCount--; // 아무런 Reader 도 진입해있지 않으므로 writer의 접근을 허용한다. if (readCount == 0) { signal(write); } signal(mutex); 이를 자바를 사용하여 구현한 코드도 살펴보자. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import java.util.concurrent.Semaphore;public class Data { private int number; public int getNumber() { return number; } public void setNumber(int number) { this.number = number; }}public class Writer { private int counter; private final Semaphore writer; private final Data data; public Writer(Semaphore writer, Data data) { this.writer = writer; this.data = data; } public void write() { try { writer.acquire(); data.setNumber(++counter); System.out.println(&quot;Write data &quot; + counter + &quot; in thread &quot; + Thread.currentThread().getName()); writer.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } }}public class Reader { private static int readCount; private final Semaphore writer; private final Semaphore mutex; private final Data data; public Reader(Semaphore writer, Semaphore mutex, Data data) { this.writer = writer; this.mutex = mutex; this.data = data; } public void read() { try { mutex.acquire(); readCount++; if (readCount == 1) { writer.acquire(); } mutex.release(); int num = data.getNumber(); System.out.println(&quot;Read data &quot; + num + &quot; in thread &quot; + Thread.currentThread().getName()); mutex.acquire(); readCount--; if (readCount == 0) { writer.release(); } mutex.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } }}public class Main { private static void sleep(long timeMillis) { try { Thread.sleep(timeMillis); } catch (InterruptedException e) { throw new RuntimeException(e); } } public static void main(String args[]) { Data data = new Data(); Semaphore writer = new Semaphore(1); Semaphore mutex = new Semaphore(1); Reader reader1 = new Reader(writer, mutex, data); Reader reader2 = new Reader(writer, mutex, data); Writer writer1 = new Writer(writer, data); Thread readerThread1 = new Thread(() -&gt; IntStream.range(0, 20).forEach(i -&gt; { reader1.read(); sleep(5); })); readerThread1.setName(&quot;readerThread-0&quot;); Thread readerThread2 = new Thread(() -&gt; IntStream.range(0 ,20).forEach(i -&gt; { reader2. read(); sleep(5); })); readerThread2.setName(&quot;readerThread-1&quot;); Thread writerThread1 = new Thread(() -&gt; IntStream.range(0, 50).forEach(i -&gt; { writer1.write(); sleep(10); })); writerThread1.setName(&quot;writerThread-0&quot;); readerThread1.start(); readerThread2.start(); writerThread1.start(); }} 이에 대한 출력은 다음과 같다. 12345678910111213141516Read data 0 in thread readerThread-1Read data 0 in thread readerThread-0Write data 1 in thread writerThread-0Read data 1 in thread readerThread-1Read data 1 in thread readerThread-0Read data 1 in thread readerThread-1Read data 1 in thread readerThread-0Write data 2 in thread writerThread-0Read data 2 in thread readerThread-1Read data 2 in thread readerThread-0Read data 2 in thread readerThread-1Read data 2 in thread readerThread-0Write data 3 in thread writerThread-0Read data 3 in thread readerThread-1...... 다만 이와같이 구현을 하게되면 Writer가 starve할 수 있는 문제가 있다. 왜냐하면 readCount가 0일때에만 writer가 접근할 수 있으므로 여러 reader들이 계속 접근을 하게되면 writer는 접근할 수 있는 기회를 얻지 못하게 된다. 동기화 알고리즘의 설계실제 실무에서도 동기화 문제에 부딪혀 이를 해결해야 하는 경우가 빈번하다.이를 해결하기 위해 다음과 같은 사항을 고려하며 생각해보면 도움이 될 수 있겠다. 먼저 동기화 문제를 동작조건과 함께 서술해보자. 어떤 조건이 있어야하는지? 어떤 접근형태를 가지는지를 먼저 정리해보자.그 다음에 사용할 semaphore를 설계한다. 몇개를 사용할지? 이 semaphore가 왜 필요할지, 어떤 대상을 protect할지 사고해보고 결정한다. 또한 각 semaphore에 알맞는 초기값을 설계해야한다.그리고 알고리즘을 설계한 후 검토한다. 다음 사항을 검토해보자. Data consistency가 확보되는지 Deadlock이 발생하는지 Starvation 가능성이 있는지 Concurrency를 얼마나 제공하는지","link":"/2019/07/25/classic-problem-of-synchronization/"},{"title":"파일시스템 4편 - 파일시스템 디자인-1","text":"이 글은 학부 System Programming 수업을 듣고 파일시스템 관련내용을 정리한 글입니다. 이번 편에서는 크게 다음 2가지를 볼 것이다. File System의 디자인 Directory의 디자인 unix 시스템에서 파일과 디렉토리는 비슷하면서도 다르게 구현이된다.파일의 디자인부터 살펴보자. File ImplementationFile 구현의 핵심은 file의 내용을 담고있는 data block이 저장되는 디스크상의 위치정보를 결정하는 것과 이에 대한 관리방법이다. data block의 디스크에서의 위치정보를 결정하기 위해서는 여러가지 할당방법들이 존재하는데 실제로 사용되고 있는 몇가지 방법들을 알아보자. Contiguous AllocationContiguous는 말그대로 연속적인, 연결되어있는 이런 의미이다.Contiguous Allocation 방식은 파일을 물리적으로 연속된 disk block에 저장한다. 우리가 프로그래밍 할때 고정 길이의 배열을 할당하고 사용하는 것과 유사하다.이 방식은 구현이 간단하며, 물리적으로 연속된 공간에 block이 위치하기 때문에 전체 파일을 한번에 읽어들일경우 성능상 매우 큰 이득을 볼 수 있다. 이처럼 파일의 맨 처음부터 끝까지 읽는 방식을 sequential read라고 한다. 이의 반대는 random read이다.Contiguous Allocation은 random read도 빠르다. 파일에서 특정 block의 위치를 바로 알 수 있다. 연속된 block이므로 index로 block 주소를 바로 계산할 수 있기 때문이다. 위 그림에서 block 14, 15, 16을 을 차지하고 있는 파일을 수정할때는 어떻게 해야할까?Contiguous Allocation에서는 파일 수정시 수정한 block들을 어디에 위치할지 빈공간을 계속해서 찾아야 한다. 새로운 빈 연속된 block들을 찾으면 이에 복사하면서 massive copy가 발생한다.위의 예시에서는 파일크기가 더 작도록 수정하기 때문에 반드시 다른 비어있는 다른 block들을 찾아야하는 것은 아니지만, 파일의 크기가 늘어나면서 수정을 하게되면 copy가 불가피하다.이런 비용을 최소화 하기위해 파일끝에 예비용 block을 남기는 경우도 있지만 이는 disk 공간을 낭비한다. 이런 방식은 많이 사용할 것 같지 않아보이지만 예상외로 많이 사용된다. 실제로 데이터베이스는 이런 방식으로 구현하는 경우가 많다. Disk block을 연속해서 여러개 할당하고 그 안에서 내부적으로 읽고 쓰고를 반복한다.static한 파일들을 담고있는 서버에서도 파일에 대한 sequential read 성능이 매우 중요하기 때문에 사용하는 경우가 있고, 나중에 보게되겠지만 log structured file system에서 flash 같은 메모리는 이런 방식을 사용한다.또한 구현이 간단하기에 임베디드에서 이를 채택하는 경우가 많다. Linked List Allocation위에서 본 Contiguous Allocation 방식은 파일을 쓸때 능동적으로할 수 없다. block이 연속적이여야 하기 때문이다. 그래서 disk의 block을 linked list로 구현해서 file의 data를 저장하도록 하는 방식이 Linked List Allocation 방식이다. 위 그림을 보게되면 파일을 읽을때 파일의 시작 block에서 출발해서 그 다음 block을 찾아가는 방식이다. data block에 다음 block의 주소를 같이 저장한다. 이 방식은 file의 data block이 disk의 어디든지 위치할 수 있기때문에 유연하다. 또한 Contiguous allocation의 방법과 다르게 공간의 낭비가 없다. 단점도 존재하는데 Random access를 하는데 성능이 좋지 못하다. File의 특정위치를 찾아가기 위해서는 해당 file의 시작 block부터 찾아가야 하기 때문이다. 이를 해결하기 위해 file의 metadata를 in-memory에 캐싱을 하기도 한다.Data block에 다음 block 주소를 저장해야 하는 공간낭비 문제도 존재한다. 실제로 MS-DOS의 FAT(File Allocation Table) File System은 Linked List Allocation 방식을 사용한다. Random access가 느리기 때문에 더 나은 방법으로 File의 data block의 위치를 별도의 block에 모아둘 수 있다. 이렇게 File의 data block을 모아놓은 block을 index block이라고 하며 이 block을 읽어 파일의 모든 data block의 위치를 알 수 있다.이 방식이 Linked List Allocation 보다 개선된 점은 random access 시 index block만 읽으면 찾아가고자 하는 data block의 위치를 알 수 있으므로 빠르게 random access가 가능하다.index block이 몇개의 block 주소를 가질 수 있느냐가 이 파일시스템이 지원하는 최대 파일크기를 결정한다. 위 그림을 보면 19번 block에 index block을 위치하고 이 block에 data block을 순서대로 위치한다. inodeinode는 unix 파일시스템에서 사용하는 방식이고, windows 에서도 이와 비슷한 방식을 사용한다.inode는 index node의 줄임말로 file의 metadata를 가지고있는 구조체이다. inode는 파일시스템 디자인에서 매우 중요한 역할을 맡는다. inode는 파일에 대한 data block index를 계층 형태로 관리하도록 한다. 메모리에서의 multi-level paging 방식과 비슷하다. 이러한 방식으로 inode의 data block 관리방식으로 구현을 하면 많은 flexibility를 얻을 수 있다. . PCB에는 프로세스 관련한 정보가 다 들어가있듯이, inode에는 파일에 대한 대부분의 데이터가 전부 다 들어가있다.inode의 구성요소는 대략적으로 다음과 같다. File에 대한 속성을 나타내는 field들이 존재한다. file owner, timestamp, hard link counter, last access time, acl 정보 등이 있다. 작은 크기의 파일을 위한 direct index가 존재한다. 파일의 크기가 커짐에 따라서 요구되는 data block의 index들을 저장하기 위한 index table들이 존재한다. 이 index table을 가리키는 single indirect, double indirect, triple indirect 이 있다. inode의 대략적인 도식을 그림으로 보자. Multi-Level Index in inodedirect block은 그 자체의 값들이 data block을 가리키는 포인터이다.direct block이 8개가 있고 block size가 4KB라면 파일크기가 최대 32KB까지는 indirect block을 사용하지 않는다.파일이 그 크기가 넘어가면, 즉 direct block에서 처리가능한 block 개수가 넘어가게되면 indirect block을 사용하기 시작한다.single direct를 사용하기 시작한다. single indirect는 data block의 주소를 담고있는 index block을 가리킨다. index block은 data block을 가리킨다. single direct에서 처리가능한 크기가 넘어가게 되면 double indirect를 사용한다.double indirect는 index block을 가리키는데 이 index block은 또다시 index block을 가리킨다. 즉 2-level page table과 동일하다.triple indirect는 이 단계가 3단계로 이루어져 있다. inode에서 direct block, single indirect, double indirect, triple indirect가 각각 지원할 수 있는 최대 크기를 계산해보자.Block size는 4KB, direct block 개수는 12개, block 주소는 4byte 라고 가정하자. Direct blocks : 4KB * 12 = 48KB Single Indirect : 1024 * 4KB = 4MB Double Indirect : 1024 * 1024 * 4KB = 4GB Triple Indirect : 1024 * 1024 * 1024 * 4KB = 4TB 이는 block 주소 크기 등을 가정한 후의 계산값으로 실제 파일시스템이 지원하는 파일 최대크기와는 다름을 참고하자. 왜 이런 방식을 사용할까?inode 같은 방식을 사용하면 inode의 크기가 고정이 된다. inode는 파일당 1개씩 존재하며 시스템에서 파일의 개수만큼 존재한다. inode는 파일의 metadata 그 자체인데 이에 대한 크기가 파일의 크기와 관계없이 모두 동일하므로, 파일의 크기가 늘어나도 충분히 유연성을 확보할 수 있어 관리하기 편하다. ExtentLinux의 ext2, ext3 파일시스템은 위와같이 Multi-Level index 방식을 사용하여 block mapping 체계를 제공한다. 다만 이 방식은 크기가 작은 파일에 대해서는 효율적이나 큰 파일에 대해서는 높은 오버헤드를 가진다. 구조상 파일의 크기가 조금 커지면 disk 에 반드시 fragment가 일어나게 되고 파일을 처음부터 연속적으로 읽어도 random access 가 많이 발생할 수 밖에 없다.그리고 위의 block pointer 지정방식은 공간낭비도 심하다. 예를들어 single direct 방식에서 각 block 이 4, 5, 6, 7, 8 를 가리킨다면 각 주소가 4byte 일때 4 * 5 = 20 byte 를 차지한다. 하지만 이는 연속된 block 일때 (4, 5) 라고 표현할 수 있다. (5개의 block 이 연속되어 있다는 의미)물론 연속된 block 이 아니라면 매 block 마다 (4, 1), (5, 1) 처럼 될 것이다.이처럼 매 block 마다 pointer를 적용하는 방식대신 포인터와 length를 명시함으로서 연속된 block을 명시하는 방식을 적용한 것이 extent 이다.ext4 파일시스템은 이 extent 를 사용한다. 위의 inode 구조체에서 앞부분은 ext3의 inode와 동일하고 single indirect block 의 필드부터 indirect block 대신 extent 들을 가짐으로서 최대한 하위호환성을 유지했다.이런 extent 기반의 방식은 disk 에 충분히 여유있는 공간이 있고 파일의 block 이 연속적으로 할당될 수 있을때 가장 높은 효율을 낼 수 있다. Directory ImplementationDirectory는 파일들의 묶음을 나타내는데, 이는 굉장히 독특한 자료구조를 가진다.Directory entry라는 Directory를 표현하기 위한 자료구조가 존재한다. 파일시스템에 따라서 directory entry를 구성하는 필드도 달라진다. MS-DOS에서의 Directory entry와 Linux에서의 Directory entry를 살펴보자. Directory in MS-DOSMS-DOS는 파일구현에 Linked List Allocation 방식을 사용한다. 여기서 사용하는 Directory entry 구조는 다음과 같다. 그림을 보면 첫 8byte는 file name을 가지고 다음 3byte는 extension을 가진다. extension은 따라서 3글자까지만 지원을 한다. 그리고 뒤에 first block number 부분이 있는데 이 값이 linked list의 첫번째 block을 가리킨다.Linked List Allocation 방식을 사용하므로 맨 처음의 data block만 알면 전체 파일의 data을 알 수 있다. Directory in LinuxLinux에서는 directory를 특별한 타입의 file로 간주한다.그래서 디렉토리도 inode가 존재한다.Linux에서의 directory entry는 file name과 inode로 이루어진다.MS-DOS와는 다르게 directory의 metadata 정보는 inode 자료구조에 담겨있다. 디렉토리의 inode는 type에 regular file이 아닌 directory라는 타입으로 명시되어있고 inode의 data block에는 그 directory의 내용이 저장되어있다. 그렇기에 우리의 directory 구조는 영구히 저장될 수 있다. Directory lookup in Linux리눅스에서 directory를 lookup 하는 과정을 살펴보자./usr/ast/mbox path에 있는 mbox라는 파일에 접근한다고 가정하자. 이 과정을 그림을 표현하면 다음과 같다. 처음의 시작은 Root Directory에서 시작한다. 그림의 맨 왼쪽은 root directory의 inode의 data block의 내용을 그림으로 표현한 것이다. 물리 disk에 있는 block들은 파일의 내용도 가질 수 있으며 directory의 내용도 가질 수 있다. 여기서 usr directory를 찾고 그에 대한 inode로 가서 data block을 읽는다.usr는 directory 이므로 이에 대한 data block은 다시 directory entry들을 담고있다. 여기서 ast directory의 inode를 찾고 이에 대한 data block을 다시 읽는다. ast도 directory 이므로 data block은 또다시 directory entry를 담고있다.최종적으로 mbox파일을 찾고 이 inode의 data block을 읽음으로서 파일을 읽게된다. 이를 정리해보면 disk 접근횟수가 상당히 많다. 이를 개선하기위해 directory cache를 사용하며 directory cache는 hit rate가 99% 정도로 매우 높은 hit rate를 가진다. directory entry가 inode number + file name 이므로 크기가 크지않다.directory cache는 시스템이 부팅할때 root directory부터 읽어 캐싱을 한다. Free space management파일시스템이 inode가 할당되어 있는지, data block이 할당되어 있는지 tracking을 해야한다. 그래야 새로운 파일을 생성할 때 적절한 space를 할당해줄 수 있다.예를들면 파일을 만들면 inode를 할당한다. 이때 inode table을 보고 비어있는 위치에 inode를 할당한다. 예를들어 inode table이 bitmap으로 구현되어있다면 해당 순서의 bit를 1로 설정한다.또 새로운 파일에 대한 data block 또한 할당을 해야한다. 이 data block 을 할당할때에도 여러가지 기법을 사용한다.ext2나 ext3 파일시스템의 경우에는 사용하지 않으면서 연속된 약 8개의 block들의 묶음을 찾고 이를 할당함으로서 파일시스템이 파일의 부분이 contiguous하도록 보장해주어 성능향상을 꾀한다.이러한 방법을 pre-allocation이라고 하며 자주 쓰이는 방법이다. Very Simple File System파일시스템이 정확히 어떻게 구성되어있고 작동하는지를 알아보기 위해 예시로 아주 간단한 파일시스템을 보자.이를 Very Simple File System 이라고 부르고 이하 vsfs 라고 부르겠다.우리는 disk partition 이 있고 이 disk를 block 단위로 나눌 수 있다. block은 일반적인 크기인 4KB로 하겠다.아래는 우리의 disk partition 에 대한 그림이다. 총 64개의 block 들로 나누어져 있고 각 block의 주소는 0부터 63까지이다.disk partition 의 크기는 64 * 4 KB 인 아주 작은 partition 이다. 우리는 이 block 들에 무엇을 저장해야할까? 가장먼저 user data 가 떠오를 것이다. 실제로도 파일시스템의 대부분의 데이터는 user data 이다.이런 user data 가 담기는 부분의 영역을 data region 이라고 부르자. 그리고 간단하게 64개의 block 중 56개를 data region 이라고 미리 정해두자. 위에서 보았듯이 파일시스템에서 각 파일을 추적하기위해 metadata 가 필요하다. 즉 inode 가 필요하다.각 파일의 data block 의 위치가 어떻게 되는지, 파일의 크기는 어떻게되는지 등을 저장한다.우리는 이런 inode 들 또한 disk 에 저장해야한다. 이들을 저장하기 위한 영역도 미리 정해두자. 64개의 block 중 5개를 잡고 이 영역을 inode table 이라고 하자. inode의 크기는 보통 그렇게 크지는 않다. inode 한개가 256 byte 라고 가정하자. 그러면 4KB 크기인 block 1개에는 64개의 inode를 저장할 수 있다.그러므로 우리의 inode table 은 5개의 block 으로 구성되어 있으므로 총 80개의 inode를 저장할 수 있겠다. 이는 우리가 설계한 vsfs 에서 가질 수 있는 최대 파일개수와 같다.실제 파일시스템에서는 훨씬 더 큰 크기의 disk를 가지고, 단순하게 더 큰 크기의 inode table 영역을 잡을 수 있기때문에 훨씬 더 많은 파일을 가질 수 있다. 우리는 inode table 영역을 할당하고 이를 I 로 표시하였고 date region 영역을 D 로 표시하였다. 하지만 궁극적으로 inode 나 data block 이 할당되어있는지를 추적할 수 있는 장치가 필요하다. 이런 할당여부에 대한 추적은 어떤 파일시스템이든 꼭 필요한 내용이다.이런 할당여부를 추적하기 위해 여러가지 방법을 생각해볼 수 있지만 우리는 간단하게 bitmap 을 활용하여 표시할 것이다. 2개의 bitmap을 사용하여 한 개는 inode bitmap 으로 사용하고 다른 한개는 data bitmap 으로 사용한다. 각 bitmap 을 각각 한 개의 block 에 할당했다. block 한개는 4KB 이므로 약 3만 2천개의 bitmap을 저장할 수 있다. 우리는 80개의 inode 그리고 56개의 data block 을 가질 수 있기때문에 bitmap 을 과도하게 크게 설계했지만 간단한 예제이므로 그렇다고 하고 넘어가자.그리고 0번째 block 이 남아있다. 이를 super block 으로 할당하자. super block 은 파일시스템의 정보를 포함한다.파일시스템에 몇 개의 inode가 있는지 몇개의 data block을 사용하는지 inode table 은 몇번째 block 에서 시작하는지 등의 정보를 담고있다. 그래서 파일시스템을 mount 하면 OS가 먼저 super block을 읽어서 어느부분 부터 읽어들여야 하는지 알아낸다. inode는 보통 inode 마다 할당되는 번호가 있다. 이 vsfs 에서는 총 80개의 inode를 담을 수 있으므로 0번부터 79번까지 번호가 존재한다.따라서 파일시스템이 특정번호의 inode에 접근이 필요할때에면 inode의 번호로 바로 주소를 계산해서 접근할 수 있다.보통 disk는 sector 단위로 접근이 가능하기 때문에 (inode table 시작주소 + (N * sizeof(inode)) / 512 로 나누면 sector 번호를 알아낼 수 있다.inode table의 시작주소는 super block 을 보고 알아낼 수 있다. 파일을 새로 만들어야 할때에는 inode bitmap을 보고 비어있는 곳을 찾아 inode 를 새로 할당하고, data block 도 data bitmap 을 보고 비어있는 block 을 찾아 할당한다.이 vsfs 예제를 보고 파일시스템 구현의 감을 잡으면 된다.","link":"/2020/09/08/file-system-design-1/"},{"title":"Memory False Sharing 이란?","text":"Memory False Sharing이란 무엇일까? 말 그대로 직역하면 메모리 거짓 공유이다. 이게 무엇을 뜻하는지 알아보자. Cache Coherence먼저 Cache Coherence를 알아야 한다. 멀티코어 환경에서 코어마다 cache가 각각 존재한다. 흔히 말하는 캐시의 개념으로 자주 사용하는 데이터들을 메모리보다 더 빠른 캐시에 저장함으로서 메모리에서 읽지 않고 바로 캐시에서 가져옴으로 성능적으로 큰 이득을 볼 수 있다. 이런 멀티코어 환경에서 각 코어들에 있는 cache들의 일관성을 유지하는 것을 Cache Coherence라고 말한다.만약에 Core 1에서 메모리 주소 X에 있는 값을 읽기 위해 먼저 메모리에서 읽고 이를 Core 1 캐시에 저장하였다. 다음으로 Core 2에서 메모리 주소 X에 있는 값을 읽기 위해 메모리에서 이를 읽고 이를 Core 2의 캐시에 저장하였다. 만약 Core 1에서 add 연산으로 해당 변수를 원래 값인 1에서 5로 증가시켰다고 해보자. 그러면 Core 1의 캐시는 5로 업데이트 된다. 여기서 Core 2가 이 변수를 읽으면 무슨 값이 반환되어야 할까? 1일까 5일까?Cache Coherence는 캐시에서 공유하고 있는 데이터의 값의 변경사항이 적시에 시스템 전체에 전파될 수 있도록 하는 원칙이다.Cache Coherence는 다음 2가지가 필요하다. Write Propagation(쓰기 전파)어떠한 캐시에 데이터가 변경이 되면 이 cache line을 공유하고 있는 다른 캐시에도 이 변경사항이 전파되어야 한다. Transaction Serialization특정 메모리 주소로의 read/write은 모든 프로세서에게 같은 순서로 보여야 한다. 두번째의 Transaction Serialization은 다음 예를 보면 이해하기 쉽다.Core 1,2,3,4 가 있을때 이들 모두 초기값이 0인 변수 S의 캐시된 복사본을 각 캐시에 가지고있다. 프로세서 P1은 이 S의 값을 10으로 변경한다. 그리고 프로세서 P2가 이어서 이 S의 값을 20으로 변경한다. 위의 Write Propagation를 보장한다면 P3와 P4가 이 변경사항을 볼 수 있다. 다만 프로세서 P3는 P2의 변경사항을 본 후, P1의 변경사항을 봐서 변수 S의 값으로 10을 반환받는다. 그리고 프로세서 P4는 원래의 순서에 따라 P1의 변경사항을 보고, P2의 변경사항을 그 다음으로 봐서 20을 반환받는다. 결국 프로세서 P3, P4는 캐시의 일관성을 보장할 수 없는 상태가 되었는데 이처럼 Write Propagation 하나만으로는 Cache Coherence가 보장이 안된다.이를 위해 변수 S에 대한 Write는 반드시 순서가 지정이 되어야한다. Transaction Serialization이 보장이 된다면 S는 위의 예제에서 10으로 write하고 그리고 20으로 write 했기 때문에, 절대 변수 S에 대해 값 20으로 읽고 그다음 값 10으로 읽을 수가 없다. 반드시 값 10으로 읽고 그 다음 20으로 읽는다. 이처럼 Cache Coherence를 유지하기 위해서는 다른 프로세서에서 갱신한 캐시 값을 곧바로 반영을 하든 지연을 하든 해서 다른 프로세서에서 사용할 수 있도록 해주어야 한다. 캐시 일관성을 유지하기 위한 다양한 프로토콜들이 존재하며 대표적으로 MESI 프로토콜이 있다. Cache Coherence에 대한 내용은 여기까지만 보도록 하고 cache line 이라는 것을 알아보자. Cache Line메인 메모리의 내용을 읽고 캐시에 이를 저장하는 과정에서 메모리를 읽을때에 이를 읽어들이는 최소 단위를 Cache Line이라고 한다. 메모리 I/O의 효율성을 위해서이며 spatial locality(공간 지역성)을 위해서이다. 보통의 cache line은 64byte 혹은 128byte로 이루어져 있으며 위에서 설명한 Cache Coherence도 cache line의 단위로 작동한다. 이렇게 cache line으로 읽어들인 데이터들로 캐시의 data block을 구성하게 된다.또 cache line은 고정된 주소단위(보통은 64byte)로 접근하고 가져온다. 예를들면 다음과 같다. 1234567891011+---------+-------+-------+| address | x1000 | int a |+---------+-------+-------+| address | x1004 | int b |+---------+-------+-------+| address | x1008 | int c |+---------+-------+-------+| address | x100B | int d |+---------+-------+-------+| ....... | ..... | ..... |+---------+-------+-------+ 위와 같은 메모리 구조가 있다고 할때 변수 a를 읽을때는 주소 x1000부터 cache line의 크기인 64byte만큼 가져오고, 변수 c를 읽을때에는 주소 x1008부터 64byte를 읽는게 아니다. 고정된 주소단위로 변수 c를 읽을때에도, write를 할때에도 주소 x1000으로 읽는다는 의미이다. 이제 cache line을 알았으니 다시 Memory False Sharing으로 돌아가자. Memory False SharingMemory False Sharing은 동일한 cache line을 공유할때 Cache Coherence로 인해 성능이 느려지는 안티패턴을 의미한다. 위에서 본 예제로 다시 이해해 보자. 1234567891011+---------+-------+-------+| address | x1000 | int a |+---------+-------+-------+| address | x1004 | int b |+---------+-------+-------+| address | x1008 | int c |+---------+-------+-------+| address | x100B | int d |+---------+-------+-------+| ....... | ..... | ..... |+---------+-------+-------+ 메모리 구조가 위와 같을때 스레드 2개가 있고 스레드 1은 int 변수 a를 1씩 계속 더하는 일을 하고, 스레드 2는 int 변수 c를 1씩 계속 더하는 일을 한다고 해보자. Thread 1: while (true) { a++ } Thread 2: while (true) { c++ } 더하기를 시작하기 전 이미 해당 cache line이 캐시에 올라와있다면 CPU 캐시의 상태는 다음과 같을 것이다. 123456789101112131415161718192021Core 1 Cache+-------------+---------+----------------------+| mem address | invalid | data block (64 byte) |+-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+| x1000 | false | a | b | c | d | .... | +-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+Core 2 Cache+-------------+---------+----------------------+| mem address | invalid | data block (64 byte) |+-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+| x1000 | false | a | b | c | d | .... | +-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+ 상황을 쉽게 이해하기 위해 두 스레드는 서로 다른 프로세서에서 실행되지만 시간상으로 볼 때 서로 사이좋게 번갈아 가며 add를 한다고 해보자. Thread 1: a++ Thread 2: c++ Thread 1: a++ Thread 2: c++ 이런 순서대로 실행이 된다고 하자. 먼저 1번의 a++가 발생했을때는 Core 1의 cache에서 a에 해당하는 부분이 1을 증가시킨 값으로 write가 일어나게 된다. 하지만 여기서 문제가 발생한다. 바로 다음 2번이 실행되기를 원하지만 그 사이에는 많은 일이 발생한다. 1번을 실행하였을때 Core 1 Cache의 data block 값이 변하였고, Cache Coherence protocol에 의하여 2가 실행되기 전에 하드웨어 병목이 생긴다. MESI protocol에 의해 Core 2의 해당 cache line의 상태가 invalid 상태로 바뀌고 Core 2가 다시 데이터를 읽으려면 해당 cache line이 invalid 이기 때문에 Core 1에서 읽거나 해야한다.즉 cache line단위로 관리되기 때문에 Thread 2는 변수 a와는 전혀 상관이 없는 작업임에도 불구하고 변수 a에 대한 변경때문에 성능저하가 급격하게 나타나게 된다.두 변수 a와 c가 서로는 전혀 상관이 없는 데이터임에도 불구하고 같은 cache line에 있기때문에 CPU는 특정 변수가 변경될때마다 캐시 일관성을 맞추기 위해 작업을 하게된다. 이는 성능하락으로 이어진다. 어떻게 해결할 수 있을까?어떻게하면 이를 해결할 수 있을까?일종의 cache line size에 맞추어 padding을 넣어 서로 다른 cache line에 속하게할 수 있다. 예는 다음과 같다. 123456789101112131415161718192021Core 1 Cache+-------------+---------+----------------------+| mem address | invalid | data block (64 byte) |+-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+| x1000 | false | a | padding | +-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+Core 2 Cache+-------------+---------+----------------------+| mem address | invalid | data block (64 byte) |+-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+| x1040 | false | c | padding | +-------------+---------+----------------------+| ..... | ..... | .................... | +-------------+---------+----------------------+ 이처럼 변수뒤에 padding을 붙여줌으로서 서로 다른 cache line에 속하게 하면 위 같은 False Sharing 문제를 해결할 수 있다.C++에서는 alignas 함수를 사용하여 padding을 넣어줄 수 있다. 12alignas(64) int a = 0;alignas(64) int c = 0; 자바도 이와 비슷한 방법으로 자바 8부터 @jdk.internal.vm.annotation.Contended 라는 어노테이션을 지원한다.먼저 클래스 내부필드에 어노테이션을 적용하는 방법을 알아보자. 클래스 내부 필드에 이를 적용하게 되면 해당 필드는 앞뒤로 empty bytes로 패딩을 추가함으로서 object 안의 다른 필드들과 다른 cache line을 사용하도록 해준다. 12345public class Counter1 { @jdk.internal.vm.annotation.Contended public volatile long count1 = 0; public volatile long count2 = 0;} @Contended에는 group tag라는 것도 지원하는데 이 group tag는 필드단위에 적용되었을때에만 작동한다. Group은 서로 다른 모든 그룹과 독립된 cache line을 가지게 된다. 12345678910public class Counter1 { @jdk.internal.vm.annotation.Contended(&quot;group1&quot;) public volatile long count1 = 0; @jdk.internal.vm.annotation.Contended(&quot;group1&quot;); public volatile long count2 = 0; @jdk.internal.vm.annotation.Contended(&quot;group2&quot;); public volatile long count3 = 0;} 위의 예처럼 group tag를 지정해주면, count1 변수와 count2 변수는 같은 그룹으로 지정이 되어있고 count3는 다른 그룹으로 지정되어있다.이런 경우 count1과 count2는 count3과는 다른 cache line을 가지게 되며 count1과 count2는 그룹이 같으므로 같은 cache line으로 될 수 있다. Contended 어노테이션은 클래스에도 적용할 수 있는데, 클래스에 적용하게되면 모든 field들이 같은 group tag를 가지는 것과 동일하다. 하지만 JVM 구현체에 따라서 다른 isolation 방법을 사용할 수 있다. 전체 object를 isolate 기준으로 할수도 있고 각 field 들을 isolate 기준으로 할수도 있다. (HotSpot JVM 기준으로는 class에 Contended 어노테이션이 적용되어있다면 모든 field 앞에 padding을 적용하는 것 같다. implementation in HotSpot JVM) 12345@jdk.internal.vm.annotation.Contendedpublic class Counter1 { public volatile long count1 = 0; public volatile long count2 = 0;} Contended 어노테이션은 이 용도에 맞게 각 object들이 서로 다른 스레드에서 접근하는 상황일때 사용하면 성능향상을 가져올 수 있을것이다.실제 Contended 어노테이션은 ConcurrentHashMap 구현이나 ForkJoinPool.WorkQueue 등에서 사용하고 있다. Reference https://en.wikipedia.org/wiki/False_sharing http://shumin.co.kr/comp-arch-false-sharing/ https://www.baeldung.com/java-false-sharing-contended","link":"/2019/07/16/false-sharing/"},{"title":"파일시스템 3편 - 파일시스템이란?","text":"이 글은 학부 System Programming 수업을 듣고 파일시스템 관련내용을 정리한 글입니다. 파일(File)이란?파일은 무엇일까?파일은 linear array of bytes 이다. 이게 무슨말일까?이전 운영체제 메모리편에서 보았듯이 address space는 sparse 하다. 즉 프로세스가 사용하는 주소공간은 중간부분을 사용하지 않고 비어있을 수 있다. 그러므로 효율적으로 사용하고자 multi-level page table을 사용하곤 했었다.이와 다르게 파일은 linear array of bytes로 중간이 비어있지 않다. 그리고 파일은 byte addressable하다. Byte 단위로 접근이 가능하다는 것이다. 그리고 여기서의 linear array는 밑에서 보겠지만 logical한 block이 linear하다는 것이다. Address space와 파일을 표로 비교해보자.만약 Address space를 잘 모른다면 운영체제 메모리편을 참고하자. File Address Space Variable length Fixed length Persistent Volatile 연속적 비연속적 Address space는 fixed space이다. 즉 주소공간이 48bit이라면 248의 주소공간이 있다. 반면 파일은 variable한 길이를 가진다.또 파일은 연속적이라고 표현하는데, 예를들어 파일은 이부분에 쓰고 이부분은 비워두고 하는 개념이 아니다. 하지만 Address space는 가능하다. 그럼 파일로 Database를 구현할 수 있을까?구현할 수 있다. 다만 Database에서 제공하는 여러 기능들도 동일하게 구현을 한다면 말이다.하지만 운영체제는 general한 서비스를 구현하는 것이 목적이고, 데이터베이스는 말그대로 data에 대한 더 구체적인 목적을 가지고 있기때문에 이 둘은 지향점이 다르다. 파일시스템(File System)파일시스템이란 무엇일까? 파일시스템은 순수한 software이다. memory처럼 성능향상을 위해 추가적인 hardware(MMU)를 도입하거나 하는 것없이 순수한 software로 작성되었다.파일시스템은 크게 2가지 의미를 가지고 있다.먼저 파일은 Abstraction이다. 즉, 사용자인 우리는 파일이 정확히 디스크 어디에 있는지 모른다.하지만 파일을 구현하려면 디스크가 있어야한다. 파일을 write하는 것과 디스크에 저장되는 것과 연결이 되어있어야 구현할 수 있을것이다. 이런 관점에서 파일시스템의 첫번째 의미로는 파일시스템은 physical storage의 종류에 관계없이 프로세스에게 read와 write을 할 수 있게끔 파일과 physical storage block간의 매핑을 제공해준다. File blocks를 disk blocks로 매핑예시 File block number Disk block number 1 108 2 3010 3 3011 이렇기에 사용자는 파일을 쓸때 이 파일이 실제 어디에 저장되는지 고민을 할 필요가 없다. 그 마법을 파일시스템이 부린다. Block파일은 block 단위로 자를 수 있다. 이 block은 대부분 page 크기에 맞춘다. 즉 4KB에 맞춘다.만약 1MB 짜리 파일은 256개의 block으로 나뉜다.왜 block size를 page size로 맞출까?왜냐하면 파일을 읽거나 할때 block을 들고와서 memory에 써주어야 하기 때문이다. 다만 어떤 파일시스템은 4MB 파일 block을 가지는 파일시스템도 존재한다. 대신 block을 메모리에 써줄때에는 contiguous한 4MB page 묶음을 확보하기 쉬지않다. 다시 파일시스템으로 돌아가서 파일시스템의 두번째 의미를 살펴보자.파일시스템은 디스크에 들어가있는 파일 전체를 총칭한다. 즉, 파일시스템이 파일들의 집합이라는 의미이다. 파일시스템에 따라서 파일들의 배치나 구성이 달라진다. 파일시스템은 Windows는 NTFS를 사용하고 Linux는 ext4 등을 사용한다. 즉 파일시스템의 첫번째 의미로는 software로서의 의미에 가까운 반면 두번째 의미로는 파일자체의 집합을 의미한다. 파일시스템을 그림으로 나타내면 다음과 같다. 위에서의 Buffer cache 라는 것은 다음편에서 보겠지만 디스크에서 읽어온 내용을 메모리에 저장해둔 것이다. 그래서 파일을 읽는다는 것은 Buffer cache를 읽는 것이다. Buffer cache는 Kernel 영역 메모리에 위치하는데 파일시스템이 Kernel mode로 돌아가기 때문이다. 이정도로만 우선 알고있자. Metadatametadata는 간단하게 파일에 대한 data이다. 위에서 본 파일시스템의 block 매핑정보 자체도 metadata이다. metadata를 살펴보는 이유는 파일시스템이 하는 일이 metadata를 관리하는 것이기 때문이다. 파일시스템에 따라서 파일들의 metadata 구성이 달라진다.파일의 metadata에는 파일의 이름, 타입, 파일크기, storage의 block 위치, protection 관련정보들이 존재한다. 이 외에도 시간관련 data, inode(index node), directory 정보 등이 존재한다. inode라는 것은 나중에 더 자세히 보겠지만 위에서 본 매핑정보를 가지고 있는 파일시스템에서의 핵심적인 data structure이다. 파일마다 inode가 한개씩 존재하며 metadata에 파일 개수만큼의 inode가 존재한다. directory 혹은 폴더 라는 개념은 결국 metadata들을 모아놓은 개념이라고 볼 수 있겠다. Metadata and cachingMetadata는 어디에 저장이 될까? metadata는 디스크에 저장이 되어 persistence가 제공된다.우리는 파일에 접근할때 먼저 metadata를 접근하고 그 metadata에서 data 위치를 읽어서 다시 그 data에 접근해야 한다. 즉 최소한 metadata에 한번은 접근을 해야한다는 것이다.이는 마치 메모리에서 메모리에 접근하기 위해 page table을 먼저 읽고 그 다음 적절한 physical memory address로 변환하여 다시 메모리에 접근하는 문제와 같다. 즉 메모리에 접근하기 위해 메모리를 2번 접근하는 문제이다. 이를 극복하기 위해 TLB를 사용하여 page mapping을 캐싱한다.이와 비슷한 이유로 metadata도 캐싱한다. metadata의 캐싱은 위에서 살짝 살펴보았던 Buffer cache와는 다르다. 말그대로 metadata 캐싱은 metadata를 위한 캐싱이다.그래서 운영체제는 부팅할때 맨처음 metadata들을 읽은다음 이들을 미리 캐싱한다. metadata 캐싱은 asynchronous update를 사용한다. metadata 변경이 일어날때 cache를 업데이트하고 나중에 disk에 저장한다. 즉 metadata 수정이 메모리에서 수정된다는 의미이다. 그러면 inconsistency를 어떻게 처리할 수 있을까? 만약 metadata 수정이 되었지만 이 내용이 disk에 반영되지 못하고 컴퓨터가 종료되면 어떻게될까?일단 처리하지 못한다. 그래서 OS에서 파일시스템에서 failure semantics가 보장되지 않는다고 한다. 물론 찾아보면 Journaling file system 같은 failure semantics를 보장해줄 수 있는 파일시스템이 존재한다. 혹은 checkpoint 같은 값을 써서 추가적인 failure semantics를 보장하기도 한다.metadata caching 말고도 Directory cache 라는 것도 존재한다. 이는 directory에 대한 cache로 hit율이 99% 이상으로 매우 높은편이다. Journaling File System저널링 파일시스템이라고 부른다.이는 file system에 변경사항을 반영하기 전에, journal 이라는 circular log의 변경사항을 보고 이를 기반으로 추적하는 파일시스템이다.이를 활용하면 시스템 충돌이 발생했을때 빠르게 복구하면서 data inconsistency를 방지할 수 있다.현재 리눅스에서 채택하고있는 ext4 파일시스템은 저널링 파일시스템이다. 파일시스템에서 보통 128MB 정도의 연속된 block을 미리 예약하고 이 공간을 Journal로 사용한다. 즉 중요한 데이터들을 이 공간에 빠르게 write한다. ext4 파일시스템은 여러가지 모드로 설정할 수 있는데 기본적인 방식은 포렌식포렌식은 지워진 파일을 복구한다. 어떻게 이것이 가능할까?파일을 지울때 실제 파일의 데이터를 지우는 것이 아닌 해당 파일에 해당하는 metadata를 지운다. 그래서 파일삭제는 파일크기에 관계없이 보통 빠르게 진행된다.결국 실제 파일의 데이터는 디스크 어딘가에 남아있는데 이를 가르키고 있는 metadata들이 전부 삭제된 상황이다. 그래서 포렌식은 파일시스템의 매핑방식과 disk 데이터를 적절하게 보고 실제 데이터들을 찾아낸다. File System layout파일시스템의 레이아웃을 보면 다음과 같다. 왼쪽 그림을 보면 하나의 디스크에 partion을 나누어서 partition마다 다른 파일시스템을 올릴 수 있다. 또 오른쪽 그림을 보면 여러 디스크를 묶어서 하나의 partition을 만들 수도 있다. 이는 디스크 경계를 넘어서는 파일을 만들 수 있다는 뜻과도 같다. 또 그림에는 metadata가 전부 맨 앞에있는데 이는 logical한 그림임을 참고하면 좋겠다. 나중에 조금 더 자세한 layout을 살펴보게 될 것이다. File OperationsFile을 읽을때 우리는 어떤 과정을 거칠까? 먼저 file을 open하고 그 다음 read or write을 하고 이를 close 해주어야 한다. 여기서의 open, read, close는 모두 시스템콜이다. open(2)int open(const char *pathname, int flags); open은 파일의 path를 인자로 받아 파일을 open한다.open을 호출하면 file descriptor를 반환받는다.file descriptor는 음수가 아닌 숫자로 handle 이라고도 부른다. handle은 자동차의 핸들과 같은 의미인데, 자동차의 핸들은 바퀴를 돌려주는 것처럼 우리는 이 handle을 이용해 파일을 조작할 수 있다. read(2)ssize_t read(int fd, void *buf, size_t count);read는 file descriptor를 인자로 받는다. 그리고 읽을 byte 수인 count를 인자로 받고 결과값을 buf에 넣어준다. close(2)int close(int fd)file descriptor를 닫는다. read, write는 어떤 파일을 읽겠다는 것을 나타내는 file descriptor를 인자로 받는다. 나중에 네트워크쪽에서도 socket 이라는 것이 있는데 socket의 생성에도 file descriptor를 반환받는다. 이를 가지고 send, recv를 하게된다. 우리는 read, write로 파일을 읽거나 쓸때 정확히 어디를 읽고 어디를 쓰겠다라는 위치를 전달하지 않는다. 그럼에도 파일을 계속해서 읽고 쓸수있다. 이것이 어떻게 가능할까?리눅스는 내부적으로 current-file-position 포인터를 프로세스마다 들고있다. 그래서 프로세스가 파일을 읽거나 쓸때 해당 프로세스가 어디까지 작업했는지 이 포인터에 기록한다. 이 current-file-position을 변경하기 위해 seek 이라는 시스템콜을 제공한다.여기서 알 수 있듯이 파일은 메모리랑은 다르다. 메모리는 seek이 필요없고 random access가 가능하다. 하지만 파일은 linear 하게만 왔다갔다 할 수 있으며 random access가 불가능하다. open, close의 역할File은 프로세스가 공유할 수 있다. 즉 파일 A가 있을때 프로세스1도 A를 읽을 수 있고, 프로세스2도 A를 읽을 수 있다. 이를 지원하기 위해서는 이 파일 A가 어떤 프로세스에게 읽히고 있다는 것을 내부적으로 저장해야한다. 그렇지 않으면 프로세스3이 파일 A를 그냥 delete 해버릴 수도 있다.이를 가능하게 한게 open이다. open의 대략적인 역할을 살펴보자.먼저 파일을 읽기위해 open을 하게되면 내부적으로 파일이 실제로 존재하는 것인지 탐색한다. 만약 존재한다면 해당 파일의 metadata를 메모리에 캐싱한다.그 다음으로 open을 하게되면 내부적으로 open-file table이라는것이 만들어진다. open-file table은 두가지 형태로 존재하는데 하나는 per-process open-file table이고 또다른 하나는 system-wide open-file table이다.이름으로 유추할 수 있듯이 per-process open-file table은 프로세스별로 존재하는 open-file table이고, system-wide open-file table은 시스템의 모든 open된 파일들에 대한 open-file table이다. per-process open-file table에는 각 프로세스에서 유지하는 state를 저장한다. 예를들어 프로세스가 파일을 읽을때 내부적으로 파일을 어디를 읽고있는지를 나타내는 current-file-position 포인터를 저장한다. system-wide open-file table에는 file open count, file access date, file location 등을 저장한다. open을 하면 file open count를 1 증가시키고, close를 하면 file open count를 1 감소시킨다.delete는 system-wide open-file table에서 file open count가 0인지 확인을 한 후 이루어진다. 누군가 이를 open 하고있다면 delete 할 수 없다. 다음 그림을 보자. open은 per-process open-file table에서의 index 주소를 반환한다. 이것이 file descriptor 이다.그리고 per-process open-file table에서의 값은 다시 그 파일의 system-wide open-file table entry를 가리킨다. 그리고 system-wide open-file table은 다시 그 파일의 metadata를 가리킨다.이 metadata는 open할때에 메모리에 캐싱하게되며 flush daemon에 의해 disk에는 asynchronous하게 update된다. 만약 프로세스가 파일에 대해 close 하는 것을 까먹고 exit 했으면 어떻게 될까? system-wide open-file table에서의 file open count가 줄지 않았으므로 영원히 파일삭제는 못하는 것일까? 그렇지 않다.실제로 close를 하지 않아도 file open count는 정상적으로 count 된다.PCB(Process Control Block)에 해당 프로세스가 어떤 파일을 open 하였는지 이미 다 기록을 하고있기 때문에 close를 하지않고 exit하여도 이 내용을 보고 close count를 유지한다. 리눅스에서 file descriptor 0, 1, 2는 미리 정해져 있다. 0은 표준입력, 1은 표준출력, 2는 표준에러이다. DirectoryDirectory는 파일을 나누어 저장하는 방법이다.초창기에는 Single-level directory를 사용했다. 이는 한 사용자에 대해서 단일 directory만 생성하는 방식으로 하부 directory를 만들지 못한다. 이 글을 쓸때만 해도 안드로이드, iOS는 single-level directory였다. 이렇게 설계한 이유는 단순하기 때문이다. 나중에는 우리가 흔히 알고 사용하고 있는 tree structure directory를 만들었다. 이는 sub-directory를 생성할 수 있다.다만 tree structure에는 root가 존재해야하며, acyclic graph 이여야 한다. LinkFile이나 directory의 공유를 허용하지 않으므로 이를 위해 두가지 종류의 link를 제공한다.soft link, hard link이다. Soft Linksoft link는 symbolic name의 개념이다. 삭제시 link만 삭제된다. 리눅스에 흔히 사용하는 symlink와 동일하다. 윈도우에서는 shortcut(바로가기)가 있다. 우리가 윈도우에서도 shortcut을 삭제하면 shortcut이 삭제되지 실제 파일이 삭제되지는 않는다. Hard Link실제 파일을 가리키는 alias이다. 삭제시 이 파일을 보고있는 process가 없다면 삭제한다. 그림을 보고 이해하면 쉽게 이해할 수 있다. 각 원본파일들에 대한 inode는 항상 존재한다. inode는 이 파일의 metadata를 의미한다.내가 만약 파일을 복사(cp)하였다면 데이터도 복사되고 이를 가르키는 새로운 inode도 생성된다. 실제로 파일을 복사하는 개념이다. hard link를 보자.hard link는 원본파일의 inode를 그냥 가리킨다. 그래서 삭제시 실제 파일이 지워지는 개념이다.이와 다르게 soft link는 inode를 새로 만든다. 다만 이 inode가 가르키는 데이터에는 원래 파일의 경로가 들어가있다. 그래서 이 원래파일의 경로를 보고 다시 파일에 접근한다. 위의 예제에서는 soft link의 파일을 읽을때에는 결국 inode #1을 통해 원본을 읽게된다. 그러므로 soft link 삭제시 본인의 inode를 삭제하므로 원본파일은 삭제되지 않는다. MountMount는 directory와 disk를 분리하는 개념이다.Mount는 비어있는 directory에 임의의 device를 붙일 수 있다. 비어있는 directory라는 것을 강조한 이유는 다른 device가 mount 되어있지 않아있어야 한다는 것이다. 만약 이미 mount되어있다면 기존의 것을 unmount 해야한다.파일시스템을 사용하려면 mount를 해야한다.그림을 보고 이해해보자. 이와같이 두개의 disk가 있고 각각 파일시스템의 directory들이 이와 같다고 해보자.disk2를 disk1의 비어있는 /usr/ directory에 mount를 해보자.$ mount disk2 /usr/ 이처럼 mount를 하면 /usr/bill로 접근이 가능하다. 사용자는 disk1, disk2를 알 필요가 없이 그저 path로 접근하면 된다. 사용자는 본인이 어떤 device로 접근하고 있는지 알필요가 없다. 그러므로 mount는 scaleability가 좋고 device가 바뀌어도 사용자는 영향받지 않으므로 distributed File System으로 확장이 매우 용이하다. NFS(Network File System)을 사용하여 mount를 하게되면 다른 머신에 있는 file system을 local에서 나의 local에서 사용하는 것처럼 접근할 수 있다.리눅스는 시스템이 부팅할때 mount를 통해 device tree를 결정한다. Windows를 보면 c: 드라이브가 존재한다. 이는 c:의 device를 분리하지않고 미리 directory를 고정시키고 미리 정해놓은 것이다. 또 다른 device를 추가하면 d:, e: 로 추가된다. 이는 일반 사용자의 편의성을 고려한 방식으로 우리가 살펴보았던 directory와 disk를 분리하는 mount 방식이랑 다르다. Namespacenamespace는 그냥 unique하고 식별가능한 이름이 있는 공간이라고 생각하면 된다.위에서 본 mount는 namespace를 define한다. 프로세스들의 file namespace는 동일하게 mount 된 곳을 가진다면 서로 동일한 namespace를 보게 된다.chroot를 사용하면 프로세스별로 격리된 file namespace를 만들 수 있다. 즉 프로세스마다 서로 다른 루트를 가질 수 있도록 설정할 수 있다. 이는 컨테이너 기술에서 핵심적인 부분을 맡고있다. Protection이전에 보았던 운영체제 메모리편에서 프로세스는 protection domain을 가진다고 했다. 그리고 일반 프로세스가 kernel에 접근하려고 하면 접근을 할 수 없다. page table entry에 read, write에 대한 권한을 설정하며 권한관리를 하고있다.하지만 파일은 이와는 다른 access control에 대한 정책을 가지고 있다.어떤 방식을 채택하고 있는지 바로 말하기 전에 어떻게하면 파일에 대한 access control을 구현할 수 있을까 생각해보자. protection을 어느 것을 주체로 할지부터 정해야한다.예를들어 프로세스를 기준으로 protection을 할 수도 있을 것이다. 예를들어 프로세스 A는 파일 1번과 2번에 접근할 수 있도록 access control을 설정하도록 설계할 수 있겠다. 실제로 이런 방식의 file protection을 구현한 운영체제도 존재한다. 접근의 종류는 어떤 것으로 정해야할까? read, write 등의 권한종류가 있고 프로세스가 특정 파일에 대해 write 권한을 가진다면 이 파일을 삭제할 수 있도록 해야할까? 이는 구현하기 나름이며 각각 장단점이 있다.우리는 unix 계열의 File Protection을 살펴보도록 하자. Linux에서는 프로세스가 아닌 파일을 주체로하여 사용자에게 파일권한을 부여한다고 정했다.그리고 access type을 다음과 같이 3가지로 정의했다. Read Write Execute 그리고 파일의 사용자들을 세가지로 분류하였다. owner : 파일을 만든 사람 group : 사용자들의 집합 public : 이외의 모든 사용자 그리고 access model을 위의 세가지 분류를 활용해 bit level로 protection을 정의한다. 이것이 무슨말인지 예제로 알아보자. 각 사용자들의 분류마다 RWX(Read, Write, eXecute) 순서의 bit를 정의하고 각 bit가 설정이 되어있으면 그에 대한 권한이 있다고 표현한다.RWX가 7이면 R(4) + W(2) + X(1) 이므로 모든 bit가 설정되어있으므로 읽기, 쓰기, 실행권한이 모두 있는 것이다.RWX가 6이라면 R(4) + W(2) 이므로 실행권한은 없으며 읽기, 쓰기 권한은 있는 것이다.이에 대한 RWX를 owner, group, public 순으로 나열한다.즉 파일에 대해 protection이 761로 설정이 되어있다면 owner(7), group(6), public(1) 로 설정되어 있다는 의미이다.프로세스를 실행한 사용자가 파일의 owner라면 7이므로 이 프로세스는 read, write, execute를 할 수 있다는 의미이다.만약 owner는 아니지만 파일에 설정된 group에 속한 사용자라면 6이므로 읽고 쓸수는 있지만 파일을 실행할 수는 없다.만약 이 group에도 속하지 못한 사용자라면 public으로 파일을 실행만 가능하며 읽고 쓸수는 없다. 보통 text file은 실행을 할 수 없으므로 666으로 많이 설정하기도 한다.download 한 파일들은 자동으로 보안을 위해 파일실행을 할 수 없도록 파일권한을 설정하기도 한다.그리고 1은 자주사용할 것 같지 않아보이지만 실제로는 kernel이 관리하는 시스템 파일들은 execute만 가능하도록 1로 설정하도록 한다. 그리고 unix 계열에서는 파일에 대해 write 권한이 있으면 파일을 삭제할 수 있다. 파일에 대한 protection mode는 chmod 명령어를 사용하여 변경할 수 있다.chmod 761 filename 을 수행하면 해당 파일의 mode가 변경되며 이 명령은 파일의 owner 혹은 root만 수행할 수 있다.파일의 owner 또한 chown 명령어로 변경할 수 있으며, 위의 group 또한 변경이 가능하다.group은 /etc/group에서 정의되고 파일의 group도 chgrp 명령어로 변경할 수 있다. 파일은 owner와 group 이 각각 1개씩만 존재할 수 있다. 다음편에서는 파일시스템이 어떤 방식으로 구현이 되어있는지 살펴보도록 하자.","link":"/2020/09/07/file-system-concept/"},{"title":"파일시스템 5편 - 파일시스템 디자인-2","text":"이 글은 지난번 파일시스템 디자인-1 에 이은 글입니다. 파일시스템 계층화파일시스템은 계층화가 잘되어있다. 일반적으로 파일시스템은 여러 개의 layer로 나뉘어져 구성된다. 위의 그림에서 각 layer를 간단하게 살펴보자. Logical file system여기서는 파일시스템의 메타데이터를 관리한다. File-organization module이 layer에서는 파일의 logical block 주소를 physical block 주소로 변환해준다. 예를들어 하드디스크를 매체로 사용하게되면 sector 주소로 접근해야 하는데 이를 위한 변환을 진행한다. Basic file system여기서는 위에서 변환된 physical block 주소를 가지고 읽고 쓰도록 command를 날린다. 여기서 DMA를 사용한다. I/O control이는 device driver다. device driver가 하드웨어에 맞게 명령을 전달한다. Virtual File System파일시스템은 어쩔 수 없이 사용하는 매체에 의존성이 있다. 그래서 파일시스템은 종류가 여러가지가 있을 수 있다. 예를들어 요즘은 잘 안쓰지만 CD-ROM이 있을 수 있고 USB, SSD, Disk 그리고 파일이 현재 호스트가 아닌 다른 네트워크의 호스트에 있을 수도 있다. Linux kernel 에서는 여러가지 파일시스템을 지원한다.VFS는 다양한 logical file system 들을 추상화한다. 따라서 VFS를 통해 실제로 시스템에는 여러개의 다른 파일시스템이 사용되더라도 마치 1개의 파일시스템만 사용하는 것처럼 프로프래밍 할 수 있다. 이는 OOP의 개념과 같은데 특정 파일에 대해 read/write syscall 이 호출되면 해당 파일이 속한 파일시스템 구현체의 read/write 가 호출되는 방식이다. File System data structure파일시스템에서 사용하는 자료구조를 알아보면서 파일시스템에 대한 이해를 높여보자.On-disk 그리고 In-memory 에서 사용하는 자료구조들을 볼 것이다. On-disk data structureOn-disk 자료구조들은 이미 이전 포스트인 파일시스템 디자인-1 에서 vsfs(very simple file system)에서 살펴봤던 내용들이 많다. Boot block첫번째 block으로 운영체제가 booting 하기위해 필요한 정보를 담아놓는 block이다. 하지만 이를 안만드는 경우도 많다. Super block파일시스템 관련 정보들이 어디에 저장되어있는지에 대한 metadata를 저장한다. 예를들어 inode table은 어디서 시작인지, data block은 어디 block부터 시작하는지, root directory에 대한 inode 번호 등을 저장한다. Super block은 보통은 각 disk partition의 첫번째 block 에 할당하고 이 super block 이 손상되면 파일시스템의 복구가 힘들기때문에 보통 중복해서 super block을 저장한다. 그리고 이 super block은 in-memory data structure로 메모리에 올려 캐싱한다. File control blockFCB(file control block)은 결국 inode와 같다. inode 자체도 disk에 저장이 필요하다. 다음의 disk structure layout를 살펴보자. 여기서 각 inode table을 여러개로 나누어 저장한 이유는 data block과 inode가 가까우면 성능을 높일 수 있기때문에 조금씩 inode 들을 나누어 설계했다. 다만 이런 내용들은 전적으로 파일시스템 구현에 달라진다.결국 여기서의 그림을 보면 파일 접근을 위해서 super block, inode, data block 에 대한 접근으로 파일 하나의 접근에 대해 3번의 disk access가 필요하다. In-memory data structure보통 파일시스템에서의 in-memory data structure 들은 거의 caching을 통한 성능향상 그리고 파일시스템 관리가 목표이다. Dentry이는 directory cache 이다. directory 접근을 위해 on-disk 의 super block 그리고 root directory 의 inode 부터 접근해서 dentry 라는 directory cache를 만든다. dentry로 파일접근시 해당 파일의 inode를 알기위해 여러번 disk 에 접근할 필요가 없이 memory 에서 처리할 수 있다. open file table이는 예전에 살펴본 내용으로 system-wide open file table 그리고 per process open file table 이 있는데 각각 open 한 file을 관리한다. per-process open file table 에서 각 index 번호가 file descriptor 가 된다. Buffer cache최근에 사용한 data block 을 memory 에 캐싱한다. Buffer CacheBuffer cache는 조금 더 자세히 살펴보겠다.Buffer cache는 파일의 메타데이터가 아닌 data block 자체를 메모리에 올려둔다. 보통 같은 data block을 다시 접근해야 하는 경우가 많기때문에 이를 캐싱해두면 다시 disk를 조회하지 않아도된다.다만 buffer cache는 완전한 software로 구현된다. 이 말은 virtual memory의 주소변환을 위해 TLB 같은 하드웨어를 도입하는게 아닌 순수한 software 레벨에서 구현한 캐시라는 의미이다.Buffer cache는 보통 물리메모리의 1 ~ 10% 정도로 할당하고 이는 kernel parameter로 수정이 가능하다. Buffer cache 도 공간이 한정되므로 교체알고리즘을 사용한다. 보통 disk access 에는 locality가 나타나기 때문에 LRU 방식을 사용한다.다만 DBMS 나 multimedia application은 LRU 로 이득을 볼 수 없는 경우가 대부분이기 때문에 이들은 buffer cache를 거치지 않고 바로 disk 에 접근할 수 있는 flag를 사용해서 buffer cache를 거치지 않도록 프로그래밍 한다. Read syscall 과 Write syscall 각각의 동작방식을 buffer cache 의 관점에서 살펴보자. Read syscall 먼저 read(fd, buf, size) syscall 을 호출한다. read syscall 의 두번째 인자인 buf 는 사용자의 buffer를 의미한다.그다음 VFS에서는 file descriptor를 보고 file이 있는 device를 알아낸 후 logical block number 를 physical block number 로 변환한다. buffer cache 쪽에 해당 block이 있는지 확인을 하고 block 이 없다면 파일시스템에서 가져와야한다. 만약 buffer cache에 이미 cache된 block이 있으면 VFS에 반환한다.cache된 block이 없다면 파일시스템에 block을 요청하고 이 read 를 요청한 프로세스는 sleep 한다. 즉, context switching 이 일어난다. I/O가 필요하기 때문이다. 나중에 disk에서 block을 읽어오면 interrupt 가 발생하고 그 block 을 다시 caching 해주고 block 을 반환한다.VFS에 반환할때는 application의 buf에 block들을 copy해주고 읽어온 byte 수를 반환한다. 만약 read 를 하더라도 buffer cache 에 대상 block 이 존재하여 cache hit 이 된다면, 해당 프로세스는 sleep 하지 않는다.그리고 buffer cache 쪽을 보게되면 hash table 구조로 구현되어 있고 value로는 linked list 로 각 cache 되어있는 data block 들이 있는 것을 확인할 수 있다. hash 의 key로는 보통은 block number 를 사용한다고 한다. Write syscall 먼저 write(fd, buf, size) syscall 을 호출한다.VFS에서 device 및 block number로 변환하여 buffer cache에 해당 block이 이미 있는지 확인한다. 만약 write 하려는 block 이 buffer cache에 없다면 먼저 파일시스템에서 읽어오도록 요청한다. 이 과정에서도 write를 요청한 프로세스는 sleep 한다.파일시스템에서 읽어왔으면 다시 write를 시도하면 해당 block이 buffer cache 에 존재하므로 이 경우에는 해당 buffer cache의 data block에 직접 write를 수행하고(in-memory) 해당 block이 disk에 있는 block과 일치하지 않는다는 표시를 하기위해 dirty bit를 체크한다.만약 처음부터 buffer cache에 write 시도시 해당 block 이 buffer cache에 있었다면 바로 그 block에 write 하고 반환한다. 이런 방식이면 buffer cache와 disk 의 sync가 깨지게 되는데 이는 다른 worker 스레드가 주기적으로 dirty bit가 체크된 data block 들을 disk에 sync 시켜준다. kworker이 kworker 라는 kernel thread가 buffer cache와 disk block 간의 동기화를 수행한다.예를들어 data block은 30초, metadata는 5초마다 동기화를 시킬수도 있다. 다만 이런 구현은 실제 OS 구현마다 다르며 파일시스템 구현마다도 다르다.다만 이런 방식이면 순간적으로 머신이 꺼지거나 할때 동기화가 되지 않은 부분은 유실될 수 있다. 즉 write를 하더라도 그 내용이 disk에 반영된다는 보장이 없다. 이를 위해 저널링 파일시스템 같은 대안을 사용한다.Application 에서 fsync system call을 사용하면 강제적으로 buffer cache의 내용을 disk에 반영한다.DBMS도 구현마다 다르지만 매번의 update 마다 fsync 를 수행하지는 않는다. DBMS에서는 이를 write ahead log 과 transaction을 이용해 해결한다. Memory-Mapped File(mmap)mmap은 파일을 프로세스의 address space 한 부분으로 mapping 한다. 파일을 프로세스의 memory space 에 그냥 가져다가 붙힌다고 생각하면 쉽다.이렇게 되면 단순히 memory access instruction 을 통해 파일에 대한 read, write을 할 수 있다. kernel은 이런 memory access instruction 을 적절하게 read, write로 변환하여 수행해준다.mmap 함수는 다음과 같다. void* mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset) start는 단지 이 주소를 사용했으면 좋겠다는 의미로 0을 보통 넣는다. 그리고 offset 부터의 length 만큼의 바이트를 start 주소로 매핑하기를 원한다는 의미이다.fd는 파일에 대한 file descriptor 로 이미 존재하는 파일에 대한 file descriptor 를 넘겨 이를 읽거나 수정할 수도 있고, 새로운 파일을 O_CREAT flag 와 함께 open 하여 이 file descriptor 를 넘겨 새로운 파일을 쓸 수도 있다. 유저는 파일에 대한 I/O 를 단지 memset, memcpy 같은 memory access 로 단순화 할 수 있다.그리고 여러개의 프로세스에서 동일한 파일을 open 하여 사용할 경우 kernel 은 동일한 파일에 대한 내용을 memory에 1개만 들고있으면 된다. mmap 파일의 동작을 그림으로 보자. 먼저 mmap 을 사용하지 않는 경우를 생각해보면, disk 에서 DMA로 data block을 가져오면 이를 buffer cache에 올리고 그다음 사용자 buffer 로 copy를 해야한다.buffer cache 로는 DMA 가 copy 해주지만, buffer cache 에서 user buffer 로는 CPU가 직접 copy 해야한다. 따라서 file IO 가 빨라지려면 CPU도 중요하다. mmap은 조금 다르다. mmap은 buffer cache를 사용하지 않는다. mmap은 buffer cache에 data block을 쓰지 않고 바로 kernel space로 DMA가 쓴다. 그리고 이를 프로세스의 address space 에 page table을 통해 매핑한다.따라서 user buffer 로의 copy가 존재하지 않는다.위의 그림에서는 process A 와 process B 가 각각 mmap 되어있는 physical fragment 를 공유하고 있다. mmap 에서는 flag에 MAP_SHARED flag 를 통해 다른 프로세스와 mmap 된 파일을 공유할 수 있다. 하지만 다른 동기화 같은 장치는 제공하지 않는다.process A 에서 먼저 특정 파일을 mmap 하였을때 그 다음 process B 에서 같은 파일에 대해 mmap을 하게되면 서로 반환받는 virtual address 주소는 다르지만, 결국 page table 에 매핑된 physical address 주소는 같아 같은 파일을 바라보게 된다.","link":"/2020/09/08/file-system-design/"},{"title":"파일시스템 1편 - 하드디스크","text":"이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다. 처음으로 파일시스템 자체를 보기전에 물리적인 device의 작동원리에 대해 이해를 하고 파일시스템을 보게되면 더 이해가 쉽습니다. 이번 편에서는 하드디스크에 대해 다룰 것입니다. 이번 편은 이전의 운영체제 3편 - 컴퓨터 구조와 I/O 글의 I/O 부분을 이해하고보면 도움이 됩니다. Disk Interface가볍에 Disk의 Interface를 알아보자.Disk는 많은 sector라는 것으로 이루어져 있고 각 sector는 읽거나 sector에 값을 쓸 수 있다. sector에는 번호가 있는데 0 부터 n - 1 까지 있다고 하면 총 n개의 sector가 disk에 있다고 할 수 있다.우리는 disk를 sector의 리스트로 생각할 수 있고 이 sector 들의 리스트가 disk의 address space를 결정한다. sector는 특별한 이유가 있지 않은이상 512byte의 크기를 가진다.다만 파일시스템에서는 block 이라는 단위로 디스크에 접근하는데 이는 4KB이다. 즉 파일시스템은 block 단위로 디스크에 읽고 쓴다.하지만 disk에서는 sector 단위(512byte)에서의 write만 atomic을 제공한다. 즉 block write를 하면 연속된 8개의 sector에 데이터를 쓴다. 그러나 각 sector 단위에서의 atomic만 보장하므로 write 하는 순간 disk에 문제가 생기면 block의 일부분만 쓰여지는 문제가 발생할 수 있다. 이를 torn write이라고 부른다. 밑에서 보게되겠지만 서로 인접해있는 sector들을 읽는 것이 random한 위치의 sector들을 읽어들이는 것보다 훨씬 빠르다. Disk Structure우선 대략적인 그림을 보며 Disk 구조를 알아보자. platter디스크는 최소 하나이상의 platter들로 이루어져 있다.위 그림에서 CD처럼 생긴 것을 platter라고 부른다. platter는 원형의 강한 재질로 이루어진 표면들로 구성되어 있고 이곳에 전자기적으로 데이터를 영구히 저장한다.platter는 윗부분, 밑부분 각각 데이터를 저장할 수 있다.보통 platter는 알루미늄으로 만들고 겉부분을 아주 얇은 자기성 성질을 가진 층으로 코팅한다. 이 부분에 자기성 성질을 가지므로 데이터를 영구히 저장할 수 있다. 그리고 이 platter는 가운데 spindle이라는 것에 붙어있으며, 이 spindle은 모터에 연결되어있어 platter들을 회전하도록 한다.이런 회전율을 RPM(Rotations Per Minute)으로 보통 측정하며 요즘에는 보통 7200rpm 부터 15000rpm 까지 가진다.10000rpm은 single rotation이 대략적으로 6ms정도 걸린다.일반적으로 platter는 계속해서 회전을 하고있다. track위에서 본 sector들이 원형으로 배치된다. 이런 원형 하나를 track이라고 부른다. 하나의 platter 표현에는 수천개의 track이 존재하며 거의 사람머리카락 수준의 두께를 가지고있다.하나의 track에 대한 대략적인 그림은 다음과 같다. 이 track은 12개의 sector들로 이루어져 있고 각 sector들은 512byte이다. 각 sector들에는 주소가 있는데 여기에서는 0 부터 11까지 존재한다. cylinderCylinder는 각 platter들에 같은 반지름을 가진 track들을 의미한다. 위 그림을 보고 이해할 수 있다. Read &amp; Writeplatter의 표면으로부터 데이터를 읽고 쓰기 위해서는 disk의 전자기적 패턴을 읽어들이거나 이를 쓸 수 있어야 한다.이러한 것들이 disk head를 통해 가능하다.head는 platter 표면마다 한개씩 존재하며 이 head 들은 disk arm이라는 것에 붙어있다. 표면을 따라 움직이며 이들을 움직임으로서 적절한 track에 접근할 수 있다. 위에서 platter는 시계반대방향으로 회전하며, track이 회전하면서 head가 적절한 sector위에 놓여지면서 올바른 데이터를 읽고 쓸수 있다. Disk Access위에서 본 track 그림을 다시보자. Rotational Delayhead가 6번 sector위에 놓여있는 상황에서 sector 0을 읽어야 하는 요청을 디스크가 받았다고 하자. 이를 어떻게 읽을 수 있을까? 여기서는 그냥 sector 0이 head위에 오도록 rotate 시키면된다.이를 회전시키는데 시간이 걸리는데 이를 rotational delay라고 부른다.위 예제에서 rotational delay가 R이라면 sector 0까지 회전하는데에는 R/2의 시간이 걸린다.만약 sector 5를 읽어야 한다면 전체 한바퀴를 돌려야하므로 R의 시간이 걸린다. Seek time 위의 왼쪽 그림에서 현재 head는 가장 안쪽 track에 sector 30번 위에 놓여있다.여기서 sector 11번을 읽으라는 요청이 들어오면 어떻게 해야할까?Disk는 먼저 head를 sector 11번이 있는 track으로 이동을 해야한다.이러한 과정을 seek라고 한다. 그리고 이에 걸리는 시간을 seek time이라고 한다. 위의 오른쪽 그림에서 sector 11번을 읽기위해 head가 세번째 track으로 이동했다. 그 와중에 platter 또한 회전했다는 것을 알 수 있다. 현재 sector 9에 head가 위치하므로 rotational delay만 기다리면 원하는 sector를 읽을 수 있겠다. Transfer Delay위의 예제에서 sector 11번이 head를 지날때 데이터를 읽거나 데이터를 write한다.이를 transfer라고 한다. 그리고 이에 걸리는 시간을 transfer delay라고 한다.보통 transfer delay는 I/O Bus를 통해 memory에 저장이 될때까지의 시간을 포함한다. 여기서 disk head가 읽어들이는 시간과 이를 DMA engine이 읽어들이는 시간보다 가장 영향을 많이 받는 것은 I/O Bus의 latency 이다. DMA engine은 충분히 빠르다. 그래서 결국 데이터를 읽거나 쓰려면 원하는 track을 seek하고 적절한 sector에 접근하기 위해 rotational delay를 기다리고 그리고 transfer 하게된다. Track skew보통 track의 경계를 넘어 head가 왔다갔다 할때에도 rotational delay를 줄여 연속적인 읽기가 가능하도록 sector 번호를 배치한다. 아래 그림을 보자. 위 그림에서 읽고 싶은 sector 번호가 22, 23, 24, 25라고 하자.head가 가장 가까운 다음 track으로 이동한다고 하더라도 head를 정확하게 reposition하는 시간이 필요하다.이런 시간들을 고려하여 sector을 위와같이 배치한다면 sector 22, 23까지 읽고 head를 옮겨 seek time이 지나면 head가 sector 24번에 위치함으로서 서로 다른 track에 위치한 연속적인 sector를 읽어들일때 rotational delay를 줄일 수 있다는 것이다. 위에서는 각 track에서 sector의 시작점이 2개씩 차이난다. Multi-zonedtrack이 바깥쪽에 있을수록 원형구조상 sector들을 더 많이 가질 수 밖에 없다.그래서 연속적인 track을 묶어 이들을 각각 zone이라고 부르고, 같은 zone에 있는 track들은 같은 sector 개수를 가진다. 원형의 가장 바깥쪽에 위치한 zone들의 track은 가장 많은 sector 개수를 가진다. Cache요즘 대부분의 disk 안에는 캐시가 존재한다. 이를 track buffer라고 부르며 보통 작은 크기의 메모리로 구성된다.예를들어 특정 sector을 읽어야하는 요청이 들어오면, 그 track의 모든 sector를 읽고 buffer에 저장해놓는다. Write 연산은 크게 두가지 방식으로 캐시를 활용할 수 있다.첫번째로는 write 요청을 memory에만 써놓고 이를 비동기적으로 disk에 동기화시킨다. 이는 write 요청에 대한 응답이 매우 빠르지만 disk에 반드시 쓰여졌음을 보장할 수 없다. 이런 방식을 write back이라고 부른다. 두번째로는 실제로 write 요청시 disk에 쓰고 buffer을 날려버릴 수 있다. 이런 방식을 write through라고 부른다. I/O 시간 계산결국 Disk I/O 시간은 다음과 같이 계산된다. TI/O = Tseek + Trotation + TtransferDisk에서는 Rate of I/O라는 단위로 성능을 측정해서 비교하곤 한다. Rate of I/O는 간단하게 1초동안 얼마나 I/O를 처리할 수 있는가로 이해할 수 있다. Rate of I/O는 일정시간동안 I/O를 처리하고 transfer 할 수 있는 크기를 I/O를 처리하는데 걸린 시간으로 나누면 된다. RI/O = Sizetransfer / TI/O예제로 disk의 random access와 sequential access의 Rate of I/O를 측정해보자.예제의 기준으로 사용한 disk는 Seagate의 높은 성능을 가진 SCSI drive인 Cheetah 15K.5와 SATA drive인 Barracuda이다. 이에 대한 스펙은 다음과 같다. Cheetah 15K.5 Barracuda Capacity 300 GB 1 TB RPM 15,000 7,200 Average Seek 4ms 9ms Max Transfer 125 MB/s 105 MB/s Platters 4 4 Cache 16 MB 16/32 MB Connects via SCSI SATA 먼저 random location의 4KB 크기의 read가 발생할때 시간이 얼마나 걸릴지 계산해보자.먼저 Chheta 15K.5 의 경우이다. Tseek = 4ms, Trotation = 2ms, Ttransfer = 30 micro secaverage seek time은 표에서 찾을 수 있다.rotation은 15000rpm을 가지므로 한번 rotate시 4ms 걸린다. 보통 평균으로 절반을 회전하므로 2ms로 계산한다.transfer time은 125 Mega Byte per sec을 역산하면 30 micro sec이 걸린다. 대략적으로 4KB의 I/O 시간은 6ms 가 되겠다.그러므로 이를 기반으로 Rate of I/O를 계산하면 Cheetah 15K.5의 random access read의 RI/O는 대략적으로 0.66MB/s 이다. 만약 sequential read 이고 100MB 대상으로 파일을 읽는다고 해보자. 이 경우에는 sequential read이므로 1 seek, 1 rotation 그리고 대량의 transfer time이 걸릴 것이다.(물론 sector들이 여러개의 track에 있을 수 있지만 그냥 가정이다.)4KB에 30 micro sec의 시간이 걸렸으므로 100MB에는 768ms가 걸린다. 대략적으로 800ms 걸린다고 가정한다면 100MB의 read에는 800ms가 걸린다.이를 기준으로 sequential read의 RI/O는 125MB/s 이다. Barracuda도 이와 동일하게 계산하면 결과는 다음과 같다. Cheetah 15K.5 Barracuda RI/O Random 0.66 MB/s 0.31 MB/s RI/O Sequential 125 MB/s 105 MB/s 위의 결과를 보면 Random access와 Sequential access의 성능차이는 어마어마 하다. 거의 200배, 300배 차이가 나는 것을 볼 수있다. 또한 제품마다 특성이 다르며 각 용도에 따라 올바르게 disk를 선택하는 것이 좋다.예를들어 RPM 수치가 높다고 반드시 좋은 것은 아니다. RPM이 높아질수록 안정성이 떨어지기 때문이다. 데이터를 오랫동안 안정적으로 보관하려면 낮은 RPM 제품을 선택하는게 도움이 될 수 있다. Disk Scheduling위에서 성능을 보았듯이 disk I/O는 millisecond 단위로 동작하기 때문에 비용이 꽤 높은 작업이다. 따라서 전통적으로 OS는 이를 개선하기 위해 I/O 작업들의 순서를 결정하는 역할도 같이 했다. Kernel 내부 disk controller에 Disk I/O 요청을 담아두는 queue를 두고 이 queue를 보고 알고리즘에 따라 순서를 변경한다. Scheduling에는 여러가지 방식이 있는데 이를 하나하나 상세히 알아보지는 않을 것이다. 이해하는데 난이도도 어렵지 않으며 우리가 Disk scheduling을 걱정할 필요가 없기때문이다. 이는 이미 기술이 다 완성되었다고 판단할 수 있으며 우리가 scheduling 알고리즘을 바꾸거나 할 경우는 없다. Disk Scheduling 알고리즘에는 seek time을 최소화하는 SSTF, 엘레베이터 알고리즘과 비슷한 SCAN 방식 등이 존재한다.하지만 이들은 head의 움직임에 대한 cost만 고려하지 rotate는 고려하지 않는다. 이들을 해결하기 위한 알고리즘이 SPTF 알고리즘이다. SPTF(Shortest Position Time First)SPTF는 어떤 정해진 알고리즘이라는 느낌보다는 그 때의 경우에 따라 적절하게 처리하는 알고리즘이라고 이해하면 쉽다.아래의 예시를 보자. 현재 head가 sector 30번에 있고 그 다음에 방문해야 하는 sector가 16과 8이 있다. 어느 sector를 먼저 방문해야할까?이는 경우에 따라 다르다.만약 seek time이 rotational delay보다 훨씬 오래걸린다면 sector 16번부터 방문하는 것이 합리적일 것이다. 만약 그 반대인 rotational delay가 훨씬 길면 어떨까? 이 경우에는 sector 8번을 먼저 방문하는 것이 합리적이다. 요즘의 disk들은 seek time와 rotational delay가 대략적으로 비슷하다. 그러므로 각각 spec과 상황에 따라 적절하게 결정하는 것이 SPTF의 핵심이다.SPTF는 OS에서 구현하기는 힘들고 device 내부에서 각자의 상황에 맞게 구현되고는 한다. Scheduling은 어디서?Disk Scheduling은 결국 어디서 일어날까?위에서도 살짝 언급했듯이 예전에는 Disk scheduling을 모두 OS에서만 처리했다. OS에서 기다리고 있는 request 들을 queueing 하고 있다가 다음 request를 요청할때 이 queue에서 적절한 request를 선택하여 disk에 요청했다. 요즘은 disk 자체가 내부적으로 각자 device에 맞는 뛰어난 scheduling 기법을 내재하고있다. 그리고 OS도 적절하게 OS가 판단한 sector 번호를 disk에 요청하면 disk도 이를 queueing 하고 적절하게 구현한 알고리즘에 따라 처리한다. 또 한가지 볼만한 점은 OS 레벨에서 연속된 sector에 대한 요청이 있을시 이들을 merge한다.예를들어 read 요청이 33, 8, 34 순서로 도착했다면 OS는 이를 보고 33, 34을 2개의 sector를 대상으로 하는 1 개의 request로 merge한다. 이는 disk 오버헤드를 크게 줄여준다. References https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4","link":"/2020/09/05/file-system-hard-disk/"},{"title":"자바 ForkJoin Framework(포크조인)","text":"이번 글은 자바 7에 도입된 Fork/Join Framework에 대한 내용입니다. Fork/Join Framework자바 7에는 Fork/Join Framework가 도입되었는데 이는 ExecutorService의 구현체로서 이를 활용하면 작업들을 멀티코어를 사용하도록 작업할 수 있습니다. 기본적으로 Fork/Join은 하나의 병렬화할 수 있는 작업을 재귀적으로 여러개의 작은 작업들로 분할하고 각 subtask들의 결과를 합쳐서 전체 결과를 반환합니다.Fork/Join은 divide-and-conquer 알고리즘과 굉장히 비슷하다. 다만 Fork/Join Framework는 한가지 중요한 개념이 있는데 이상적으로는 worker thread가 노는경우가 없다. 왜냐하면 Fork/Join Framework에서는 work stealing이라는 기법을 사용해서 바쁜 worker thread로 부터 작업을 steal, 즉 작업을 훔쳐온다.먼저 ForkJoin의 thread pool에 있는 모든 thread를 공정하게 분할한다. 각각의 스레드는 자신에게 할당된 task를 포함하는 double linked list를 참조하면서 작업이 끝날때마다 queue의 헤드에서 다른 task를 가져와서 처리한다. 다만 아무리 공정하게 태스크들을 분할한다고 해도 특정 한 스레드는 다른 스레드보다 자신에게 할당된 태스크들을 더 빠르게 처리 할 수 있는데, 이렇게 자신에게 주어진 태스크들을 다 처리해서 할일이 없어진 스레드는 다른 스레드의 queue의 tail에서 작업을 훔쳐(steal)온다. 모든 태스크가 다 끝날때까지 이 과정을 반복하여 스레드간의 작업부하를 균등하게 맞출 수 있다. ForkJoinPooljava.util.concurrent.ForkJoinPool은 위에서 설명한 work stealing 방식으로 동작하는 ExecutorService의 구현체이다. 우리는 ForkJoinPool의 생성자로 작업에 사용할 processor number를 넘겨줌으로서 병렬화 레벨을 정할 수 있다. 기본값은 Runtime.getRunTime().availableProcessors() 결과로 결정된다. 또 다른 특징으로는 ExecutorService들의 구현체와는 다르게 ForkJoinPool은 모든 워커 스레드가 데몬스레드로 명시적으로 program을 exit할 때 shutdown을 호출할 필요가 없다. ForkJoinPool의 내부에서 worker thread를 등록하는 과정에서 daemon 스레드로 설정한다. ForkJoinTaskjava.util.concurrent.ForkJoinTask는 ForkJoinPool에서 실행되는 task의 abstract class이다. ForkJoinTask&lt;V&gt;는 Future&lt;V&gt;를 구현한다. ForkJoinTask는 일종의 light 한 스레드라고 생각하면 쉽다. 여러개의 task 들이 생성되면 이들은 ForkJoinPool의 설정된 스레드들에 의해 실행되게 된다.RecursiveAction와 RecursiveTask&lt;R&gt;가 ForkJoinTask의 서브클래스들인데 이들 또한 abstract class들이다. 그래서 이들을 구현한 서브클래스를 만들어서 사용한다. RecursiveAction과 RecursiveTask&lt;R&gt;의 차이점은 RecursiveAction은 태스크가 생성하는 결과가 없을때 사용하고 결과가 있을때에는 RecursiveTask&lt;R&gt;을 사용한다. 두 클래스 모두 abstract method인 compute() 를 구현해야한다. ForkJoinTask는 현재 실행상태를 확인하기 위한 몇가지 메서드를 제공한다.isDone()은 태스크가 완료되었는지의 여부를 반환한다. isCompletedNormally()는 태스크가 cancellation이나 exception 없이 완료되었는지의 여부를 반환하고 이외에도 isCancelled(), isCompletedAbnormally() 등의 메서드를 제공한다. RecursiveTask 활용스레드 풀을 이용하기위해 RecursiveTask&lt;R&gt;의 서브클래스를 만들어보자. parameter type R은 결과 형식을 의미한다. 우리는 RecursiveTask의 compute 메서드를 구현해야 한다.protected abstract R compute();compute 메서드는 태스크를 서브태스크로 분할하는 로직과 더이상 분할할 수 없을때 개별 서브태스크의 결과를 생산할 알고리즘을 정의한다. 따라서 대부분의 compute 메서드의 구현은 다음과 같다. 12345678if (태스크가 충분히 작거나 분할할 수 없으면) { 태스크 계산} else { 태스크를 두 서브태스크로 분할한다. 태스크가 다시 서브태스크로 분할되도록 이 메서드를 재귀적으로 호출한다. 모든 서브태스크의 연산이 완료될때까지 기다린다. 각 서브태스크의 결과를 합친다.} 그렇다면 1부터 N까지의 합을 구하는 프로그램을 Fork/Join Framework를 사용하여 작성해보자. 코드는 다음과 같다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.concurrent.RecursiveTask;public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; { private final long[] numbers; private final int start; private final int end; private static final int THRESHOLD = 10_000; public ForkJoinSumCalculator(long[] numbers) { this(numbers, 0, numbers.length); } private ForkJoinSumCalculator(long[] numbers, int start, int end) { this.numbers = numbers; this.start = start; this.end = end; } @Override protected Long compute() { int size = end - start; if (size &lt;= THRESHOLD) { return computeSequentially(); } ForkJoinSumCalculator leftTask = new ForkJoinSumCalculator( numbers, start, start + size / 2); leftTask.fork(); ForkJoinSumCalculator rightTask = new ForkJoinSumCalculator( numbers, start + size / 2, end); long rightResult = rightTask.compute(); long leftResult = leftTask.join(); return leftResult + rightResult; } private long computeSequentially() { long sum = 0; for (int i = start; i &lt; end; i++) { sum += numbers[i]; } return sum; }} 그리고 다음과 같이 ForkJoinPool의 invoke 메서드를 사용해 실행시켜보자. 1234567891011121314import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.stream.LongStream;public class Main { private static final long N = 30_000_000L; public static void main(String args[]) { long[] numbers = LongStream.rangeClosed(1, N).toArray(); ForkJoinTask&lt;Long&gt; task = new ForkJoinSumCalculator(numbers); long sum = new ForkJoinPool().invoke(task); System.out.println(sum); }} Fork/Join을 사용할 때 왼쪽 작업과 오른쪽 작업에 모두 fork를 호출하는게 자연스러운것 처럼 보이지만 한쪽에는 fork를 호출하는 것 보다 compute를 호출하는게 더 효율적이다. 한 태스크에는 이 Fork/Join 스레드를 실행시킨 스레드를 재사용할 수 있으므로 불필요한 태스크를 다른 스레드에 할당하는 오버헤드를 피할 수 있다.또 멀티코어에 Fork/Join을 사용하는게 무조건 순차처리보다 빠르지 않다. 각 서브태스크의 실행시간이 새로운 태스크를 forking하는데 드는 시간보다 충분히 길수록 좋다. 위의 예제에서는 덧셈을 수행할 숫자가 만개 이하면 분할을 더이상 하지 않고 계산했다. 그러면 현재는 태스크가 3천개가 생성되는데 어차피 코어의 수는 정해져있으므로 코어가 3개라면 각 코어마다 1천만개씩 덧셈을 수행하면 딱 알맞게 효율적으로 동작하지 않을까?그렇지는 않다. 실제로는 코어 개수와 관계없이 적절하게 작은 크기로 분할된 많은 태스크를 forking 하는것이 바람직하다. 1천만개씩 덧셈을 수행하도록 한다고 해도 각 3개의 코어에서 이루어지는 작업이 동시에 끝나지는 않는다. 각 태스크에서 예상치못하게 지연이 생길 수 있어 작업완료시간이 크게 달라질 수 있다. 다만 Fork/Join Framework는 work-stealing 기법으로 idel한 스레드는 다른 스레드의 workQueue로 부터 작업을 훔쳐오기 때문에 모든 스레들에게 작업을 거의 공정하게 분할할 수 있다. 그러므로 태스크의 크기를 작게 나누어야 스레드 간의 작업부하 수준을 비슷하게 맞출 수 있다. References https://howtodoinjava.com/java7/forkjoin-framework-tutorial-forkjoinpool-example/ https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html","link":"/2019/07/15/fork-join-pool/"},{"title":"파일시스템 2편 - Flash 그리고 SSD","text":"이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.이전 포스트의 하드디스크에 이어 이번 포스트에서는 Flash 그리고 SSD에 대해 알아볼 것입니다. 이번 편은 이전의 운영체제 3편 - 컴퓨터 구조와 I/O 글의 I/O 부분을 이해하고보면 도움이 됩니다. Flash Memory하드디스크와 다르게 Flash는 메모리나 CPU처럼 트랜지스터 들로 구성이 되어있다. 즉 disk의 arm이 움직이거나 platter가 회전하는 물리적인 움직임이 없다.Reliability 측면에서도 disk에 비해 훌륭한 편이다. Disk는 물리적인 head crash가 날수도 있고, dead block(더이상 사용할 수 없는 block) 자체가 생길 수 있다. 물리적으로 읽고 쓰기 때문이다. 그리고 disk는 생각보다 자주깨진다.이와는 다르게 Flash는 순수한 silicon으로 구성되어 있으며 전자적으로 동작하기에 더 신뢰성이 높다. Flash는 또한 매우 빠른 access time을 제공하고 power가 적게드는 반도체 특성으로 disk에 비해 매우매우 저전력이다. Flash Memory는 여러타입이 있는데 보통은 NAND-Flash를 의미한다. 다만 Flash가 가진 특이한 특성들 때문에 이를 해결하기 위해 몇가지 기법들을 적용해야한다. 이들은 밑에서 자세히 알아볼 것이다. Flash chip은 하나의 transistor의 1개 이상의 bit를 저장할 수 있다. SLC(Single-Level Cell) flash는 오직 1개의 bit만 transistor에 저장할 수 있고, MLC(Multi-Level Cell) flash는 2개의 bit를 저장할 수 있다. 그러므로 00, 01, 10, 11을 저장할 수 있다. TLC(Triple-Level Cell) flash는 3개의 bit를 저장가능하다. 전반적으로 SLC가 더 성능이 좋고 가격이 비싸다. Block and Page이를 설명하기 전에 명확히 해야할게 여기서도 block과 page 용어가 등장한다. 하지만 Flash에서 말하는 block과 page는 디스크에서의 block 그리고 virtual 메모리에서의 page와 다르다. Flash는 크기에 대해 두가지 단위를 사용하는데 그것이 block과 page이다.전형적으로 block은 128KB 혹은 256KB 의 크기를 가진다. page는 이보다 작은 2KB 혹은 4KB의 크기를 가진다.Flash는 여러개의 block들로 이루어져 있고 1개의 block은 여러개의 page를 가진다. 위 그림은 아주 간단하게 표현한 Flash chip의 구성이며 총 3개의 block, 그리고 각 block은 4개의 page로 구성되어 있다. Basic Flash OperationsFlash chip 에서 제공하는 기본적인 3개의 low-level operation 들이 있다. read, erase, program 이 3가지이다. ReadRead는 읽으려는 page number를 받아 page 1개를 읽어들인다. 보통 page를 읽어들이는데 10 micro sec 정도 걸리며 읽으려는 location에 무관하다. 즉 disk처럼 이전에 실행한 operation에 따라 head 위치와 rotation에 의존하는 방식이 아니며 어떤 location이라도 균일한 높은 읽기성능을 제공한다. EraseFlash는 page에 쓰기전에 page가 속한 block 전체의 지움이 선행되어야 한다. 즉 page가 속한 block을 먼저 erase해야한다. 그러므로 erase 전에 block을 memory나 다른 flash block에 copy 해놓아야 한다. Block이 지워지면 모든 bit이 1로 설정된다. erase operation은 비싼 작업이며 millisecond 단위이다.erase가 되면 해당 block은 program 가능한 상태가 된다. ProgramBlock에 erase가 먼저 진행된 page에 program을 할 수 있다. erase가 진행되었으니 해당 block은 모든 bit이 1로 설정되어있을 것이다. 이 1로 설정되어 있는 bit를 적절하게 0으로 바꾸면서 데이터를 write 하는데 이를 program이라고 한다. program은 erase에 비해 빠른편이며 100 micro sec 정도 걸린다. Flash chip의 각 page는 metadata 정보를 담을 수 있는 조그만 공간이 존재한다. 이곳에 state 정보를 담고있다.Page는 INVALID 상태에서 시작한다. Block이 erase 되었으면 그 안에있는 page 들은 ERASED 상태가 된다. 이들은 programmable 한 상태이다. page에 program을 하면 VALID 상태가 되며 이는 내용이 써져있는 상태이고 읽을 수 있는 상태임을 뜻한다.Page는 한번 program 된 이후에 이 내용을 바꾸기 위해서는 해당 page의 block을 먼저 erase해야한다.다음 예제를 보며 이해하면 쉽다. block마다 4개의 page가 있는 구조에서 page에 program 하는 과정을 표현했다. 123456 IIII // 초기는 4개의 page 모두 INVALID stateErase() -&gt; EEEE // block erase로 모두 ERASED stateProgram(0) -&gt; VEEE // page 0을 programProgram(0) -&gt; ERROR // page 0에 다시 program 할 수 없음Program(1) -&gt; VVEE // page 1을 programErase() -&gt; EEEE // block erase로 모두 ERASED state Flash chip ReliabilityFlash에서 read는 굉장히 쉽고 그냥 읽으면된다. disk보다 훨씬 빠른 access time을 제공하고 random read 성능이 뛰어나다. SLC의 경우 10 micro sec만 걸릴뿐이다. Disk는 읽기도 millisecond 단위였던걸 기억하자.하지만 Flash의 물리적인 특성상 page에 쓰기위해서는 해당 block이 먼저 지워져야 한다. 이를 Program / Erase 라고 하여 PE cycle이라고 부른다. 하지만 block에 PE cycle이 반복될시 flash chip에 reliability 문제가 생긴다.Flash에서는 block 당 가능한 P/E cycle 횟수가 한정적이다. MLC(Multiple-Level Cell) 에서는 1만번의 P/E cycle 수명을 가진다. SLC는 이보다 높은 10만번의 P/E cycle 수명을 가진다.이를 넘어가면 그 block이 unstable 해진다. 조금 더 정확히는 bit를 0과 1을 구분하기가 점점 어려운 상태가 되므로 그 block은 더이상 사용되지 못하게 된다. 이를 wear out 이라고 부른다.따라서 Flash에서는 wear out을 해결하기 위한 또다른 숙제가 존재한다. 또 다른 reliability 문제로는 disturbance라는 문제가 존재한다. Flash의 page를 읽을때 같은 block에 있는 주변의 page들의 bit를 flip할 수 있는 가능성이 있다. 이를 read disturb라고 부르며 block erase 이후의 page read count의 threshold를 넘어가면 해당 문제가 일어날 가능성이 높다고 한다. 이를 해결하기 위한 숙제도 존재한다. Flash to SSD이제 flash chip의 특성에 대해 어느정도 알아보았다. 이 flash chip 하나로는 storage로 활용하기 힘들고 flash chip 여러개를 모아 flash 기반의 SSD를 만든다. 표준 storage 인터페이스는 block 기반으로 512 byte인 sector 크기 단위로 읽고 쓰여질 수 있다.flash 기반의 SSD의 역할중 하나가 내부적으로는 flash를 사용하지만 그 위에서 이 standard storage block 인터페이스를 외부에 제공하는 것이다. 내부적으로 SSD는 여러개의 flash chip 들로 구성되어 있으며 위 구조에서도 볼수있듯이 내부에 SRAM 같은 메모리도 존재한다. 이 메모리로 캐싱이나 buffering에 활용한다. 그리고 device operation을 위한 control logic을 포함한다.이 control logic 에서 해야하는 주된 내용은 client로부터의 read, write를 내부적으로 적절하게 flash operation으로 변환하는 것이다. FTL(Flash Translation Layer)가 이 역할을 하게된다.FTL은 logical block 기반의 read, write 요청을 받고 이를 flash의 read, erase, program operation 으로 적절하게 변환한다. 그리고 FTL은 이런 작업들을 높은 성능과 높은 reliability를 제공하면서 진행해야 하는 책임도 가지고 있다.높은 성능을 위해 여러개의 flash chip들을 병렬로 활용하기도 한다.그리고 flash 특성상 INVALID 혹은 VALID 상태에 있는 page에 program을 하기 위해서는 반드시 그 block은 먼저 erase가 선행이 되어야 하는데 page 1개만 변환하려 해도 전체 block을 다시 써야하는 문제가 있다. 이를 쓰기 증폭, 즉 write amplification 이라고 부르는데 이를 줄이기 위한 노력도 같이한다. 그리고 높은 reliability를 제공하기 위해 wear out 문제도 고려해야한다. 한개의 block에만 PE cycle을 하는 것이 아닌 최대한 모든 block에 균등하게 처리되도록 해야 wear out을 막을 수 있다. 이를 wear leveling 이라고 한다.또 위에서 본 disturbance를 최소화하기위해 erased 된 block에 page를 낮은 page부터 높은 page 순서로 program 하는 방법을 택한다. FTL 구현어떻게 하면 FTL을 구현할 수 있을까?간단하게 logical page number 그대로 physical page number 로 매핑을 해주는 FTL이 있다고 생각해보자.이 방식이라면 그대로 매핑해주는 방식이기에 logical page N 에 대해 read 요청이 오면 그대로 physical page N 을 읽어 반환한다. read에는 큰 문제가 없다.write를 생각해볼때, write 요청이 오면 flash 특성상 해당 page의 block을 erase 한 후 그 block의 page로 다시 program 해야한다. 그래야 다음 read에 대해 동일한 physical page N을 읽을 수 있도록 보장할 수 있다.이 방식으로 FTL을 구현하면 첫번째로 성능측면에서 문제가 있고 reliability 문제도 존재한다.flash의 특성으로 page에 write을 할때마다 해당 block의 모든 page를 먼저 읽어들이고, 해당 block을 erase, 그리고 이 block의 page에 다시 program을 해주어야한다. 이를 매 write 마다 해주어야 하므로 디스크의 write보다 느리다. 그리고 파일시스템의 metadata 혹은 data block이 업데이트 될때마다 같은 block이 PE cycle을 반복하게 되므로 그 block은 빠르게 wear out이 될 것이다. 이처럼 write 요청을 다른 여러 물리 block에 균일하게 배정하지 않으면 block이 빠르게 wear out 된다. 그러므로 direct로 logical page를 그대로 physical page로 매핑하는 방식은 좋은방법이 아니다. Log-Structured FTL여기서는 log structured 방식을 설명하는데 이는 storage device 뿐만아니라 file system 에서도 유용한 아이디어이다. 대부분의 FTL은 log structured 방식을 사용한다. 이를 살펴보자. Log-Structured 방식에서는 logical page에 write 요청이 왔을시, 현재까지 쓰여진 block에 바로 다음 free spot에 데이터를 쓴다. 이런 방식을 logging 이라고 부른다.하지만 이런 방식으로 쓰면 read를 할때 logical page가 어떤 physical page에 쓰여졌는지 기록을 해놓아야 하므로 mapping table이 필요하다. 예시를 보며 이해해보자. 클라이언트는 그저 device를 sector 단위인 512 byte(혹은 sector의 group)로 읽고 쓸수있는 전형적인 disk 라고 생각한다는걸 잊지말자. 여기서의 클라이언트는 파일시스템이라고 하자.예시를 간단하게 하기위해 몇가지 가정을 하자. 파일시스템은 4KB 단위의 chunk로 데이터를 read, write 한다. SSD는 16KB의 block들로 이루어져있고 각 block은 4KB의 page들로 이루어져 있다. 클라이언트는 다음 순서로 operation을 요청한다. 데이터 “a1”을 logical block 100에 write한다. 데이터 “a2”을 logical block 101에 write한다. 데이터 “b1”을 logical block 2000에 write한다. 데이터 “b2”을 logical block 2001에 write한다. 여기서의 logical block number는 flash의 block과는 다름을 주의하자. 위 요청을 받았을때 초기의 SSD는 모든 block의 page들이 INVALID 상태이므로 어느 block에 이를 program하든지 erase가 먼저 선행되어야 한다. FTL이 이를 block 0에 program 한다고 가정했을때 먼저 block 0을 erase 한다. block 0 은 이제 program을 할 수 있는 상태이다. 대부분의 SSD는 앞에서 본 disturbance(read 시 주변 셀들을 변경할 수 있음) 문제를 줄이기 위해 page를 앞에서부터 차례로 쓴다.그러면 처음 logical block 100에 대한 요청을 physical page 0에 write한다. 위처럼 physical page 0에 data가 쓰여졌다. 만약 파일시스템이 logical block 100을 다시 읽으려면 어떤 처리를 해주어야할까? FTL은 read 요청을 받았을때 logical block을 physical page로 적절하게 mapping 해줄 수 있어야한다. 이를 위해 in-memory mapping table을 만들어 기록하자. SSD에 write을 할때에는 현재까지 작성한 block의 다음 비어있는 page에 program한다. 그리고 이에 대한 정보를 mapping table에 기록한다. 다음 해당 page들에 대한 read요청이 왔을때 client로 부터 온 logical block 을 physical page로 mapping table을 이용해 변환하여 실제로 어떤 physical page를 봐야하는지 결정한다. 나머지 3개의 logical block도 다 쓰게되면 다음과 같다. 최종상태는 위와같은 그림이 될 것이다.이런 logging 기반의 구조는 성능적으로 뛰어난데 매번 write 할때마다 block을 erase할 필요가 없다. 그리고 reliability 측면에서도 FTL이 write 시 여러 block을 골고루 사용하도록 설계할 수 있으며 이 덕에 device의 수명을 늘릴 수 있다. 이런 해결을 wear leveling한다 라고 표현한다. 하지만 이런 logging 기반 구조는 단점이 존재하는데 logical block을 수정해야 할때에는 garbage가 생긴다. 새로운 page에 다시 program 하므로 이전에 있던 page는 garbage가 된다. 그래서 SSD는 주기적으로 이런 garbage들을 정리하고 free space를 확보하는 garbage collection을 진행해야한다. 과도한 GC는 write amplification을 발생시키고 성능을 낮춘다.그리고 mapping table을 관리해야 하는 문제도 있다. mapping table이 클수록 그만큼 더 큰 memory를 요구하게 된다. mapping table을 in-memory로 관리하는데 power가 꺼지면 어떻게 될까? memory는 휘발성으로 당연히 날아간다. 하지만 mapping table이 없으면 read operation을 처리해주지 못하므로 SSD는 mapping table을 복구할 수 있는 장치를 마련해야한다.간단하게는 각 page에 out-of-band(OOB)라는 영역에 mapping 정보를 기입해놓을 수 있다. 이를 기반으로 SSD를 켤때 모든 page들을 읽어 mapping table을 재구성할 수 있다. 하지만 모든 page를 scan해야 하므로 성능이 좋지않다.최근에는 복잡한 logging과 checkpoint 방식의 기법으로 빠른 recovery를 지원한다. Garbage Collection위의 예제를 다시 이어서 보면 마지막에는 page 0과 1에 각각 logical block 100 그리고 101 이 매핑되어 있었다. 만약 logical block 100과 101을 다시 write 하면 어떻게될까?다음 block의 free page에 쓰여질 것이다. 그리고 page 0과 1은 VALID 상태이지만 최신버전이 아니므로 garbage이다.log-structure 기반의 device는 garbage를 계속 만들어내므로 free space를 더 확보하기 위해 garbage collection을 진행해야한다. 이는 최신 SSD에서도 고려해야하는 중요한 요소이다. GC의 기본적인 과정은 이와같다. 최소 1개이상의 garbage page를 가지고 있는 block을 찾는다. 그 block의 아직 live 한 page들을 읽는다. 읽은 live한 page들을 log(block)에 쓴다. 그리고 기존 block을 erase 하여 새로운 write에 사용할 수 있도록 한다. GC가 이 과정을 수행하기 위해 어떤 page가 live 한지 혹은 garbage 인지 판단할 수 있어야하는데, 간단하게는 mapping table 을 이용할 수 있다. mapping table에는 실제 physical page들이 명시되어있으므로 이들이 live한 page이다.이런 방식으로 위의 예제에서 logical block 100과 101에 각각 데이터 “c1”과 “c2”를 다시 쓰게되면 다음과 같이된다. 현재 mapping table을 보게되면 page 0과 1이 garbage인 것을 알 수 있다. 그러므로 block 0 에 있는 live한 page인 page 2와 3을 읽어 이들을 log에 쓴다. 즉 다음 free space에 쓰고 block 0을 erase 하여 다음 program operation에서 활용할 수 있도록 한다. 따라서 mapping table도 새로 update 되고 block 0 도 erase된 상태가 되었다.이처럼 GC는 live data를 판단하기 위한 read 그리고 이들을 copy 하여 rewrite, 만약 block이 전부 dead page들로 이루어져 있으면 erase 까지 진행하므로 비싼 작업이다. 그래서 현대 SSD에서는 추가적으로 flash 용량을 더 두어서 device가 바쁘지 않을때 background 에서 GC를 수행하도록 한다. flash 용량을 더 두게되면 data cleaning에 사용할 수 있고 storage bandwidth도 높일 수 있다. Mapping Table위에서 확인했듯이 Log-Structured 방식은 mapping table이 필요하다. 다만 이 mapping table 크기가 문제이다. 예를들어 4KB page로 구성되어있는 1TB의 SSD가 있다고 할때 mapping table entry가 4byte라면 mapping table의 크기는 1GB가 된다. 즉 mapping table 만을 위해서 1GB의 memory가 필요한 것이다.이를 극복하기 위한 Block-Based Mapping 방식과 최근 많은 SSD에서 채택하는 Hybrid Mapping 방식을 알아볼 것이다. Block-Based MappingBlock 기반 mapping은 page 별로 mapping을 하는게 아니라 block 단위로 mapping을 한다.이렇게 block 단위로 매핑을 하면 mapping table의 크기는 size of block / size of page 로 나눈 값으로 줄일 수 있다. 예를들어 block 1개에 4개의 page가 들어간다면 page-based mapping 방식에 비해 mapping table의 크기를 4분의 1로 줄일 수 있다. 간단하게 예시를 보며 이해해보자.위에서 본 예시와 조금 다르게 지금 우리가 logical block 2000, 2001, 2002, 2003 까지 각각 데이터 a, b, c, d를 write한 상태라고 하자.block 단위로 mapping을 하므로 block 번호의 맨 뒤 2bit만 offset으로 활용하고 그 앞부분은 block number로 활용할 수 있다. 마치 page table 에서 page number와 offset으로 물리메모리를 찾는 방식과 비슷하다. 따라서 2000, 2001, 2002, 2003 은 각각 offset 0, 1, 2, 3을 가지고 block number는 같다. 예시에서는 block 1개에 page가 4개 있고 위의 write을 physical page 4에 할당하였으므로 mapping table에 500 -&gt; 4로 기록한다.read 요청이 왔을때에는 logical block 번호를 4로 나눈 값으로 physical page number를 찾고 offset으로 page 순서를 계산할 수 있다. 하지만 Block-Based Mapping은 성능적으로 많이 좋지않다. 문제는 작은 단위의 write이 일어날때 발생한다. 작은 단위의 write이 발생해도 old block 전체를 읽어 새로운 block에 다시 써줘야한다. 이런 data 복사는 write amplification 으로 이어진다. 만약 위의 예제에서 logical block 2002를 다른값으로 수정하면 어떻게될까?FTL은 logical block 2000, 2001, 2003을 읽고 이들을 다른 block에 새로 써주어야 한다. 그리고 block 1은 erase될 수 있다. 이처럼 mapping table 크기관점에서는 훨씬 작은 크기를 가져갈 수 있기때문에 좋은 해결책일 수 있으나 성능이 매우 좋지않아 이 방식을 그대로는 활용하기 힘들다. Hybrid Mapping많은 현대 SSD 제품들은 Hybrid Mapping 방식을 사용한다. 이를 알아보자.Hybrid Mapping은 이름에서 유추할 수 있듯이 block-based mapping과 page-based mapping을 함께 사용한다.먼저 FTL은 몇개의 block들을 erased 상태로 남겨두고 모든 write를 이 block들에 쓴다. 이 block 들을 log blocks 라고 부른다. Hybrid Mapping 에서는 page에 대한 write을 log block의 아무 위치에 쓸수있도록 고안되었기 때문에 이 log block에 속한 block들에 한하여 page-based mapping table을 가진다. 따라서 FTL은 두종류의 mapping table을 사용한다. 먼저 log block 들을 대상으로 작은 크기의 page-based mapping table을 가지고 나머지 부분인 data table에 대해 block-based mapping table을 가진다. logical block에 대한 read 요청이 오면 먼저 log table을 확인하고 없다면 그때 data table을 확인한다. (mapping table을 확인한다는 것이다.) Hybrid Mapping에서 중요한 것은 log block을 작게 유지하는 것이다. 그러려면 주기적으로 log block을 검사해 이들을 data block으로 만들어 block-based mapping을 사용하도록 해야한다. 예제를 보며 이해해보자. FTL이 이미 logical page 1000, 1001, 1002, 1003을 write을 한 상태이고, 값을 각각 a, b, c, d 라고 하자. 이들은 physical block 2에 쓰였다. 그런데 파일시스템이 logical page 1000, 1001, 1002, 1003 순서대로 하나씩 수정했다고 해보자. 이때 program 할 수 있는 log block은 block 0 이라고 해보자. 그러면 다음과 같은 상태가 된다. log block에 대해서는 page-based mapping을 사용하는 것을 주목하라.운이 좋게도 이전에 block 2 에 쓰였던 순서와 동일하게 block 0 에 쓰여졌으므로 FTL은 switch merge라는 것을 진행할 수 있다. 여기서는 block 0 이 data block이 되고 block-based mapping 방식으로 변경될 수 있다. 그리고 block 2는 erase될 수 있다. 이는 FTL이 가질 수 있는 최상의 시나리오다. 결과는 다음과 같다. page-based mapping이 모두 block-based mapping으로 대체되었고 block 2는 erase 되어 log block으로 사용할 수 있게 되었다.만약 이와 같은 최상의 시나리오가 아니라 파일시스템이 logical page 1000, 1001 만 수정했다고 하면 어떻게 될까? 다음과 같은 상태가 될 것이다. 여기서 page 들을 합쳐 재구성하기위해서 FTL은 partial merge라는 것을 진행할 수 있다. physical block 2에서 logical block 1002, 1003 을 읽어 이 내용을 다시 log block에 append 한다. 그러면 결국 결과는 위의 switch merge와 같아진다. 다만 여기서는 추가적인 read, write가 필요하므로 write amplification이 증가할 수 있다. 만약에 최악의 상황에서 logical block 0, 4, 8, 12 가 physical block A 에 써있다고 하자. 그러면 이 log block 들을 data block로 변경하기 위해서는 logical block 1, 2, 3 이 써져있는 physical page를 찾아 이들을 읽고 새로운 block에 logical block 0, 1, 2, 3을 차례로 써주어야한다. 그다음 logical block 5, 6, 7 이 써져있는 page를 찾아 다시 새로운 block에 써준다. 그리고 이들을 data block으로 만든다. 이를 full merge 방식이라고 부르며, 이 full merge는 성능상 좋지않으므로 자주 실행되면 안된다. Wear LevelingFTL은 반드시 wear leveling을 구현해야 한다. FTL은 이를 위해 최대한 모든 block에 균등하게 write하려고 노력한다.앞서본 log-structured 방식은 구현자체가 write를 여러 block에 나누어할 수 있고 GC 또한 wear leveling에 도움이 된다. 하지만 몇몇 data block들은 오랜 수명을 가지고있어 수정되지 않고 계속 변하지 않고 그대로 저장되어 있을 수 있다. 이들은 정상적인 data로 GC 대상도 아니다.전체 block을 대상으로 write을 균일하게 써야하는 FTL 입장에서는 이런 long-lived data 가 문제일 수 있다. 이를 극복하기 위해 FTL은 주기적으로 모든 live data를 읽고 다른 block에 다시 write하는 작업을 진행한다.이는 write amplification을 증가시키고 성능저하를 일으킬 수 있지만 모든 block이 동일한 수명을 가지게 할 수 있다. 이 방법 말고도 많은 알고리즘이 이미 고안되어 있다. SSD ArchitectureSSD 전체구조를 그림으로 한번 보고가자. FTL layer를 보고 어떤 일들을 하고있고 이들을 왜 해야하는지 이제 이해를 할 수 있을것이다. PerformanceSSD는 하드디스크처럼 모터로 돌아가는 기게적인 부분이 없이 전자적으로 작동한다. 성능측면에서 SSD와 하드디스크를 비교를 해보자. Random I/O의 경우 하드디스크는 거의 1초에 몇백개의 byte만 처리할 수 있는반면 SSD는 훨씬많이 할 수 있다. SSD는 random I/O를 거의 몇십 몇백 MB/s 로 처리할 수 있으므로 아주 높은 성능을 가진다. Sequential I/O를 보면 SSD가 더 빠르긴 하지만 하드디스크도 괜찮은 선택일 수 있다. 하드디스크도 충분히 좋은 성능을 내고 있음을 볼 수 있다.조금 특이한건 SSD의 random write이 매우 좋은 성능을 가지고 있는데 이는 대부분의 SSD가 채택하고 있는 log-structured 구조 덕분이다. 이는 random write을 sequential write처럼 동작하도록 FTL이 처리하기 때문이다.또 SSD에서 random read 보다는 sequential read가 더 빠른걸 볼 수 있는데, 이는 sequential read는 read에 대한 operation을 큰 data 를 대상으로 한번에 받을 수 있기 때문이다. random read는 매번 각기 다른 주소에 대한 read 요청을 받아야 한다. 그러므로 처리량이 sequential read가 더 높을 수 밖에 없다. 경우에 따라 내부적으로 가진 cache의 영향도 있을 수 있다. SSD가 하드디스크에 비해 가격이 높으므로 사용하려는 시스템에 따라 높은 성능과 random read performace가 중요한 시스템은 SSD를, 엄청난 양의 데이터를 저장해야하는 data center를 구축해야 하는 경우는 하드디스크가 가성비 측면에서 좋은 선택이 될 수 있겠다. References https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4","link":"/2020/09/06/file-system-flash/"},{"title":"운영체제 1편 - 추상화","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 먼저 운영체제를 공부하기에 앞서 살짝 마음가짐을 다르게 해야한다.운영체제를 이해할때는 Abstraction, 즉 추상화를 이해하는게 가장 중요하다.운영체제에 대한 연구는 매일매일 이루어지고 실제 구현내용은 정말 자주 바뀐다. 그러므로 운영체제를 공부할때에는 실제 구현에 초점을 맞추기 보다는 Abstraction을 이해해보자. Memory를 abstract 해주는 것은 memory address 이고, 디스크를 abstract 해주는 것은 File 이다.그러면 Hardware를 Abstraction 해주는 것은 무엇일까? 바로 운영체제이다.운영체제는 하드웨어를 손쉽게 사용할 수 있는 Abstraction을 제공한다.운영체제를 공부할때에는 실제 구현에 초점을 맞추기 보다는 Abstraction을 이해하는게 가장 중요하다. 대표적인 Abstraction인 프로세스와 포트를 알아보자. ProcessProcess는 실행되고 있는 프로그램에 대한 Abstraction 이다.프로그램은 일련의 순차적으로 작성된 명령어들의 모음으로 disk와 같은 secondary storage에 저장되어 있다. 이들이 메모리에 올라오면서 실행이 되는 프로그램이 된다. 그러면 프로그램은 프로세스가 된다.메모리에 올라와서 동작을 하게되면 이를 프로세스라고 한다.프로세스의 메모리구조는 다음과 같다. 이후의 프로세스편에서 다룰 예정이라 메모리구조만 살짝 보기만 하고 넘어가자.물론 당연히 여기서의 메모리구조는 가상메모리를 얘기하는 것이다. 그러면 프로세스는 파일인가? 아니다. 하지만 이를 볼 수 있도록 구현해놓은게 있다./proc 을 보면 현재 실행되고있는 프로세스들을 파일로 볼 수 있다.프로세스가 뜨면 각각 프로세스들은 address space를 가진다.이는 다른 프로세스끼리 공유가 안되고 직접 접근할 수도 없다. 즉 A 프로세스에서 B 프로세스의 함수로 한번에 jump가 불가능하다는 것이다. Port우리가 흔히 말하는 포트도 Abstraction이다.포트는 프로세스가 세상을 바라보는 창이다.우리는 여러 프로그램을 동시에 켜고서는 네트워크를 통해 각 패킷을 주고받지만, 외부에서는 우리의 프로세스의 존재를 알지 못한다.외부에서는 우리의 port를 향해서 보낸다.포트는 컴퓨터간의 메시지를 주고받는 communication endpoint로 설명할 수 있다. 운영체제의 특징운영체제는 supervisor mode(kernel mode), user mode가 있다. 운영체제는 hardware 자원을 접근하고 제어한다. 그래서 운영체제는 supervisor mode에서 동작을 한다.supervisor mode는 cpu가 제공을 해준다. 운영체제가 시작을 하면서 supervisor mode로 switching을 한다.이와 반대로 다른 애플리케이션들은 user mode로 동작한다. device 들도 프로세스가 직접 제어를 못하고 운영체제를 통해서 제어가 가능하다. 운영체제의 Kernel 이라는 것은 운영체제의 핵심적인 부분으로서 자원할당, hardware에 대한 인터페이스 제공, 보안 등을 담당한다. Time sharing운영체제는 CPU의 실행시간을 time slice로 나누어서 실행한다. kernel이 프로세스를 돌리다가 context switching을 한다. 즉, 모든 프로세스는 time slice 동안 CPU를 점유하고 그 시간이 끝나면 CPU를 양보한다.Time slice는 옛날에는 10ms로 했었는데 이는 아주 옛날이야기이고 점점 timeslice값이 줄어들고 있다.단순하게 왜 time slice가 줄어들까? CPU가 점점 빨라지고 있기 때문이다.예전에 10ms로 정했던 이유는 task에 대해 대략적으로 휴리스틱하게 시간을 측정해서 10ms정도면 다 끝나겠다라고 생각해서 정했는데 cpu가 점점 빨라지면서 이 값이 같이 줄어드는게 당연하다.요즘에는 timeslice를 상수로 두지 않는다. 이를 가변으로 둔다. 이런 CPU의 time sharing으로 concurrent 하게 여러 프로세스들이 수행된다.싱글코어에서는 cpu에서 특정 시간의 그 순간에 수행되고 있는 프로세스는 1개이지만 밖에서 바라볼때에는 여러개를 동시에 수행하는 것처럼 보인다.concurrent라는 뜻은 물리적으로는 1개의 프로그램만 수행하더라도 다른 프로그램들도 같이 수행중인 것임을 의미한다.이와 반대되는 단어로 simutaniously 가 있는데 요즘에는 여러개의 코어로 컴퓨터가 구성되어있는데 실제로 여러 cpu에서 여러 프로그램이 동시에 수행되고 있음을 의미한다. Cpu mode위의 운영체제 특징에서 보았듯이 cpu에는 execution mode(실행모드)가 있다.user mode와 kernel mode가 있다. Kernel mode 모든 권한을 가진 execution mode Kernel이 실행되는 mode Privileged 명령어 실행 및 register 접근 가능 ex) I/O 장치 제어 명령어, memory management register CR3 User mode Kernel에 비해 낮은 권한의 execution mode User Application이 실행되는 mode Privileged 명령어 실행 불가능 kernel은 오직 kernel mode에서만 작동한다.user mode에서는 instruction이 수행할 수 있는 previledge가 달라진다.execution mode는 하드웨어가 지원해준다. 만약에 유저의 application이 device로 직접 접근하려고 하면 hardware로 막아버린다.kernel mode에서 쓸수있는 instruction을 user mode에서는 쓸수가 없다.예를들어 in, out이라는 instruction 이 있다. 이런 instruction은 usermode 에서 작동을 안한다. intel processor에는 wing이라는 mechanism을 지원해서 이 wing이 execution mode를 지원한다.wing 0 는 kernel mode이고 wing 3가 user mode이다.virtualization으로 가면 wing을 한가지 더 쓴다. 처음에 설계를 할때 wing1과 wing2는 미래를 위해 남겨놓았는데 여기서 쓴다.당연한 얘기지만 여기서 말하는 kernel mode는 sudo가 아니다. sudo 라는 것은 사용자 관점에서의 권한을 다루는 것이다.아무리 sudo라고 해서 kernel level에 접근할 수 없다. 그렇다면 user mode에서 kernel mode로 들어갈 일이 있을까? 그러면 뭔가 스위칭 해주는게 필요한가?여기서 시스템 콜이 필요하다.시스템콜을 통해서 유저가 커널로 파일을쓰거나 패킷을 읽을 수 있다.시스템 콜의 종류에는 open, write, msgsnd, shm 등이 있다. 시스템콜시스템콜은 기본적으로 사용자 프로그램에서 시스템콜을 호출하면 user mode에서 kernel mode로 스위칭이 된다.그러면 커널함수로 점프를 한다. 예로 open이라고 하면 open을 구현한 커널함수로 뛰어간다. system call에 파라미터를 받기도하는데 register를 통해서 전달한다. 애플리케이션과 커널 사이에는 시스템콜이라는 interface가 있는 것이다. 같은 개념으로 하드웨어와 커널사이에는 driver interface가 있다. 정리 운영체제는 하드웨어의 Abstraction 이다. 운영체제는 Time sharing을 지원한다. 운영체제는 Kernel mode에서 실행되며, 애플리케이션이 user mode에서 kernel mode로 가려면 systemcall이 필요하다.","link":"/2019/07/08/os-abstraction/"},{"title":"운영체제 3편 - 컴퓨터 구조와 I&#x2F;O(Interrupt &amp; Trap)","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번에는 운영체제를 이해하기 위한 컴퓨터 구조에 대한 내용이다.Bus, I/O 기초를 설명한다. Bus단일 Bus초창기에는 Bus가 1개였다. CPU와 Memory가 붙어있는 구조이다.이 방식은 CPU, Memory, I/O의 속도가 비슷했던 초창기에 사용했던 모델이다. 그런데 CPU, Device 들은 발전하는 속도가 각각 다르다.1995년도에는 CPU clock이 45MHz였다. 지금은 거의 3.5GHz 정도이다. 속도는 CPU &gt; Memory &gt; IO 속도 순으로 빠를 것이다.1GHz면 명령어 처리에 1 nano second 속도이다. 이와 비교하여 Disk 속도는 ms 단위이다. nano second와 milli second 차이는 100만배 차이이다. 고속도로에서 빠른차와 느린차와 같이 다닐 수 있을까? 불가능하다.이처럼 같은 버스에 연결된 device들 간의 속도차이로 인해 병목현상이 발생한다. 시스템속도는 결국 느린 시스템 속도로 결정이 되게 된다.위처럼 단일 Bus에서 CPU와 Disk를 붙여놓으면 CPU는 대부분 놀고있게 된다. 계층적 버스구조 이를 해결하기 위해 Bus를 나누게 된다.이를 계층적 버스구성이라고 하는데, 접근 빈도가 적고 처리속도가 느린 device들은 System Bus에 직접 연결하지 않고 I/O Bus를 거쳐 연결해서 stall 현상을 방지하는 것이다.위 그림을 보게되면 CPU는 Memory와 Memory Bus(System Bus)로 연결되어있고, 몇몇 device들은 General I/O Bus에 연결된다. 현대 시스템에서는 이 general I/O bus가 PCI가 될 것이다.높은 성능이 필요한 I/O device 들이나 graphic card를 이 general I/O bus인 PCI에 연결한다. 더 밑에는 Peripheral Bus가 위치하는데 여기에는 SCSI, SATA, USB 등이 속한다. 여기에는 disk, mouse, 키보드 같은 속도가 느린 device들이 연결된다.memory bus는 매우 높은 성능이 필요한데 이처럼 매우 성능이 높은 bus는 비용문제, 난이도 문제로 device를 plug할 수 있는 공간자체를 크게 설계할 수 없다. 따라서 시스템 디자이너들은 이런 계층적 버스구조를 선택했고 I/O Bus도 종류에 나누어 높은성능이 필요한 device들은 CPU에 가깝게 배치하였다.Peripheral bus는 조금 느린대신 많은 device들을 배치할 수 있다. Modern 시스템 구조 요즘에는 점진적으로 특수화한 chipset을 많이 사용하고 있고 성능향상을 위해 point-to-point interconnect를 많이 사용한다.위 그림은 나름 최신?인 2017년에 인텔에서 출시한 Z270 chipset의 대략적인 구조이다. CPU는 memory에 가장 가깝게 배치하고 높은 성능이 필요한 graphic card도 가깝게 배치한다.CPU는 I/O chip에 Intel이 만든 DMI(Direct Media Interface)를 통해 연결하고 나머지 device들은 이 I/O chip에 연결을 한다.I/O chip의 오른쪽에는 하드디스크들을 eSATA 인터페이스를 통해 연결하고, 그 밑에는 USB(Universal Serial Bus) 연결로 키보드나 마우스들을 연결할 수 있다.왼쪽에는 PCIe(Peripheral Component Interconnect Express)를 통해 더 높은 성능의 device들이 연결될 수 있다. 이 그림에서는 NIC(Network Interface Card)가 연결되었다.높은 성능이 필요한 NVMe 같은 스토리지 device들도 이곳에 연결되기도 한다. 이제 I/O 기초에 대해 알아보자. InterruptInterrupt는 비동기적인 이벤트를 처리하기 위한 기법이다. CPU는 외부에서 일어나는 이벤트를 모른다. 그래서 Hardware 적으로 알려주는게 Interrupt이다.예를들어 CPU는 10ms로 time slice를 한다고 할때, 10ms가 경과하는걸 어떻게 알 수 있을까? Timer가 interrupt를 날려준다.또 다른 예로 packet이 도착한다. 그러면 packet을 읽어줘야 하는데, 어떻게 읽을 수 있을까? 패킷이 도착하면 Interrupt를 발생시킨다. 그러면 현재 수행하고 있는 프로그램을 잠시 멈추고 ISR(Interrupt Service Routine)을 수행한다.ISR을 수행하고 다시 멈췄던 곳으로 돌아가 수행을 계속한다. Interrupt 처리순서 현재 state를 저장 ISR(Interrupt Service Routine)으로 점프 ISR 수행 저장한 state를 복원 Interrupt로 중단된 지점부터 다시 시작 ISR은 interrupt 종류마다 따로 존재한다. Disk에서 IO block을 읽으면 그에 대한 ISR로 jump 하고, 네트워크 패킷을 읽으면 그에 대한 ISR로 jump 한다.ISR로 jump를 한 후 전부 처리한 후 다시 멈췄던 곳으로 어떻게 다시 돌아가 프로그램 수행을 계속할 수 있을까?이를 위해 ISR로 jump 하기전에 context를 저장을 해야한다. context는 현재 실행 상태를 의미한다.CPU register를 저장해 현재의 state를 저장해야하며, Program Counter 즉 어디까지 수행하다가 멈췄는지를 저장해야한다. ISR 수행을 마치면 저장했던 state를 복구한 후 수행을 계속 이어나간다.인터럽트에는 우선순위가 있다. 이는 Hardware 장치별로 우선순위가 다르게 설정된다.그리고 ISR은 짧아야한다. 너무 길면 다른 Interrupt들이 제 시간에 처리되지 못할수 있다. ISR을 들어갈 때 interrupt를 disable 시킨다. 그렇지 않으면 하염없이 중간에 계속 interrupt가 중첩될 수 있다. Interrupt flow는 간단하게 다음 그림과 같다. TrapTrap은 Interrupt와 유사하다. Interrupt가 비동기적인 이벤트를 처리하기 위한 기법이였다면 Trap은 동기적인 이벤트를 처리하기 위한 기법이다. 동기적이라는 의미는 현재 수행하고 있는 프로그램에 의해 발생한다는 것이다.Trap은 kernel 안에 있는 Trap handler를 invoke 시키는 event라고 이해하면 쉽다.예를 들어서 divide by zero 하면 멈춘다. divide by zero하면 어떻게 이를 인지할 수 있을까? 바로 Trap 이다. 하드웨어가 divide by zero를 하면 스스로에게 trap을 건다.divide by zero 뿐만 아니라 잘못된 메모리 주소에 접근하려고 하면 segmentation fault가 발생하는데 이때 스스로에게 Trap을 건다.혹은 process가 특별한 instruction을 실행하면 Trap이 발생한다. 이 경우는 보통 system call을 처리하기 위한 Trap이라고 볼 수 있는데, system call은 call의 name과 arguments들을 register나 stack에 저장을 하고 user-initiated한 trap을 발생시킨다.그러면 Trap Handler인 Trap Service Routine을 실행하는데 내부적으로 trap의 타입을 보고 system call에 대한 요청일 경우 call의 name을 보고 그에 맞는 kernel procedure를 호출한다. 그리고 다시 원래의 프로그램 진행을 계속 한다.System call이 trap을 발생시키는 이유는 user mode에서 kernel mode로 변경해야하기 때문이다. Interrupt와 다른점은 interrupt는 발생하였을때 context를 전부 다 저장한 후 ISR로 넘어간다. 다만 trap은 따로 context를 저장할 필요가 없다.여기서의 context는 대표적인 예로 register가 있다. Interrupt는 발생했을때 CPU의 register state를 모두 저장하는데 trap은 이들을 저장할 필요가 없다. .그렇다고 모든 상태를 저장하지 않는다는 이야기는 아니고 trap는 현재의 Program Counter와 stack pointer 등을 하드웨어적으로 자동으로 저장한다. Trap은 무수히 많이 발생하므로 하드웨어적으로 자동으로 저장하도록 처리해놓아 직접 state를 저장할 필요 없고 가볍게 하드웨어적으로만 저장하기 때문에 state를 저장하지 않는다고 표현한다. (TA라는 instruction으로 현재의 stack pointer를 하드웨어적으로 저장하고 수행을 마치고 복구할때는 RTT라는 instruction으로 복구시킨다)사실 조금 더 정확히 말하면 위에서의 Program Counter는 정확히말하면 Kernel mode와 User mode가 각각 서로 따로 PC를 가지기 때문에 따로 저장하지 않는다고 한다. Interrupt Service Routine도 Trap도 모두 kernel mode에서 실행된다.Trap은 짧을 필요는 없다. 시스템 콜을 봐도 수행시간이 긴 시스템 콜도 존재한다. 시스템 콜을 할 때에는 중간에 interrupt를 허용하기도 한다. 간단하게 Trap flow는 다음과 같다. I/O Device운영체제의 관점에서는 I/O Device를 어떻게 바라볼까?일반적으로 하드웨어에는 장치를 제어하는 controller라는게 있다. 이 controller 안에는 대부분 크게 4가지 종류의 register를 가진다.Control(command) register, Status register, Data register(Input register, Output register) 이다.I/O 라는 것은 결국 CPU가 I/O Device의 register에 읽고쓰는 동작이라고 이해할 수 있다.Disk에 데이터를 쓴다는 것은 Disk의 control register에 쓰기 명령을 주는 것이다. 그리고 data register에는 어떤 내용을 write할 건지에 대한 정보를 준다.I/O하는 과정은 대략적으로 다음과 같다. Status Register가 BUSY 상태가 아닐때까지 기다린다. Data register에 값을 쓴다. Control Register에 command를 쓴다. 그러면 device가 command를 수행을 시작한다. Device 작업이 끝날때까지 기다려야 하므로 Status Register가 다시 BUSY 상태가 아닐때까지 기다린다. 작업이 끝났으면 Data register를 보면 그 결과에 대한 값이 올라와있다. Data Register는 Input, Output 용을 나누어 Input Register, Output Register로 나누기도 한다.이것이 운영체제가 바라보는 I/O Device model 이다. 여기서의 device register에는 어떻게 접근할까? 보통 이들의 register는 메모리 영역에 mapping을 해놓는다. Memory mapped I/O라고도 부르는데 나중에 더 자세히 다루겠지만 매핑된 영역의 주소로 memory read, write instruction만 수행으로 Device Register에 읽고 쓸수있다고 생각하면 쉽다. I/O 처리기법만약에 Device에 읽기 요청을 보냈다. 그런데 언제 이 읽기요청이 완료되었는지 모른다. 이를 어떻게 알 수 있을까?CPU는 크게 2가지 방법있다. Polling과 Interrupt이다. Polling위에서 예시로 본 I/O 과정은 CPU가 직접 Device의 Status Register가 준비된 상태인지 확인한다. 그리고 Device 작업이 끝났는지 확인하기 위해 계속해서 Device Status Register를 확인한다. 이 방식이 Polling 방식이다.Polling은 loop이나 time-delayed loop에서 특정 이벤트의 도착여부를 계속 확인하는 방식이다. Polling을 한 후에는 PIO(programmed I/O)라는 것을 수행한다. 이는 CPU가 직접 I/O를 한다. Disk를 read할 때 Disk로 부터 block이 도착했다면 이를 memory로 copy하는 것까지 CPU가 모두 처리해 주어야 한다. 그러면 CPU는 너무 할일이 많고 다른 process를 수행하는 시간도 줄어들게된다. Using Interrupt위의 Polling 방식에서의 CPU 낭비를 줄이기 위해서 Interrupt를 활용할 수 있다. Device를 polling 하는 방식이 아닌, I/O를 요청한 process를 sleep하게 하고 CPU는 context switching을 하여 다른 프로세스를 수행한다. Device가 operation 수행을 끝냈으면 hardware interrupt를 발생시키고 CPU는 ISR을 수행하면 해당 I/O를 요청한 프로세스를 다시 깨운다. 그리고 다시 스케줄링을 받아 수행을 이어가게 된다. 얼핏보면 위에서 본 Polling 방식보다는 Interrupt 방식이 훨씬 좋아보인다. 그럼에도 polling이 유리한 상황이 있다.Device에 굉장히 짧은 주기로 빠르게 operation을 해야하는 경우에는 polling이 유리하다.예를들어 화면을 보여주는 display device를 생각해보자.Frame buffer라는 곳이 있는데 이 frame buffer에 frame들을 쓰면 화면에 바로 찍히게 된다. Frame buffer에 계속해서 pixel들을 던지고 refresh 하고 이 일들을 계속 반복한다. 이렇게 빠르게 동작하는 장치는 interrupt로 처리하는 것보다 계속 polling을 하는 것이 효과적이다.이를 Interrupt 방식으로 구현한다면 전체 시스템이 굉장히 느려질 수 있다. 한번 Display Device에 쓸때마다 해당 프로세스는 sleep하고 Context Switching이 일어난다. 그러면 operation이 완료되면 hardware interrupt가 발생하고 해당 프로세스는 다시 ready queue에 들어가 스케줄링을 기다린다.하지만 1초에 60번씩 계속 Device에 써줘야 하는 상황이라면 과도한 Context Switching만 발생할 뿐이다. 이런 경우에는 polling이 유리하다. Interrupt handling과 Context Switching 대한 비용이 polling 비용을 넘어갈 수 있다.Polling 방식과 Interrupt를 합친 하이브리드 방식도 존재한다. DMA위에서 본 Programmed I/O 방식은 크기가 큰 data를 device에 전달하기 위해서는 CPU가 많은 작업을 수행해야 한다. 다음 그림은 이런 상황을 묘사한다. 프로세스 1 이 수행중이고 disk에 data를 쓰기를 원한다. Interrupt 방식을 사용한다면, 먼저 I/O를 하기위해 write하고 싶은 데이터를 메모리로부터 device로 전송해주어야 한다. 정확히는 data register에 이를 copy하여 써주어야한다. 이에 대한 수행시간을 c(copy)로 표현했다.Data copy가 완료되었으면 disk는 I/O operation을 수행하고 CPU는 Context Switching 하여 다른 프로세스 2 를 수행할 수 있다.이 상황에서 CPU는 데이터 전송에 너무 많은 시간을 할애하는 문제가 있다. 이를 해결하기 위해 CPU를 Device 데이터 이동에 사용하지 않고 I/O를 위한 별도의 장치를 사용할 수 있는데 그것이 DMA(Direct Memory Access)이다. DMA는 특수목적 프로세서이다. CPU가 DMA에게 I/O를 요청하면 DMA는 CPU를 대신하여 I/O장치와 메인 메모리 사이 데이터전송을 수행한다.CPU가 DMA에게 I/O를 위임한다. DMA가 대신 I/O를 수행해준다. CPU가 DMA에게 I/O를 요청하면, DMA는 CPU를 대신하여 I/O 장치와 메인 메모리 사이에 데이터 전송을 수행한다. CPU는 이 기간동안 다른 작업을 수행할 수 있다. CPU는 Device에 직접 memory로 부터 data를 copy할 필요없이 DMA에게 copy할 메모리의 주소와 얼마나 전송할지를 알려준다. DMA는 I/O를 대신 수행하고 완료되면 Interrupt를 발생시킨다. 위 그림에서 Data copy에 걸리는 작업은 DMA가 수행하는 것을 볼 수 있다. CPU는 I/O를 DMA에 요청하고 바로 다른 프로세스를 스케줄링하여 수행할 수 있다. DMA는 Memory Bus를 공유한다. CPU도 메모리에 접근하고 DMA도 메모리에 접근이 필요한데 메모리에 둘다 접근하려면 Bus를 통해 접근을 해야한다. 이 과정에서 둘이 충돌을 할 수 있는데 DMA는 CPU가 Bus 사용을 하지 않을 때 살짝살짝 사용한다. 이를 Bus stealing이라고 한다. 예제로 Disk에 Read하는 과정을 보자. Disk Read 과정 CPU는 DMA Controller를 초기화하고 전송모드를 DMA_MODE_READ로 설정한다. CPU는 DMA Controller에게 buffer(memory)의 주소(X)와 크기(C)를 알려준다. DMA Controller는 Disk controller에게 데이터를 전송한다. Disk controller는 매번 byte단위로 읽어오는 데이터를 DMA로 전송한다. DMA Controller는 받은 데이터를 주소 X의 buffer에 기록한다. 매 전송마다 C값을 감소시키고 C=0일때까지 전송받는다. C가 0이되면 전송이 완료된 것이므로 DMA Controller는 전송이 완료되었음을 interrupt를 통해 CPU에 알린다. 위 과정의 1번, 2번까지 하면 그 프로세스는 sleep 한다. I/O가 끝나야 이를 호출한 process는 다음 step으로 넘어가는데 이를 synchronous I/O model이라고 한다.다른 I/O model들도 있는데 이는 여기서는 범위가 넘어가므로 다루지 않을 예정이다. 보통은 DMA가 CPU와 병렬적으로 같이 작동할 수 있기때문에 high performance가 필요할 때에는 더 좋다고 할 수 있다. 다만 DMA라는 장치가 필요하므로 조금 더 cost가 있다고 할 수 있겠다. I/O 접근방법I/O device에 접근하는 방법은 크게 2가지가 있다. I/O instruction을 사용한다. CPU가 제공하는 instruction을 통해서 장치의 register를 읽고 씀으로서 device 장치와 통신한다. 예로는 intel의 I/O instruction인 in, out, ins, outs 등이 있다. 두번째로는 Memory Mapped I/O를 사용하는 것이다. 위에서도 살짝 언급했지만 Device 장치 register들을 memory 공간으로 mapping시키는 것이다. 그냥 load, store 명령어를 통해 장치의 register를 읽고 쓸 수 있다. 다만 instruction 관점에서 I/O instruction과 다른 점이 있는데, Memory Mapped I/O 에서는 control register에 값을 쓰는게 1개의 instruction이 된다. I/O instruction은 control register에 READ를 넣고 in register에 주소를 넣어주는 이런 일련의 동작들을 하나의 instruction으로 처리한다. Memory Mapped I/O는 Memory space와 I/O space를 구별하지 않는다. 최근의 Device들은 거의다 Memory Mapped I/O를 지원한다. 단, Memory Mapped I/O를 위해서는 IO MMU(Input Output Memory Management Unit)라는 걸 사용해서 MMU가 CPU에서 보는 virtual memory address를 physical memory address로 바꾸어주듯 Device 입장에서 보는 virtual memory address를 physical memory address로 변환해주는 장치가 필요하다. Device Driver각 device들은 서로다른 interface를 가지고있는데 어떻게 이들의 interface에 맞게 접근할 수 있을까?파일시스템을 예로들면 파일시스템은 SSD, USB, SCSI disk 등의 위에서 작동할 수 있다. 하지만 우리는 이 각각의 매체에 의존하지 않고 block read, write을 하고싶은데 어떻게 이를 가능하게 할까?이들위에 Abstraction layer을 둠으로써 가능하다. 위에 General한 Interface를 두고 이 Interface를 통해 접근하도록 한다.하지만 결국에는 OS row level 어딘가에 각 device에 specific한 interface에 접근하는 코드가 존재를 해야한다. 이것이 바로 Device Driver이다. 밑의 그림은 간단히 표현한 Linux Software 구조이다. 파일시스템은 단지 generic block layer를 바라보면서 block read, write 에 대한 요청만 수행하고 실제 disk 매체에 의존하지 않는다.Block layer가 요청을 보고 적절한 device driver에 라우팅을 해준다. 그림에서는 raw interface도 볼 수 있는데 이는 File Abstraction을 사용하지 않고 direct block read, write을 허용하게 해준다. 예를들어 파일조각모음 tool이나 file system checker 같은 소프트웨어들이 이 raw interface를 사용하여 구현한다.다만 Generic 한 Interface에만 의존하기 때문에 단점도 존재하는데, 예를들어 SCSI는 detail한 error log을 볼 수 있도록 해준다. 하지만 generic interface에는 generic IO Error만 받을 수 있기때문에 SCSI의 이런 이점을 가져갈 수 없다. Device driver들의 구현코드들이 존재해야 이런 device들에 접근할 수 있기때문에 Kernel은 많은 Device driver 코드들을 가지고있다. 실제로 Linux kernel의 70% 정도가 device driver 코드들로 이루어져 있다. IDE Disk Driver 예제실제 IDE Disk Driver의 예제를 가볍게 보자. 이를 보고나면 대략적으로 Device driver가 어떤 방식으로 이루어져 있는지 알 수 있을 것이다. 먼저 IDE Interface는 다음과 같다. 123456789101112131415161718192021222324252627[Control Register] Address 0x3F6 = 0x08 (0000 1RE0): R=reset, E=0 means &quot;enable interrupt&quot;[Command Block Registers] Address 0x1F0 = Data Port Address 0x1F1 = Error Address 0x1F2 = Sector Count Address 0x1F3 = LBA low byte Address 0x1F4 = LBA mid byte Address 0x1F5 = LBA hi byte Address 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive Address 0x1F7 = Command/status[Status Register (Address 0x1F7)] 7: BUSY, 6:READY, 5: FAULT, 4: SEEK, 3: DRQ, 2: CODE, 1: IDDEX, 0:ERROR[Error Register (Address 0x1F1)} (check when ERROR==1) 7: BBK, 6:UNC, 5: MC, 4: IDNF, 3:MCR, 2: ABRT, 1: TONF, 0:AMNF BBK = Bad Block UNC = Uncorrectable data error MC = Media Changed IDNF = ID mark Not Found MCR = Media Change Requested ABRT = Command aborted T0NF = Track 0 Not Found AMNF = Address Mark Not Found IDE Disk은 4개의 register를 제공하는데 control, command, status, error register 들이다.이 register들은 x86의 in, out instruction을 사용해 I/O address를 명시함으로서 읽거나 쓸 수 있다. 예를들어 Disk가 ready 상태인지 알기위해 위에 명시된 Status Register의 주소인 0x1F7을 읽어 READY 상태이면서 BUSY 가 아닌지 확인한다.Command Register에 Write parameter를 쓰기 위해서는 sector count, sector의 LBA(Logical Block Address), drive number 등을 register에 각각 작성한다. 그리고 I/O를 시작하려면 Command Register인 Address 0x1F7에 WRITE command를 쓴다.실제 데이터 전송을 위해서는 데이터를 전송해야하는데 데이터를 Data Port에 써준다.Interrupt는 각 sector가 전송되면 발생하게 할수도 있고, 전체 전송이 완료되면 발생하게 할수도 있다. 위에서 본 것은 IDE Disk의 Interface이고 이를 이용하는 Disk Driver를 작성해야한다.이는 실제 구현된 IDE Disk Driver를 보면 이해가 쉽다.IDE Disk Driver는 크게 4가지 주요한 함수가 있는데 각각 가볍게만 알아보도록 하자. ide_rw()맨처음 read, write을 하기 위해 호출하는 함수이다.다른 작업들이 있으면 요청을 queueing 하고 없으면 바로 다음함수인 ide_start_request()를 호출한다. 이를 요청한 프로세스는 여기서 sleep하도록 설정한다. ide_start_request()이는 read, write 요청을 disk에 전달하는 역할을 가진다. 여기서 device register에 in, out x86 instruction을 호출한다. 이 함수는 밑의 ide_wait_ready() 함수를 사용한다. 이는 요청을 device에 보내기 전에 ready 상태일때까지 기다린다. ide_wait_ready()ready 상태일때까지 busy waiting 한다. ide_intr()Interrupt가 발생했을때 수행되는 함수이다. 만약 read 에 대한 interrupt였다면 device로부터 data를 읽어들이고 해당 read 요청을 한 프로세스를 깨운다. 그리고 다른 작업들이 queueing 되어있으면 다시 ide_start_request()를 호출한다. 코드는 다음과 같다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static int ide_wait_ready() { // Ready 상태일때까지 Busy waiting while (((int r = inb(0x1f7)) &amp; IDE_BSY) || !(r &amp; IDE_DRDY));}static void ide_start_request(struct buf *b) { ide_wait_ready(); outb(0x3f6, 0); // Enable interrupt outb(0x1f2, 1); // Sector count 설정 outb(0x1f3, b-&gt;sector &amp; 0xff); // Logical block 설정 outb(0x1f4, (b-&gt;sector &gt;&gt; 8) &amp; 0xff); outb(0x1f5, (b-&gt;sector &gt;&gt; 16) &amp; 0xff); outb(0x1f6, 0xe0 | ((b-&gt;dev&amp;1)&lt;&lt;4) | ((b-&gt;sector&gt;&gt;24)&amp;0x0f)); if(b-&gt;flags &amp; B_DIRTY){ // This is write command outb(0x1f7, IDE_CMD_WRITE); outsl(0x1f0, b-&gt;data, 512/4); // Data transfer } else { // This is read command outb(0x1f7, IDE_CMD_READ); }void ide_rw(struct buf *b) { acquire(&amp;ide_lock); for (struct buf **pp = &amp;ide_queue; *pp; pp = &amp;(* pp)-&gt;qnext); // walk queue *pp = b; // Queue의 tail에 request 추가 if (ide_queue == b) // if q is empty ide_start_request(b); // send req to disk while ((b-&gt;flags &amp; (B_VALID|B_DIRTY)) != B_VALID) sleep(b, &amp;ide_lock); // Process sleep release(&amp;ide_lock);}void ide_intr() { struct buf *b; acquire(&amp;ide_lock); if (!(b-&gt;flags &amp; B_DIRTY) &amp;&amp; ide_wait_ready() &gt;= 0) // Read 요청이면 device register에서 data를 읽어온다. insl(0x1f0, b-&gt;data, 512/4); b-&gt;flags |= B_VALID; b-&gt;flags &amp;= ~B_DIRTY; wakeup(b); // 요청 process를 깨운다. if ((ide_queue = b-&gt;qnext) != 0) // 다음 작업이 있다면 계속한다. ide_start_request(ide_queue); release(&amp;ide_lock);} 번외 위의 I/O instruction과 개념은 비슷하게 최근에 나온 neural processor는 metric 연산을 하나의 instruction을 제공한다.기존의 metric 연산은 여러개의 instruction이 필요했는데 neural processor는 1개의 instruction으로 수행이 가능하다. Interrupt는 CPU clock 마다 작동할 수 있다. 다만 instruction이 수행중에는 interrupt가 발생할 수가 없다.Instruction은 hardware와 software의 경계라고 볼 수 있다. 만약 instruction이 8 cycle이 걸린다고 하면, 그 8 cycle이 끝나는 시점에 interrupt가 발생한다. 이 instruction 중간에는 interrupt가 발생할 수 없다. 정리 Interrupt는 비동기적인 이벤트를 처리하기 위한 기법이다. Trap은 잘못된 메모리주소 접근, system call 호출 등과 같은 동기적인 이벤트를 처리한다. I/O는 결국 Device register에 읽고 쓰는 것이다. DMA라는 하드웨어가 CPU대신 I/O 작업들을 처리해줄 수 있다. Memory Mapped I/O를 사용하면 Device 장치 register들을 메모리 접근 instruction으로 읽고 쓸 수 있다. Reference https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4 http://www.cs.unc.edu/~dewan/242/f96/notes/notes1/node13.html","link":"/2019/07/09/os-computer-architecture/"},{"title":"운영체제 4편 - 프로세스","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번 편은 프로세스에 대한 내용입니다. 프로그램과 프로세스프로그램은 일련의 순차적으로 작성된 명령어들의 모음으로 disk와 같은 secondary storage에 저장되어 있다. 이들이 메모리에 올라오면서 실행이 되는 프로그램이 된다. 그러면 프로그램은 프로세스가 된다.소스코드 단계부터 어떻게 이것이 프로세스가 되는지 단계적으로 살펴보자.먼저 소스코드는 compile을 통해 Object 파일로 변환된다. Object 파일은 machine이 이해할 수 있는 기계어로 구성이 된 파일인데 이 자체로는 수행이 되지 못한다. 프로세스로 수행을 하기 위해서는 여러가지 다른 정보들이 삽입되어야 한다. 그리고 Object 파일은 relative한 주소값들을 가지게 된다. 각 소스코드 파일은 각각 Object 파일로 변환된다.그리고 이후 linking을 통해 executable한 파일로 된다. 이때 relative 한 주소들이 absolute한 주소로 변경된다. Relative한 주소라는 것에 대해 조금 더 얘기해보면, 컴파일러는 소스코드를 파일 한개씩 보고 compile을 한다. 각 코드를 machine code로 바꾸어 주는 역할을 하고 address는 해결을 해주지 못하기 때문에 relative한 address를 사용한다. relative는 간단하게 이 파일 내부에서 몇번째 주소인지 정도를 의미한다. 이런 relative한 주소는 linking에서 해결을 하는데 linking 과정에서는 여러개의 object 파일들과 라이브러리들을 연결하여 메모리에 load될 수 있는 하나의 실행파일을 생성한다. Linking은 Address Resolution 과정이라고 볼 수 있다.여기서의 실행파일은 exe 파일 같은 executable 파일이며 특정한 환경(OS)에서 수행될 수 있는 파일을 뜻한다. executable 파일은 Process로의 변환을 위한 header, 작업 내용인 text, 필요한 데이터인 data를 포함한다. Object 파일은 여러개더라도 실행파일은 1개이다.Compiler와 Linker는 결과물이 수행될 OS와 CPU에 따라 다른 실행파일을 생성한다. LinkingLinking 과정을 조금 더 보겠다. Linking에는 static linking과 dynamic linking이 있다.static linking은 object file과 library들을 하나의 파일에 다 집어넣는다. 표준 C 라이브러리인 libc가 다 포함된다. 다만 이 방법은 너무 파일이 커지므로 dynamic linking이 대안될 수 있다. dynamic linking은 linking을 할때 각 라이브러리를 붙이지 않고 runtime에 붙인다.윈도우에서는 dll이라는 Dynamic Link Library가 존재한다. 라이브러리들이 메모리에 올라오면서 이들이 올라온 메모리의 주소를 찾아서 연결해준다. 이를 가능하게 해주는게 Runtime System 이다.Runtime System은 프로그래밍 언어마다 존재하는데 Java에는 JVM, javascript에는 NodeJS 등이 있다. C programming에서 초기 실행함수가 main인 이유가 Runtime System이 호출하는 함수가 main 함수이기 때문이다. Programming language는 각각 execution model 이라는 것을 정의하는데 process를 어떻게 메모리에서 배치할지, 어떻게 parameter passing을 할지를 정의한다.우리가 흔히 자주 사용하는 environment variable 들도 Shell script에서 자주 사용하는데 이들에 대해 shell의 Runtime System에서 linking을 해준다. Program to Process그러면 다시 돌아와서 프로그램을 어떻게 프로세스로 만들까?프로그램은 format이 있다. 기본적인 format은 맨 첫 부분에는 header가 들어간다. 그 다음에 text, 그 다음에는 data가 들어간다.text 영역에는 code가 배치되고, data는 우리가 만든 static variable, global variable 들이 들어간다.Executable을 정의하는 방식도 여러개 있는데 대표적으로 ELF(Executable and Linkable Format)과 COFF 등이 있다.이런 format을 따르는 executable 파일들이 disk에 저장되어 있다.이들을 실행시키면 프로세스가 되는데 text는 text segment로 data는 data segment로, uninitalize가 되어있는 global variable들은 bss(block start by symbol)로 올라간다.각 영역의 크기들은 이미 executable header에 명시되어 있기때문에 이 값을 보고 크기를 정할 수 있다. 우리가 소스코드에서 많이 선언하는 local variable은 모두 stack으로 간다. stack은 push, pop으로 동작하는데 local variable 선언으로 push를 하면 그 주소를 알 수 있다. 이 주소로 local variable에 접근한다. 우리가 쓰는 program들의 stack 크기가 얼마정도일까?Linux는 default 값으로 stack이 8196Kb 즉 8Mb로 설정이 되어있는데 보통의 프로세스에서는 사용량이 8Kb를 넘지 않는다. 위 그림에서 Heap 은 무엇일까?C programming에서 dynamic memory allocation(malloc, calloc)을 할때 메모리를 heap에 할당한다.이를 호출하면 heap에 할당된 주소를 받는다.위 프로세스 메모리구조를 기반으로 보면 실제로 처음 Process가 시작하면 stack pointer가 주소에서 가장 큰값을 가리키고 있다. Text segment에는 binary instruction들이 copy 된다.값이 정해진 global variable들은 data segment에 들어가서 process에 복사된다.uninitialized global variable들은 memory에 안잡히는 것이 아니라 process를 만들때 그만큼 bss영역에 memory가 잡히게 된다. 보통 heap을 이야기할때의 동적 메모리와 비교되는것은 array인데 이는 헷갈리기 쉽다. Array들은 어느 segment에 할당이 될까?Global array는 data segment에 잡히고, local array는 stack에 잡힌다. 당연한 이야기지만 array를 define하려면 크기를 미리 알아야 한다. 참고사항으로 자바에서의 array는 object 이므로 stack 영역에 할당되지 않는다. 이는 자바의 Runtime System이 정한 내용으로 array는 heap영역에 할당되고 그에 대한 reference를 stack에 가지는 방식이 될 것이다.(조금 더 정확하게는 자바의 Array는 primitive type 혹은 reference type들을 요소로 가지는 object들로 다른 object들과 같이 heap 영역에 할당된다. 하지만 Java 7에서 기본으로 탑재된 Escape Analysis로 object라고 해서 꼭 heap 영역에 할당되는 것은 아니다. Escape Analysis는 object의 scope에 관한 내용으로 만약에 object가 method scope에 있다면 JVM은 이 object가 다른 scope에서 사용되지 않는 다는 것을 알 수 있기때문에 Constant Folding 같은 최적화를 사용할 수 있고 object 자체를 Thread의 stack 영역에 할당할 수 있다.) Process프로세스는 크게 2가지를 하는 abstraction 이라고 볼 수 있다. scheduling의 단위이다. 다른 execution unit도 있는데 이는 Thread이다. protection domain을 제공해준다. 프로세스끼리 서로 접근을 못하게 해준다. Protection domain을 제공한다는 것은 다른 프로세스가 우리 프로세스의 heap, stack, data, text segment 등에 접근할 수 없게 보호해준다는 것이다.Kernal은 접근할 수 있다. Monolithic Kernel에서는 kernel과 process가 메모리상에서 같이 있기 때문이다. Process는 어떻게 구현이 될까? 크게 이 핵심적인 3가지로 구현이 된다고 볼 수 있다. program counter stack pointer data section 이라는 register Program counter는 text segment에서 현재 실행중인 instruction을 가르키고, data section은 data 영역중 어딘가를 가르킨다.프로세스가 execution 한다는 것은 PC가 움직이고 stack pointer가 움직인다는 것이다. Context SwitchProcess는 execution unit 이다. Process A가 실행되다가 Process B가 수행되게 바뀌는 것을 context switch(문맥전환)이라고 한다.그럼 Context Switching은 언제 일어나는가? Time quantum expires I/O 호출 위의 두가지 경우에 한하여 Context Switching이 일어난다고 볼 수 있다. Process old가 위의 조건중 하나에 해당한다면 old의 수행을 멈추고, Process new를 수행한다.Kernel에서 Context Switching을 담당하는 부분을 dispatcher라고 한다. Context Switch가 발생하면 먼저 context를 저장한다. PCB(Process Control Block)라는 자료구조가 등장하는데, PCB는 Kernel data structure이다. PCB 안에 Process를 나타내는 모든 정보(Process id, Program Counter, CPU register, CPU scheduling information, Memory management information, I/O status information 등)가 다있다.각 Process 들은 PCB로 표현할 수 있다. PCB의 구조는 위와 같다.Context Switching이 발생하였을 때에는 현재 process의 state, 즉 현재 프로세스의 PCB를 저장하고 다음 프로세스를 reload 한다. 그리고 다음 프로세스를 수행한다.CPU의 입장에서는 지금 어떤 것을 수행하는지 아무것도 모른채 instruction만 계속 수행한다. 프로세스가 I/O를 만나게 되면 system call로 trap이 발생하고 이 프로세스는 I/O가 끝날때까지 기다려야 하므로 sleep queue에 넣는다.System call 자체는 Context Switching을 일으키지 않는다. 단지 이는 user mode에서 kernel mode로 갔다가 돌아갈 뿐이다.Context Switching은 I/O 같은 요청이 와서 더이상 프로세스가 다음 instruction을 수행할 수 없을때 발생한다.일반적인 System call에서는 문맥교환이 발생하지 않는다. Computer Architecture in Context SwitchingContext Switching은 컴퓨터 구조의 영향을 많이 받는다. 먼저 CISC와 RISC를 알아보자. CISC 복잡한 명령어 set으로 구성하여 효율을 높였지만, clock 속도가 저하된다. 복잡한 회로로 물리적인 공간을 많이 차지하여 register 용량이 저하된다. 예로 Intel pentium processor가 있다. RISC 간단한 명령어 set으로 구성하여 clock 속도를 높여 빠른 수행속도를 가진다. 상대적으로 간단한 회로로 물리적인 공간에 보다 많은 register를 가진다. 예로는 ARM processor가 있다. RISC는 간단한 명령어 set으로 구성되어있어 똑같은 작업을 하는 프로그램도 크기가 CISC보다 더 늘어날 수 밖에 없다. 예를들어 CISC의 경우는 I/O 자체를 instruction으로 바로할 수 있다. Context Switching 입장에서는 RISC가 state가 더 많을 수 밖에 없기때문에 Context Switching을 할 때에 더 많은 것을 저장해야 하므로 register 내용변경에 큰 overhead가 생겼다. 그래서 Register window 같은 개념이 나오게 되었다. Process State프로세스는 실행을 하면서 여러 state를 가진다. 그전에 설명했던 context 의미를 가진 state와는 다른 state이다.new, ready, running, sleep, terminated 가 있다. 간단하게 보자. Process State 종류 new: 새로운 process가 만들어진 것이다. running: 현재 수행되고 있는 상태이다. sleep: I/O 같은 이벤트가 완료되기를 기다리고 있는 상태이다. ready: process가 processor로 dispatch가 되기를 기다리는 상태이다. terminated: 수행이 종료된 상태이다. 간단한 flow를 설명하면 new는 새로운 프로세스가 만들어 진 것이고, 이들이 ready queue로 들어간다.그러면 dispatcher가 이들중 하나를 선택한다. 여기서 어떤 대상을 선택할지 결정하는 것이 스케줄링이다.선택이 된 process는 running이 된다. 만약 여러개의 core가 있다면 running queue가 따로 존재한다. Time quantum을 다 소진해서 다시 ready 상태로 가거나 I/O가 발생하여 sleep을 하게된다.이 모든것을 관리해주는 것을 process management라고 한다.실제 Kernel 코드를 보면 sleep 자체도 여러가지 종류가 있다. Process creationProgram을 process로 만드려면 비용이 많이든다. 프로세스를 새로 만드는 것은 수행시간이 오래걸릴수 밖에 없는데 PCB같은 프로세스에 대한 Kernel data structure를 모두 만들어 주어야 하고 동적메모리도 전부 다 할당해야한다. Process creation의 생성시간은 최소 millisecond 단위이다.그래서 더 효율적으로 프로세스를 생성하기 위해 기존의 프로세스에서 다른 프로세스를 만들 수 있게 해준다. 이를 fork라고 한다. fork는 시스템 콜이다. 프로세스 생성 시간을 줄이기 위해 fork를 한다. fork는 기존 프로세스 자체를 복사를 해버린다. 메모리 관점에서는 User space에서의 메모리를 복사하는 것도 있고 이와 동시에 Kernel 영역에 있는 PCB 같은 kernel memory도 복사를 한다. 이런 복사 자체도 시간이 걸리지만 처음부터 process를 새로 전부 만드는 것보다는 훨씬 시간절약이 많이된다. (위의 program to process에서 설명하였듯이 process로 올리기위해 Data 들의 크기를 재서 할당하고 이들의 alignment를 맞추고 하는 것보다는 절약이 많이된다는 의미이다) Parent 프로세스가 fork를 호출하면 커널은 이를 그대로 복사해 child 프로세스를 만든다. 그러면 Child 프로세스는 기존의 Parent 프로세스와 동일한 메모리의 상태를 가지게 된다. 그러면 Child 프로세스는 exec이라는 시스템콜을 호출한다. 이 시스템콜은 새로운 프로그램을 load하도록 한다. 프로세스 메모리 구조에서 data segment와 text segment만 copy를 하면 된다. 그러면 Parent 프로세스와 Child 프로세스는 서로 다른 프로그램을 수행시킬 수 있다. stack 같은것은 따로 복사를 안하기 때문에 이전 프로세스의 스택의 값을 그대로 가지고 있다. vfork라는 것도 있는데 address space를 공유를 하다가 나중에 fork off를 한다.위에서 copy를 하게될때 parent 프로세스보다 child 프로세스가 더 text 영역이 크면 copy를 못하는 것이 아닌가 라고 생각을 할 수 있지만 처음 소개했던 process memory 구조는 logical 한 구조이다. 즉 나중에 더 보게되겠지만 virtual memory의 관점이기 때문에 이는 문제되지 않는다. 그러면 프로세스들이 fork를 통해 계속해서 생성된다면 맨처음에는 프로세스를 직접 전부 만들어줘야 하지 않을까?그 프로세스를 init 프로세스라고 부른다. 아래의 프로세스 Hierarchy에서도 init 프로세스를 찾아볼 수 있다. 이는 모든 프로세스들의 시초가 된다. init 프로세스를 만들고 이후로는 모두 프로세스들이 fork로 생성된다. init 프로세스를 kill하면 프로세스가 더이상 만들수없다. init 프로세스는 운영체제가 부팅을 할 때 만들어진다. 사실 init 프로세스를 fork한 shell 프로세스라는 것이 존재한다.init -&gt; shell(Command Line Interface, window) -&gt; User processShell 프로세스는 CLI나 window를 의미하는데 예를들어 CLI에서 우리는 프로그램을 실행시킴으로서 프로세스를 만들때 shell 프로세스가 새로운 프로세스를 만들어내게 된다. Child 프로세스가 terminate 할때에는 SIGCHLD라는 signal을 parent 프로세스에게 날린다. parent는 이를 받을수도 있고 안받을 수도 있다. c에서는 fork()함수를 통해 프로세스를 생성할 수 있다. fork()함수는 생성한 child 프로세스의 pid를 반환한다. 예제코드를 보자. 123456789101112131415161718192021222324252627282930313233#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;unistd.h&gt;int main(int argc, char* argv[]) { int counter = 0; pid_t pid; printf(&quot;Creating Child Process\\n&quot;); pid = fork(); if (pid &lt; 0) { // Error in fork fprintf(stderr, &quot;fork failed, errno: %d\\n&quot;, errno); exit(EXIT_FAILURE); } else if (pid &gt; 0) { // This is Parent Process int i; printf(&quot;Parents(%d) made Child(%d)\\n&quot;, getpid(), pid); for (i = 0; i &lt; 10; i++) { printf(&quot;Counter: %d\\n&quot;, counter++); } } else if (pid == 0) { // This is Child Process int i; printf(&quot;I am Child Process %d!\\n&quot;, getpid()); execl(&quot;/bin/ls&quot;, &quot;ls&quot;, &quot;-l&quot;, NULL); // Run 'ls -l' at /bin/ls for (i = 0; i &lt; 10; i++) { // Cannot be run printf(&quot;Counter: %d\\n&quot;, counter++); } } wait(NULL); // Wait for child termination return EXIT_SUCCESS;} 위의 코드에서 parent process면 fork()의 반환값으로 child process의 pid를 전달받고 child process이면 0을 반환받아 서로 다른 실행흐름을 가지게된다.Child 프로세스의 경우에는 execl 시스템 콜을 호출하여 첫 인자인 /bin/ls 의 프로그램을 자신의 메모리에 copy하기 때문에 그 다음의 for loop을 실행할 수 없다. Process Termination프로세스를 terminate할 때는 exit 시스템 콜을 호출한다. exit을 호출해야 커널이 프로세스 및 PCB등을 지운다.그러면 C언어에서 main 함수를 작성할 때 main 함수 내에서 직접 exit을 하지 않았는데에도 프로세스가 종료되었다. 이는 어떻게 설명할 수 있을까?Runtime System이 자동으로 exit을 호출해준 것이다. abort라는것으로 프로세스가 종료될 수 도 있는데 이는 비정상적으로 종료되었다는 것을 의미한다. 이는 signal도 보내고 core dump도 만들어준다.","link":"/2019/07/09/os-process/"},{"title":"운영체제 9편 - 메모리","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번 편은 메모리에 대한 내용입니다. 서론우리 핸드폰에서 가장 비싼 부품이 무엇일까? 첫번째는 CPU, 두번째로는 LCD 패널, 그 다음이 메모리다. CPU는 가장 비싸다. 이를 놀게만들수는 없다. 결국 범용 컴퓨터의 목적은 CPU 활용률(utilization)을 극대화 하는게 중요하다. 이전의 작성한 운영체제편들에서 사용자에게 빠른 응답을 제공하기위한 멀티프로그래밍도 알아보았고 이를 위해 스케줄링하는 것도 알게되었다. 여러 프로그램이 concurrent하게 돌아가다 보니 동기화 라는 것도 알게되었다. 하지만 여러 프로그램이 동시에 메모리에 적재되어 실행되면서 메모리를 공유해야할 필요성이 생겼다. 메모리가 아무리 커진다고 하더라도 메모리관리는 필요하다. 여러개의 프로세스가 동시에 뜰 수 있기 때문이다. 어떻게 이를 관리할 수 있을까? 주소공간(Address space)주소공간은 프로세스에서 참조할 수 있는 주소들의 범위이다. 주소공간은 프로세스와 1대1로 매핑되며 스레드들은 이를 공유한다. 주소공간의 크기는 CPU의 address bus에 의존한다. Address Bus에 대해 다룬 글이 있으니 참고하면 좋겠다. 예를들어 address bus가 32bit width이면 주소자체의 개수는 232개 있고 byte-addressable 이면 주소공간의 크기는 4GB가 되겠다. 만약 word-addressable이고 word가 16bit이면 주소공간의 크기는 8GB가 된다.byte-addressable 이라는 것은 최소로 읽어낼 수 있는 단위가 byte인 것이다. 즉 모든 byte 단위들로 주소를 가지고 있는 것이다.word-addressable이고 word가 16bit이라면 char 타입을 저장할때 나머지 8bit는 비어있게된다. 이런면에서 memory 효율화 차이가 있지만 이런 내용들은 컴파일러가 적절하게 최적화할 수 있다.요즘은 프로세서들은 byte-addressable 하지않다. 대부분 work-addressable 하므로 한번 읽으면 4byte를 한번에 읽는다. 또 메모리는 Memory alignment 라고하여, 메모리 주소로 접근할 때 word size에 대해서 align하는 것으로 제한한다. 즉 word size가 4byte이고 byte-addressable이라면 메모리 주소로 접근할때 반드시 4의 배수로 접근해야 한다는 것이다. 그렇지 않으면 trap이 발생한다. 물리주소와 가상주소물리주소라는 것은 메모리를 접근할 때 사용되는 주소다. Address Bus로 메모리에 주소가 들어갈때는 다 물리주소로 들어간다. 메모리 자체는 수동적인 device이다. 주소가 들어오면 그냥 주소를 읽어들이는 것 뿐이다.가상주소는 프로세스 관점에서 사용하는 주소이다. 우리가 이미 사용하고 있는 변수접근도 이미 가상주소를 사용하여 접근하고 있다. Logical한 주소공간이기 때문에 주소공간을 의미있는 단위로 나누어 사용할 수 있다.이전에 다루었던 내용에서 프로세스의 메모리구조를 보았을때 text segment, data segment, stack 등으로 공간을 나누었었다. 이들은 가상주소를 의미있는 단위로 나누어 사용한 것 뿐이다. Compile &amp; Link time에서의 주소Compile과 linking에서 다양한 논리주소들이 생성된다. Compiler는 symbol table을 만드는 것이 핵심이다. 예를 들면, 프로그래머들은 변수로 전부 이해할 수 있는 이름을 사용한다. int a, b에서 a, b 모두 symbol이다. 각 symbol이 가리키고 있는 메모리 주소를 map으로 가지고있는게 symbol table이다.다만 이 symbol table에서의 주소는 relative한 주소이다. 오직 Object file 안에서만 유효한 주소이다. 주소는 0부터 시작하고 이후 relocate할 수 있다. 상대적인 위치만 정해놓은 것이다.Linking을 하게되면 하나의 바이너리가 만들어진다. Linker가 하는것은 address resolution이다. 앞에서 만들어낸 relative한 주소를 relative하지 않게 만든다. 모든 Object 파일들과 라이브러리들을 묶어서 symbol table에 의존적이지 않은 주소를 만들어 낸다. 주소를 0부터 쫙 매긴다. 각각 들어가는 모듈들을 시작하는 주소에 상대적으로 다 더해줘서 주소를 모두 만들어낸다. 여기서 만들어지는게 가상주소이다. 이것이 프로그램이 사용하는 주소가 된다.프로그램의 수행을 위해 Loader는 executable을 메모리로 load 한다. 메모리에 올라가면 그때 또 프로그램이 가지고있는 주소와 물리주소간에 연결을 시켜주어야 한다. 이를 위해 binding을 한다. 프로그램이 가지고 있는 주소와 물리주소를 연결시킨다.Load가 다시 진행되면 똑같은 executable이 다른 물리주소로 가게될 수 있다. 그리고 런타임시에도 밑에서 자세히 보겠지만 swapping과 paging을 통해 물리메모리에서 내려갔다가 올라갈때 주소가 바뀔 수 있다. 이야기가 길어졌는데 결국 하고자 하는 이야기는 Compile time, Link time, load 과정에서의 주소와, runtime 할때의 주소가 따로 있다는 것이다. General purpose 한 운영체제에서는 보통 이런방법을 사용한다. 초창기 컴퓨터에서의 주소관리초기의 시스템에서는 아주 간단한 주소변환만 했다.주소에 base register에 있는 값을 더한다. 그리고 이 값을 바로 물리주소로 사용했다. base register를 프로그램마다 바꾸어 주는 방식이다. 즉, 각 시작번지를 바꾸는 방법이다. 이렇게 하는방법은 오버헤드가 적었다. 그냥 base register에 더하기만 하면 되었다. 다만 약점도 명확했는데, 프로세스가 반드시 continuous 해야 동작했다. 즉 한덩어리로 연결이 되어있어야 한다. linear 해야한다는 의미이다.이를 극복하기 위해 virtual memory개념을 만들고 이를 translation을 하는 방법을 생각해냈다. 지금은 전부 이런 memory translation 을 전제로 하고있다. 가상주소를 변환하고 그것을 가지고 물리메모리에 접근한다.리눅스 Kernel이 virtual translation과 physical memory 주소로의 변환을 모두 다룬다. 여기에 DMA가 붙으면 어디에 붙어야할까?DMA는 I/O를 대신해준다. I/O 데이터를 읽어온 것을 메모리에 써주고, 또는 I/O에 쓸 것을 메모리에서 가져와서 대신 write 해준다.DMA를 위 그림에 넣으려한다. DMA가 받아들이는 주소는 Virtual Address일까 Physical Address일까?보통은 DMA는 Physical Address를 받는다. DMA중에 Virtual 주소를 받는것도 있긴하다. 이를 DVMA라고 부른다. 다만 일반적인 General purpose PC에서의 DMA에서는 물리주소를 받는다. MMU(Memory Management Unit)위에서 virtual address를 physical address로 변환하기 위해서는 translation을 해야한다고 했다. 다만 이 translation 속도가 굉장히 중요하다. 잘못하면 memory 접근속도가 반으로 떨어질 수 있다. 이 성능을 높이기 위해서는 software로는 더이상은 한계가 있다는 것을 알고 하드웨어를 도입했다. 이것이 MMU(Memory Management Unit)이다.CPU는 그냥 주소를 MMU에 보낸다. MMU는 이를 physical address로 변환한다.MMU는 CPU 칩안에 붙어있으며, address bus에는 physical address가 실린다. 위의 구조에서 CPU 캐시는 어디에 위치할까? 캐시는 translation 하기 전에 작동한다. 그 이유는 캐시는 빨리 접근하는 것이 중요하다. 캐시 hit이 되면 이를 translate 할 필요가 없으므로 더 빠르게 값을 얻을 수 있다. 즉 캐시는 virtual address를 보고있다. 다만 virtual address는 그 유효범위가 프로세스에서만 유효하다. Context Switching이 일어나면 캐시를 전부 flush 시켜야 한다. virtual address 이기 때문에 서로 다른 프로세스가 virtual address가 같지만 physical address는 다를 수 있기 때문이다. 다만 또 어떤 디자인에서는 캐시를 physical address에 붙이는 것도 존재하여 항상 그런것은 아니다.이 CPU 캐시는 Kernel이 flush 해주어야 한다. 이를 flush할 수 있는 instruction이 존재한다. 다만 이 또한 연구가 계속 되고있으므로 언제든 변할 수 있다. 가상메모리(Virtual Memory)가상메모리는 실제로 존재하지 않지만 사용자에게 메모리로서 역할을 한다. 가상 메모리를 생각한 아이디어는 다음과 같다.물리메모리를 가지고 어떻게 이를 관리할 것인가에 대해 초점이 맞추는게 아닌, virtual memory가 있다고 가정을 하고 사용자들에게 그냥 virtual memory를 가지고 마음껏 사용할 수 있도록 하자는 것이 핵심이다.프로세스가 수행되기 위해서 프로그램의 모든 부분이 물리메모리에 있을 필요가 없다는 것이다. 결국 프로세스는 현재 실행하고있는 code/data/stack 부분만 물리메모리에 있으면 실행가능하다. Paging메모리 주소공간을 동일한 크기인 page 라는 단위로 나누어 관리한다. 이를 Paging 이라고 한다.보통 1 page의 크기를 4KB로 한다. 보통 Page Frame이라고 하면 물리메모리를 고정된 크기로 나누었을때 하나의 블록을 의미한다. Page 라고 하면 보통 가상메모리의 블록을 의미한다. Frame과 Page의 크기는 동일하다. 이 둘을 잘 구분해서 이해할 수 있도록 하자. Page가 하나의 Frame을 할당받으면 물리메모리에 위치하게 된다. Frame을 할당받지 못한 Page들은 외부 저장장치(Backing Store)에 저장된다. Backing Store도 Page, Frame과 같은 크기로 나누어져 있다. CPU가 관리하는 모든 주소는 두 부분으로 나뉜다. page 번호와 page offset이다.page 번호는 각 프로세스가 가진 페이지 각각에 앞에서 부터 부여된 번호이다. 예를들어 1번 프로세스는 0부터 63번까지의 페이지를 가지고 있다.Page offset은 각 페이지 안에서의 내부주소를 가리킨다. offset은 page가 4KB이니 0부터 4095까지 존재한다.Page 번호와 offset으로 모든 주소를 표현할 수 있다. 예를들어 1번 프로세스의 12번 페이지(page 번호)의 34번째(offset) 데이터로 표현할 수 있다. 잘 이해했는지 확인하기 위해 퀴즈를 풀어보자. 128MB의 물리메모리를 4KB 단위로 페이징하려면 몇개의 frame이 필요할까? 4GB의 logical address를 페이징하려고 하면 총 몇개의 page가 필요한가? (페이지 크기는 4KB) page 크기가 4KB일때, 한 페이지의 메모리를 access 하기 위한 주소 bit는 몇 bit인가? 해답은 다음과 같다. 227 / 212 = 215개 (약 32,000개) 232 / 212 = 220개 (약 1,000,000개) 4KB 크기이므로 12bit이다. 가상메모리 mapping 실제 물리 메모리에는 이렇게 그림처럼 올라간다. 어떤 조합으로 올라갈것인가는 운영체제의 몫이다.예를들면 위 그림에서 P3에 있는 page를 하나 더 물리메모리에 올리고싶다고 하자. 근데 현재는 물리메모리가 모두 사용중이다. 새로운 page를 올리기 위해 이미 올라가있던 Frame을 빼서 외부 저장장치에 임시로 저장해놓는다. 나중에 이 Page가 다시 필요하면 다시 가져와서 다시 메모리에 넣는다. 그리고 이를 반복한다. 서로 다른시간 t1, t2가 있다고 했을때 동일한 page frame에 대하여 t1과 t2에 특정 page frame에는 서로 다른 virtual page가 올라갈 수 있다. Page TablePage Table은 virtual memory와 physical memory 사이의 매핑이다. 각 프로세스의 페이지 정보를 저장한다. 그러므로 프로세스마다 하나의 page table을 가진다.테이블의 인덱스는 페이지 번호이다. table 내용으로는 물리 frame의 시작주소가 있다. 이 시작주소와 페이지 offset을 결합하여 원하는 데이터가 있는 물리 메모리 주소를 알 수 있다.page 크기가 4KB이면 offset이 12bit이므로 page(20 bit) + offset(12 bit)가 된다.그러므로 물리 frame 번호만 변환하고 뒤에 offset을 붙이면 물리메모리 주소를 알 수 있다. Page table은 Kernel이 관리하고 있는 data structure이다. Page table은 물리메모리 상에 저장이 되어있다. Page-table base register가 현재 수행중인 프로세스의 물리메모리 내 page table의 위치를 가리키고 있다. Context Switching이 일어나면 이 register의 값을 PCB에 저장한다. page table의 위치를 저장하는 것이다. 새로운 프로세스가 스케줄링되면 Kernel이 register의 값을 변경해주어야 한다. 결국 이를 위해서는 Kernel이 프로세스의 page table 위치를 알고있어야하는데, Page Table은 프로세스가 맨 처음 만들어질때 생성된다. 그림의 p는 페이지의 인덱스이며 page 번호이다. 이 page가 매핑된 값을 앞에 20bit에 넣고 offset은 그대로 뒤에 붙여준다. 그러면 정확한 물리메모리의 address가 나온다.이 작업을 MMU가 한다. MMU는 하드웨어이다. 왜 이걸 MMU가 할까?page table은 Kernel data structure로 memory에 있다. memory에 있다는 것은 page table에 접근할때 memory 접근 후, 이를 읽어낸 후 다시 이를 보고 physical 주소로 변환해야한다. 그러면 메모리 접근을 10번하려면 20번을 메모리 접근해야한다. 당연 성능이 좋지않다. 그래서 이를 하드웨어로 하자는 이야기로 MMU를 사용한다. MMU는 밑에서 다시 볼 것이다. Page Table Entrypage table 내부를 살펴보자. page table을 통해서 메모리 접근이 다 일어나므로 매우 중요하다. page 별로 메모리에 올리고 내리고 하는게 가능하다. 이를 위해 page table이 어떻게 구현되어있는지가 핵심적이다.Page Table Entry란 Page table에 있는 하나의 record-page를 의미한다. 간단하게 PTE라고 한다. PTE는 page에 대한 접근이 있었는지, 사용이 되는지의 여부, page가 바뀌었는지의 여부를 저장할 수 있다.PTE의 대략적인 필드내용은 다음과 같다. Page base Address해당 페이지에 할당된 프레임의 시작주소이다. 이를 통해 물리메모리에 접근할 수 있다. Flag bitsAccessed bit: 페이지에 대한 접근이 있었는지에 대한 bitDirty bit: 페이지 내용의 변경이 있었는지에 대한 bitPresent bit: 현재 페이지에 할당된 Frame이 있는지에 대한 bitRead/Write bit: 읽기/쓰기에 대한 권한표시 bit 실제로 프로세스가 작업하는 페이지는 물리메모리에 없을 수 있다. 이를 해결하기 위해 Present bit이 필요하다.Dirty bit은 밑에서 볼 내용이지만 page가 page-out 되고, 다시 frame을 할당받았을때 dirty bit가 마킹이 안되어있으면 이전과 같은내용으로 다시 page-out이 될 때 I/O로 다시 써줄 필요가 없으므로 성능상 이득을 볼 수 있다. Segmentaion fault의 발생은 언제 일어날까? 예를들어 *p = 1의 코드가 있는데 p가 NULL, 즉 0인 경우에 발생한다. 프로세스가 만들어 질때 첫번째 page는 읽어서도 안되고 써서도 안된다고 표시를 한다. 정확히는 fork 할때 page table도 copy를 하므로 똑같이 page 0번의 PTE에 Read/Write bit을 마킹을 해준다. 그래서 NULL pointer에 접근할때 0번의 PTE에 접근시 trap이 발생하여 segmentation fault가 발생한다. 이 Page table과 PTE를 보면 예전 프로세스에서 말했던 프로세스는 protection domain이라는 것을 이해할 수 있다. 프로세스 A는 프로세스 B의 page table에 접근할 수 없기 때문이다. 이는 MMU가 접근을 시켜주지 않는다. 다른 프로세스의 메모리에 접근할 수 없다는 것에 대해 이런 배경이 숨겨져 있다. TLBComputer Science의 magic 2가지가 있다. 하나는 caching이고 또 하나는 indirection이다.성능이 안나오면 caching을 하는 경우가 많다. 그리고 뭐든지 복잡하면 layering을 한다. 그러므로 항상 computer science를 공부할때에는 여기서는 어떤 indirection을 사용하고 있는지 보면 도움이 될때가 많다.캐싱을 사용하지 않으면 페이징 방법에서는 데이터로의 접근이 항상 두번의 메모리 접근을 거쳐야 한다. Page table에 한번 그리고 물리메모리에 한번이다. 이것이 메모리 접근 속도를 떨어뜨린다.이를 극복하기 위해 MMU도 caching을 사용한다. 이것이 TLB(Translation Look-aside Buffers)이다. Page table을 이용해 변환된 주소를 TLB에 저장해둔다. 그리고 다음 접근시에 TLB에 hit이 된다면 TLB에 저장되어있는 값을 이용하여 빠르게 변환된 주소를 얻는다. TLB는 register이기 때문에 빠른 수행이 가능하다. TLB hit ratio를 높여야 전체적인 메모리 접근속도를 높일 수 있다. miss가 나면 원래처럼 page table을 물리메모리에서 다시 조회해야한다. TLB entry는 보통 16개에서 512개 정도이다. 이미 우리가 사용하는 PC들은 모두 MMU가 붙어있다.TLB를 이용한 paging을 그림으로 나타내면 다음과 같다. MultiLevel Page TablePage Table의 크기는 얼마일까? PTE 하나는 보통 4 byte의 크기를 가진다. 그러므로 page table의 크기는 4MB이다. 크기가 작다고 생각할 수 있겠지만 프로세스가 100개라면 400MB가 된다.우리가 만든 프로그램이 있다고 해보자. 이 프로그램에서 사용하는 page의 개수는 몇개일까?segment별로 생각해보자. stack은 보통 프로그램에서 32KB를 넘어가지를 않는다. text segment는 코드가 1만 라인이라면 Page 10개를 넘지 못한다. text segment도 40KB를 넘기기 힘들다는 것이다. data segment도 보통은 작은 크기를 가진다. 결국 우리가 만든 프로그램은 PTE(page table entry)가 100개를 넘는 프로그램을 만들기 힘들다는 것이다. Page table의 PTE는 총 220개이다. 약 100만개이다. 그러면 보통 프로그램에서는 이 100만개의 PTE중에 100개를 사용한다는 것은 비율상 0.01%개의 PTE만 사용된다는 것이다. Kernel 메모리 낭비가 굉장히 심하다. 이를 어떻게 메모리 사용량을 줄일수 있을까 하여 Multi-Level Page Table이 나오게 되었다. Intel은 거의다 2-level page table을 사용한다. page table 자체도 paging된 공간에 저장된다. 2-Level Page TableOuter page table을 한개 더 두어서 이들이 page table들을 가르키도록 한다. 여기서 말한 outer page table은 level-1 page table이 된다.예를들어, 20-bit를 차지하고 있는 page number를 다시 아래와 같이 나눈다.10-bit page 번호 + 10-bit page 주소결국 32bit 주소의 모양은 다음과 같아진다. 그러면 level-1 page table에는 1024개의 entry가 들어간다. 이 각각의 entry가 level-2 page table을 가리키는 구조이다. 이 level-2 page table에도 1024개의 entry가 존재하게 된다. 결국 level-1 page table entry 중 1개가 mapping 하고 있는 virtual memory의 크기는 4MB이다.만약 page table entry의 4개만 사용하고있는 프로세스는 page table에 얼마의 메모리가 필요할까?level-1 page table은 4KB가 필요하고, level-2 page table은 3개를 사용하고 있다고 가정하자. 최대 4개가 사용될 수 있지만, 2개의 page는 동일한 level-2 page table 에 있다고 하자.그러면 총 4개의 page table이 사용되므로 16KB가 필요하다. Page WalkMMU가 page table에 접근하는 것을 Page Walk라는 표현을 사용한다(Table Walk라고도 부른다). 처음에 outer page table을 보고 그리고 level-2 page table을 보고 그 다음 물리메모리 주소를 찾는다. 다음은 2-Level page table 에서의 page walk 예시다. page walk는 모든 메모리에 접근할때마다 발생한다.만약 3-Level page table을 도입하면 어떻게될까? 이는 page size들을 더 줄일 수 있겠지만 그만큼 성능에 대한 tradeoff가 존재한다. page walk가 더 길어지기 때문이다.보통 시스템은 2-level 까지만 하는 경우가 많은데 이는 3-level 부터는 성능이 개선되는게 별로 크지 않기때문이라고 한다. Inverted Page64bit 주소공간을 가진 시스템에서는 multi-level paging을 위한 정보의 크기는 32bit에 비해 현격하게 증가되어 문제가 있었다. 그래서 새로 연구된 paging 방법이 있는데 이것이 이번에 소개할 Inverted Page이다. 다만 이 방법은 현재는 거의 사용되고 있지는 않으므로 참고하는 정도로만 보면 좋겠다. 64bit 주소공간에 모든 물리메모리가 매핑되어 있지는 않지만, 이의 반대로 모든 물리메모리는 가상메모리에 전부 매핑이 되어있을 확률이 높다는 것에서 출발한다.원래는 page table이 virtual page 번호가 들어오면 이에 매핑되는 physical page 번호를 반환하는 거였다면 이를 반대로 physical page 번호가 들어오면 virtual page 번호를 반환하자는 것이 아이디어다.결국 physical page 번호를 가지고 page table을 만드는 것이다.page table entry로는 process id와 virtual page 번호를 넣는다. 즉 pid와 virtual address의 조합으로 page id를 만든다. 이렇게하면 얻을 수 있는 장점이 시스템 전체에서 하나의 page table만 사용하면 된다.문제는 page table을 검사하는데 너무 오랜 시간이 걸렸다. 테이블 검색을 전체를 검색하지말고 Hash table을 사용하여 개선하기도 하지만 이또한 한계가 존재한다.그러하여 이런 방식보다는 virtual page를 찾고 TLB와 함께 사용하여 성능을 개선한 기존 방식을 많이 사용한다. Demand PagingDemand Paging은 프로세스의 실행을 위한 모든 page를 메모리에 올리지 않고, 필요한 page의 요청이 발생할 때 메모리에 올리는 paging 기법이다.PTE의 valid bit을 활용하여 한 프로세스에 필요한 page를 memory와 secondary storage 간에 이동을 시킨다. 이런 방법을 통해 물리메모리 구성의 시간이 줄어든다. 그리고 프로세스 전체 이미지를 메모리에 올리지 않기때문에 실제 필요한 물리메모리의 양을 줄일 수 있다. page table에서 참조하려는 page가 valid한 경우에는 이 page가 실제 물리메모리에 frame이 할당되어 있기 때문에 정상적인 참조가 가능하다. 다만 참조하려는 page가 invalid하다면 이 page가 실제 물리메모리에 존재하지 않으므로 이에 대한 처리가 필요한데 이것을 Page Fault라고 한다. Page Fault프로세스가 page를 참조하였을때 해당 page가 할당받은 frame이 없을경우 page fault가 발생한다. 그러면 page fault handler를 수행한다.page fault handler는 새로운 page frame을 할당받는다. 그리고 backing store에서 해당 page의 내용을 frame에 불러들인다. 그리고 page table을 재구성한 후 프로세스의 작업을 재개한다.다음 그림을 보자. 위 그림의 1번에서 페이지 참조를 하면 page fault가 일어난다. page fault는 trap이다.그러면 disk로 부터 해당 page를 읽고 이를 물리메모리에 할당한다. page table을 적절하게 마킹을 표시해주고 재개한다. 프로세스가 맨 처음 실행될때는 어떤 일이 일어날까?프로세스가 처음 실행을 시작하면 page fault가 일어난다. 처음에는 page가 물리메모리에 frame이 할당되어 있지 않기때문이다. 그래서 맨처음에 제일먼저 page fault가 일어난다.그러면 page fault handler가 disk에서 해당 page의 내용을 다시 frame에 불러오고 이를 재개한다. 만약에 우리가 local variable을 접근하고 있다고하자. local variable은 stack에 존재한다. 하지만 맨 처음에는 이 접근에 page fault가 발생한다. 그럼 이 또한 disk 에서 읽어와야할까?text segment와 data segment는 disk에서 읽어오는데 stack은 읽어올 내용이 없다. stack은 프로세스가 수행을 시작하고 프로세스가 끝나면 사라져야한다. 그러므로 stack 같은 경우에는 disk에 접근하는 것을 생략하고 frame만 할당한 후 page table을 업데이트하고 수행을 재개한다. 그러면 맨처음 page fault가 일어날때 어떻게 disk에서 이 위치를 알고 가져올 수 있을까?Loader가 프로그램을 시작할때 executable file을 읽고 disk에 address space를 적절하게 build 해준다. executable file 자체가 text segment와 data segment의 page file이 되도록 하는 시스템도 존재한다. page fault는 비싼작업이다. 일단 프로세스는 page fault가 발생하면 멈추어야한다. page fault는 trap으로 동작하고 할당가능한 frame을 찾고 필요하다면 disk에 접근하여 page file을 가져오고 다시 page table을 mapping 해주어야한다. frame을 찾는 것은 micro second 단위이지만 disk 접근은 milli second 단위이다. page fault가 최대한 발생하지 않도록 Kernel에서도 많은 노력을 하고있다. 프로세스의 실행시간중 page fault를 처리하는 시간이 execution보다 긴 상황을 Thrashing이 일어났다고 표현한다. 이런 Thrashing 현상을 줄이기 위해 여러 노력을 하고있는데 그중 하나가 working set model이 있다. Working Set메모리의 Locality 속성을 기반으로 프로세스가 일정시간동안 원활하게 수행되기 위해서 한꺼번에 메모리에 올라와 있어야 하는 page set을 working set이라고 한다. 특정 time window 동안에 접근된 page 들의 집합이 working set에 적용될 수 있다.Kernel은 프로세스의 working set을 계산하여 이를 프로세스에게 제공하여 page fault를 최소화한다. working set에 포함된 page 수는 시스템 전반에 걸쳐 사용가능한 총 page 수에 따라 증가되기도 감소하기도 한다. 또 working set을 프로그램이 시작하면 바로 할당하면서 맨처음 수행시 page fault가 나지않도록 text, data segment에 해당하는 frame을 미리 할당하는 전략도 존재한다. Page Replacement멀티 프로그래밍 시스템에서 user process가 증가하면 모든 user process가 사용하는 page 수보다 물리메모리의 frame이 부족한 상황이 발생할 수 있다. 이럴때에는 page 교체(replacement)를 진행해야 한다. page fault를 할 때 page replacement가 추가된다.물리메모리에 위치한 page를 disk에 저장한다. 그 frame에는 virtual page의 내용이 저장되어 있는데 이 값을 disk에 쓰고, 이 frame에 다른 virtual page를 불러와 교체한다. 이를 Page Replacement라고 한다.다음은 page replacement 과정을 간략하게 설명한 내용이다. Disk에서 요구된 page의 위치를 찾는다. 물리메모리에서 free frame을 찾는다. 만약 free frame이 있다면 이를 사용하고, 없다면 page replacement 알고리즘을 사용하여 교체할 frame(victim frame)을 선택한다. 교체할 frame을 disk에 저장하고 page table, frame table을 변경한다. 요구된 page를 free frame으로 읽어들이고 해당 page table, frame table을 적절하게 변경한다. user process를 재개한다. 비어있는 frame이 존재하지 않는다면 어떤 frame을 교체할지 적절하게 알고리즘으로 판단하여 victim을 정한다. 그리고 이 victim을 disk에 write하고 page table을 변경한다. 이때 invalid bit(present bit)를 설정한다. 이를 page out 되었다고 표현한다. user process가 page out된 page에 다시 접근하면 page fault가 발생한다. 이 말은 physical address binding이 runtime에 일어난다는 증거이다.Kernel은 page fault가 날때마다 page table 업데이트를 2번 해주어야한다. victim은 page out하고 이 victim process의 page table의 invalid bit 세팅하고, 그리고 page-in 된 프로세스의 page table도 수정해야한다. Page Replacement AlgorithmsPage Replacement가 필요할 때 어떻게 교체할 페이지를 고를까?이는 여러 알고리즘이 존재하는데 결국 핵심적인 내용은 이 알고리즘들은 모두 Page Replacement에 의한 I/O 작업 수행 횟수를 최대한 줄이려는 목적을 가지고 있으며, 적합한 알고리즘의 사용은 시스템의 성능을 크게 좌우하는 요소이다. I/O 작업은 매우 큰 비용을 지불해야 하기 때문이다. 여러 알고리즘이 존재하고 이들은 매우 간단하게 몇가지만 알아볼 것이다.먼저 가장 목표로 해야되는 것은 앞으로 가장 오랫동안 사용되지 않을 page를 교체해야한다. 그렇게 해야 가장 낮은 page fault 발생빈도를 가질 수 있다. 다만 미래에 어떤 page가 사용되지 않을지는 알수가 없다. FIFO 알고리즘그냥 먼저 frame이 할당된 page를 교체한다.가장 단순한 알고리즘이며 FIFO 큐를 이용해 구현가능하다. NRU 알고리즘NRU(Not Recently Used) 알고리즘은 이름 그대로 최근에 사용하지 않은 페이지를 교체하는 방식이다.각 page마다 reference bit와 modified bit를 둔다. 이 bit들의 설정은 MMU가 한다.reference bit는 최초로 frame에 로드되었을때 그리고 참조되었을때 bit를 1로 설정한다. 그리고 주기적으로 0으로 리셋한다.modified bit은 최초로 frame에 로드될때는 0으로 설정하고 이후에 page의 내용이 변경되었을때 1로 설정한다.그래서 page replacement가 필요한 시점에 다음 순서대로 rough 하게 교체대상을 찾는다. reference bit = 0, modified bit = 0 reference bit = 1, modified bit = 0 reference bit = 0, modified bit = 1 reference bit = 1, modified bit = 1 LRU 알고리즘LRU(Least Recently Used) 알고리즘은 가장 오랜시간 참조되지 않은 페이지를 교체하는 방식이다.이는 Page의 locality를 고려하고 가장 이상적인 알고리즘에 근접해있어 가장 많이 사용하는 방법이다.Counter를 사용하여 참조된 시간을 기록하는 방식으로 구현하거나, bit와 Queue를 사용하여 구현한다. SwappingPage Replacement, Page out은 4KB 짜리 page를 disk로 내보내는 것이다. 하지만 물리메모리가 부족한 상황에서는 page를 하나씩 내보내는 것으로는 부족하다. 이때에는 특정 프로세스 전체를 통째로 내려야한다. 이를 Swapping이라고 한다.Swap 대상이 된 프로세스 전체를 secondary storage로 보낸다.이렇게 page out이나 swapping에 사용되는 secondary storage의 영역을 swap 영역이라고 부른다.OS를 설치할때 맨처음 disk에 대해 swap 영역의 크기를 물어보는 부분이 나올때가 있다. 이 부분은 처음에 OS설치시 file system이 이 영역을 사용하지 않고 swap 영역으로 사용하도록 한다. 서버에서 swap이 발생했다면 이미 상황이 좋지 않은 것이다. 왜냐하면 swap이 발생하면 Thrashing이 발생할 수 밖에 없는데 CPU가 굉장히 바쁘다. 한 프로세스가 사용하는 모든 page들을 disk로 내리고, 다른 프로세스를 swap in 시켜주어야 하기 때문이다. 그러면 이를 방지하기 위해서는 어떻게 해야할까?현실적으로는 물리 메모리의 크기를 늘리거나 프로세스 숫자를 줄여야한다. 다음은 swapping의 swap-out과 swap-in 과정을 그림으로 표현한 것이다. Kernel MemoryKernel Memory 영역은 어떻게 잡히는 것일까? 이또한 위에서 살펴보았던 Virtual Memory와 연관이 있는것일까?시스템이 virtual memory를 사용한다면, Kernel 또한 virtual memory를 사용한다. Windows 같은 경우는 2GB이상을 사용한다. 이는 kernel code, kernel data, process 별 page table 등을 포함하고 이들또한 disk로 page-out 될 수 있다.즉 virtual memory의 특정영역이 Kernel에 사용되도록 되어있다.예를들어 32bit 주소공간을 가진 시스템에서 주소가 2GB 이상부터는 Kernel 영역이라고 정할 수 있다. 그 영역을 매핑하는 page table들은 각 PTE가 kernel page들을 가리키게 된다. 이 PTE들은 user page를 매핑할 수 없다. page table은 프로세스마다 존재한다. context switching이 일어나면 바라보는 page table 전체가 변경된다. 하지만 전통적으로 모든 프로세스들이 공유를 하고있는 영역도 존재를 한다.이런 영역같은 경우는 page table에 특별한 marking을 해놓아서 TLB에 캐시가 invalidate 되지 않도록 한다. 그리고 매우 critical 한 부분은 swap이 되지 않도록 설정하기도 한다. 다음의 virtual memory segment 그림을 보자. Kernel virtual memory 영역은 kernel code와 kernel data를 포함한다.그리고 kernel memory의 특정영역은 physical memory에 매핑이되어 모든 프로세스들이 이를 공유하기도 한다. 예를들어 모든 kernel code와 kernel data는 모든 프로세스들이 공유한다.리눅스 또한 특정영역의 인접한 virtual page들을 인접한 physical page로 매핑하기도 한다. 이는 커널이 physical memory에서 특정 영역을 접근하는데 편하게 해준다.예를들어 page table에 접근을 할때나 memory mapped I/O operation을 수행해야 할때 도움을 준다. Kernel virtual memory 영역에서 또다른 영역은 프로세스 별로 데이터를 가지고있는 영역도 있는데, 이 영역에는 그 프로세스의 page table이나 그 프로세스의 context에서 실행되는 kernel stack, 다양한 데이터 자료구조들이 존재한다. 기타사항그렇다면 Virtual Memory를 사용하지 않는 운영체제도 있을까?일부 임베디드 시스템들은 virtual meomory를 사용하지 않는 시스템이 있다. 예를들어 자동차 하드웨어의 일부는 virtual memory를 사용하지 않는데 MMU를 사용해야하므로 비용문제등으로 사용하지 않는 경우가 있다.","link":"/2019/08/08/os-memory/"},{"title":"운영체제 5편 - CPU 스케줄링","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번 편은 CPU 스케줄링에 대한 내용입니다. CPU SchedulingCPU 스케줄링은 어떻게 프로세스들에게 CPU의 사용을 분배할 것인가에 대한 내용이다. 메모리 내에 ready queue에 있는 실행이 준비된 상태인 프로세스들 가운데 하나를 선택하여 CPU를 할당한다.여기서 어떤 프로세스를 다음차례로 선택할 것인가에 대해 많은 알고리즘이 존재하는데 밑에서 살짝만 보고 넘어갈 것이다.결국 CPU의 idel 시간을 최소로하고 CPU를 최대한 효율적으로 사용하기 위해서 CPU 스케줄링이 존재하는 것이다. CPU 스케줄링의 결정은 다음 2가지 상태변화일때 일어난다. I/O로 인해 Process가 Running 상태에서 Waiting 상태로 가는경우 Time quantum을 전부 소진하여 Process가 Running 상태에서 Ready 상태로 가는경우 스케줄링은 크게 비선점형 스케줄링(Non-preemptive scheduling)과 선점형 스케줄링(Preemptive scheduling)으로 나뉜다. 비선점형 스케줄링OS가 강제로 프로세스의 CPU사용을 중단시키지 않는다. 프로세스 스스로 CPU 사용을 중단하거나 I/O를 하는 상황에서만 스케줄링 된다. 선점형 스케줄링OS가 현재 수행중인 프로세스를 강제로 중단시킨다. 보통 프로세스의 Time quantum을 다 소진한 상황에서 스케줄링을 한다. 각 스케줄링들의 특성을 비교하기 위해서는 기준이 필요하다. 여기서의 기준을 몇가지 소개하겠다. CPU Utilization전체 CPU 사용중에서 user process들이 작업을 처리하는 시간을 CPU Utilization이라고 한다. Throughput단위시간동안 처리하는 프로세스 개수이다. 즉 얼마나 많은 양을 처리할 수 있는가이다. Response Time하나의 프로세스 관점에서 입출력을 시작해서 첫 결과가 나오기까지 나오는 시간이다. Waiting Time프로세스가 얼만큼 queue에서 대기를 하고있느냐이다. Turnaround Time프로세스가 시작해서 끝날때까지의 시간이다. 만약 Dos(Denial Of Service)공격을 받으면 패킷도착에 대한 interrupt가 미친듯이 발생한다. 그러면 대부분의 CPU 시간은 Kernel에서 ISR을 처리하는데 보내게 된다. 그러면 CPU utilization이 극도로 낮아지게 된다.가장 이상적인 스케줄링은 CPU utilization이 높을수록 좋고, Throughput은 높을수록 좋고 Response Time은 낮을수록 좋다. 하지만 이런 이상적인 스케줄러를 설계하는 것은 현실상 불가능하다.그래서 각 컴퓨터의 사용종류에 따라 특정 기준을 극대화하는 방식을 사용하는데 예를들어 슈퍼컴퓨터의 경우에는 CPU utilization을 극대화 한다. 그리고 우리가 보통 사용하는 서버를 포함한 일반 컴퓨터는 Response Time을 줄이는 것에 초점을 두게된다. 일반적인 프로세스는 CPU Burst(CPU로 연산하는 시간)과 I/O Burst(I/O 처리를 위해 기다리는 시간)을 번갈아 가며 수행한다.이 두가지 특성에 따라 긴 CPU burst를 가진 프로세스를 CPU-bound 프로세스라고 하고 짧은 CPU burst를 가지고 I/O 시간이 많은 프로세스를 I/O-bound 프로세스라고 한다.어떤 종류의 프로세스들이 많은지에 따라 CPU 스케줄링 기법의 효율성이 달라진다. 선점형(Preemptive) 스케줄링선점형 스케줄링은 어떻게 구현할 수 있을까?어떻게 preemption이 일어날 수 있을까? user program을 돌리고 있을때 kernel이 다시 제어권을 얻어야지만 현재 수행중인 프로세스를 멈추고 스케줄링을 할 수 있을 것이다. 현재 수행중인 프로세스를 멈추고 kernel이 제어권을 가지고 와야한다.어떻게 하면 kernel이 제어권을 가져올 수 있을까? 두가지가 있다. 바로 interrupt와 시스템콜이다.timer interrupt가 발생하면 그에맞는 ISR이 수행되면서 수행되고 있는 프로세스를 멈추고 다음 프로세스로 스케줄링을 해줄 수 있을 것이다. 다만 timer interrupt가 발생한다고 해서 그때마다 스케줄링을 하는 것은 아니다. 커널에서는 interrupt를 받지않도록 구간을 코드로 설정할 수가 있다. 예전의 커널에서 멀티스레드를 지원하지는 않았을때에는 현재 수행중인 프로세스가 시스템 콜을 통해 Kernel에 들어가게 되면 그때에는 아예 interrupt를 disable 시켰다. 그러므로 Kernel에서 나와 user mode로 나오는 순간 그때 모두 처리되었다.지금은 이렇게 동작하지는 않고 preemption point라고 하여 나중에 동기화 쪽에서도 살펴보겠지만 kernel 에서는 많은 state를 관리하는데 동기화의 문제로 코드의 특정 구간에서는 scheduling이 되지 않도록 설정할 수 있다. 이 경우에도 interrupt 자체는 발생한다. 이것이 무슨말이냐면 해당 코드 구간에 들어갈때에는 preemptive disable flag를 ON 해놓고 들어가게되면, timer interrupt가 발생했을때 그에 맞는 ISR을 수행하게 되는데 이때 preemptive disable flag가 설정이 되어있다면 스케줄링 하지 않는다. CPU Scheduling 알고리즘여러가지 CPU Scheduling 알고리즘이 존재한다. 어떤 알고리즘들이 있는지만 간단하게 짚고 넘어가겠다. FCFS (First-Come First-Serve) SchedulingFCFS는 ready 큐에 있는 순서대로 CPU를 할당한다. FIFO 큐로 간단하게 구현이 가능하다. Shortest Job First SchedulingCPU Burst 시간이 가장 짧은 프로세스에게 CPU를 할당한다. 그러므로 최소의 평균대기시간을 제공한다.다만 프로세스들의 CPU Burst 시간을 미리 알 수 없기때문에 대략적으로 CPU Burst 시간이 짧은 프로세스를 선택하여 time quantum 만큼 수행하는 방식으로 구현하기도 한다. Priority Scheduling프로세스에 priority를 두고 priority에 따라 CPU를 할당한다. 예를들어 time quantum마다 현재 프로세스보다 높은 priority를 가진 프로세스를 찾아 다음에 수행하는 방식으로 구현할 수 있다.다만 priority가 낮은 프로세스들에게 기아현상(starvation)이 발생할 수 있고 이를 극복하기 위해 waiting 시간에 따라 프로세스의 priority를 높여주는 Aging 기법을 같이 사용하기도 한다. RoundRobin Scheduling선점형 스케줄링 방식으로 RoundRobin으로 동작한다. 보통 time quantum을 10ms 밑으로 두고 time quantum이 소진된 프로세스는 ready queue의 맨 끝에 들어가 다시 CPU 할당을 기다린다.이 방식은 Response Time이 짧은 이점이 있다. RoundRobin 방식과 context switching을 연결하여 조금 더 살펴보도록 하자.Time quantum이 소진되면 다음 ready queue의 헤드에 있는 프로세스를 다음에 실행을 시키게 될텐데 time quantum은 어떻게 측정할 수 있을까?Kernel은 time quantum을 하나하나 정확하게 측정해서 scheduling을 하지 않는다. 그리고 Linux 또한 고정된 Time quantum을 상수로 가지지 않고 매 프로세스마다 다르도록 varaible 한 값을 가지고 있다. timer interrupt가 발생하면 대략적으로만 시간을 approximate 하여 스케줄링 한다. RoundRobin 방식에서는 time quantum이 너무 작으면 너무 많은 Context switching이 일어날 수 있는데 이렇게 CPU는 바쁜데 user process는 CPU를 할당받지 못한 상황을 Thrashing 현상이라고 한다.나중에도 다루겠지만 Virtual Memory에서도 메모리가 너무 작아지면 Thrashing 현상이 일어날 수 있다. Multi-Level queue Scheduling큐를 한개만 두지말고 여러개를 두는 방식이다.예를들어, Foreground queue와 Background queue를 두고 Foreground queue는 roundRobin 방식으로 Background queue는 FCFS 스케줄링 방식으로 구현할 수 있겠다. 그래서 Realtime process는 foreground queue에 할당하고, batch job process는 background queue에 할당할 수 있다. Multi core Scheduling여러개의 코어를 사용하는 시스템의 경우 스케줄링은 더욱 복잡해진다.여러개의 코어를 사용하는 경우 ready queue를 어떻게 설계하는 것이 좋을까? global queue 한개를 두고 이 queue를 바라보고 모든 코어들에게 프로세스를 할당해주는 방식 각 코어마다 queue를 두고 프로세스를 할당해주는 방식 위의 두가지가 대표적인 설계방식일 수 있겠다.다만 전자의 global queue를 두는 방식은 scalable 하지 않아 사용하지 않는다. 왜냐하면 queue라는 것은 kernel의 메모리에 있는 data structure이다. 커널에서 한개의 큐를 바라보고 여러개의 코어에 할당하는 방식으로 구현하게 되면 동기화 이슈를 해결하기 힘들기 때문이다. 그리고 후자의 코어마다 queue를 두게되면 얻게되는 이점이 있다.각 코어마다 queue를 두게되면 프로세스는 계속 같은 코어에 할당이 되는데 CPU Cache의 hit rate를 높일 수 있기때문에 더 효율적으로 작동할 수 있다. Affinity 라고 특정 코어에 프로세스를 할당할 수 있는 방법이 있다. In current linux현재 Linux Kernel 에서는 2.6.23 버전부터 CFS(Completely Fair Scheduler) 라고 불리는 CPU scheduling 알고리즘을 기본값으로 사용한다.이는 각 task마다 고정된 CPU time을 가지는 것이 아닌, 각 task의 weight 정도에 따라 서로 다른 CPU time을 할당한다.","link":"/2019/07/23/os-scheduling/"},{"title":"운영체제 7편 - 동기화(Synchronization)","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번 편은 동기화(synchronization)에 대한 내용입니다. 동기화(Synchronization)공유하고 있는 데이터에 동시에 접근할 때 동기화를 적절하게 해주지 않으면 데이터에 대한 일관성(consistency)를 보장할 수 없다. 공유데이터에 대해 여러 프로세스가 동시에 접근하여 read, write를 함으로써 데이터의 일관성이 보장되지 않는 상황을 race condition이라고 한다.데이터의 일관성을 유지하기 위해 수행하는 프로세스들이 순차적으로 데이터를 접근하게 하면 일관성을 보장해줄 수 있는데 이를 동기화라고 한다. 한번에 하나의 프로세스만 그 값을 write할 수 있게 하는 것이다. Race condition 예시흔한 잔고 출금에 대한 예시를 들어보자.process A는 balance 변수를 읽고 balance = balance - 500을 하고 process B는 balance 변수를 읽고 balance = balance + 500을 하면 process A와 process B가 concurrent하게 수행이 될때 언제 context switching이 일어날지 모르기때문에 이들을 적절하게 동기화를 해주지않으면 결과를 정확이 보장할 수 없다. 이 말고도 우리가 평소에도 흔하게 사용하는 Thread 들에서도 data section을 서로 공유하기 때문에 race condition이 발생할 수 있고, Kernel 내부에서도 PCB(Process Control Block)이 공유되고 이 외에도 page table 같은 Kernel data structure 들에서도 race condition이 발생한다. Critical Section여러 프로세스들이 공유하는 데이터에 접근을 하는 코드의 영역을 critical section이라고 한다. 데이터 일관성을 해결하기 위해서는 오직 하나의 process만이 critical section에 접근할 수 있도록 해야한다. 동기화 문제를 critical section을 사용하여 모델링하면 다음과 같다. 1234entry section [critical section]exit section [remainder section] Critical section에 작성한 코드들이 제대로 동작되기 위해서는 오직 하나의 프로세스만 접근을 허용해야한다. 이를 해결하기 위한 알고리즘은 다음 3가지 조건을 만족해야 한다. Mutual Exclusionprocess A가 critical section에 진입해있다면, 다른 프로세스는 진입할 수 없어야 한다. Progresscritical section에 진입하려는 프로세스가 있다면, 그중 한개의 프로세스는 진입할 수 있어야 한다. 그리고 진입을 시도하는 다른 프로세스는 critical section에 진입하기 위해 영원히 대기할 수 없다. (즉, critical section 안에서 deadlock이 발생하면 안된다) Bounded Waiting어떤 프로세스가 critical section에 들어가기 위해서는 특정 시도횟수 안에 critical section에 진입할 수 있어야 한다. 즉 starve 상태인 프로세스가 없어야 한다. 현재 critical section에 진입해있는 프로세스를 제외하고 critical section에 진입을 원하는 다른 프로세스도 critical section에 진입할 수 있는 기회가 보장되어야 한다는 것이다. 동기화 알고리즘동기화를 해결하기 위한 알고리즘 몇가지를 살펴보자. 12345678910111213141516171819Shared Variable: /* * turn = 0 일때 critical section에 접근가능하다. */ int turn = 0;Process A: while (turn != 0); [critical section] turn = 1; [remainder section]Process B: while (turn != 1); [critical section] turn = 0; [remainder section] 위의 방식은 알고리즘 조건의 Progress와 Bounded waiting을 만족하지 못한다.Process B를 시작하지 않으면 Process A는 아예 critical section을 진입하지 못하므로 Progress 조건을 만족하지 못하고, 무한정 busy waiting을 하기때문에 Bounded waiting 조건도 만족하지 못한다. 이를 개선하기 위하여 두개의 flag 변수를 활용하는 방법이 있다. 123456789101112131415161718192021Shared Variable: /* * flag[0] = flag[1] = false 로 초기화 */ boolean flag[2];Process A: flag[0] = true; while (flag[1]); [critical section] flag[0] = false; [remainder section]Process B: flag[1] = true; while (flag[0]); [critical section] flag[1] = false; [remainder section] 이 방법도 mutual exclusion 조건은 만족하지만 그전에 보았던 방식과 동일하게 다른 2개의 조건은 만족하지 못한다. 경우에 따라 flag[0]과 flag[1]이 모두 true가 될 수 있기 때문이다. 이를 해결하기 위해 Peterson Solution이라고 불리는 방법이 있다.위와 동일한 방식에 turn이라는 변수를 한가지 추가한다. 123456789101112131415161718192021222324Shared Variable: /* * flag[0] = flag[1] = false 로 초기화 */ int turn = 0; boolean flag[2];Process A: flag[0] = true; turn = 1; while (flag[1] &amp;&amp; turn == 1); [critical section] flag[0] = false; [remainder section]Process B: flag[1] = true; turn = 0; while (flag[0] &amp;&amp; turn == 0); [critical section] flag[1] = false; [remainder section] 이 알고리즘은 flag 변수도 쓰고 turn 변수도 사용한다.알고리즘을 보면 flag[0]과 flag[1]이 둘다 true인 경우가 있을 수 있지만 turn 변수는 반드시 0 혹은 1 이여야 하므로 두개의 프로세스 모두 critical section에 들어가는 경우는 존재하지 않는다. 또한 두개의 프로세스 모두 critical section에 들어가지 못하므로 flag[0], flag[1] 모두 false인 경우도 발생하지 않는다.그러므로 알고리즘의 조건인 Mutual Exclusion, Progress, Bounded Waiting을 모두 만족한다.다만 Peterson Solution은 확장성에 한계가 존재한다.만약 2개의 프로세스가 아닌 3개 이상의 프로세스에 대해 지원하려면 매우 복잡해지고 구현하기가 쉽지 않다. 해결책초기 해결책kernel에서도 critical section에 대한 동기화가 필요했다. 초기에는 kernel mode로 들어갈때 kernel code 자체를 critical section으로 만들어 kernel mode 일때에는 아예 interrupt 자체를 막아버리는 방식으로 구현을 했었다. (현재는 이렇게 구현되어 있지 않다.) 그래서 kernel mode에서 빠져나와 user mode가 되는 순간 interrupt가 발생하는 방식이였다. interrupt가 disable 되어있으니 context switching은 발생하지 않는다. kernel 전체가 커다란 critical section으로 동작하는 것이다. 다만 kernel 전체가 프로세스가 많아질때 오직 한개의 프로세스만 kernel에 진입할 수 있었으므로 대기시간이 길었고, Kernel 레벨에서 multi-threading 을 지원하기 힘들었다. 하드웨어 해결책interrupt로 동기화를 해결하려다 보니 쉽지않아 hardware instruction으로 해결하자는 이야기가 등장했다. instruction으로 처리하면 알고리즘이 매우 간단하기 때문이다. 흐름은 다음과 같다. 1234acquire lock [critical section]release lock [remainder section] critical section에 진입하기 전에 lock을 잡고 lock 획득에 실패한 프로세스들은 모두 lock을 대기한다. 오직 한개의 프로세스만 lock을 잡고 critical section에 들어갈 수 있다. 이런 방식을 Mutex Lock 이라고도 부른다.lock은 동기화 instruction(Synchronization instruction)을 사용하여 구현할 수 있다. Synchronization instruction동기화 instruction은 hardware에서 제공해주는 instruction으로 CPU에서 원자적으로(atomically) 수행되는 것을 보장한다. instruction 사이에는 당연히 interrupt가 발생할 수 없다. 동기화 instruction의 예시들을 몇가지 살펴보자. testAndSettestAndSet의 semantics는 다음과 같다. 12345boolean testAndSet(boolean *target) { boolean rv = *target; *target = true; return rv;} 여기의 testAndSet의 semantics가 그대로 instruction으로 구현이 되어 있는 것이다. boolean 변수를 받아 true로 설정하고 그 전에 설정되어 있던 값을 반환한다. 다만 testAndSet은 느린 instruction이다. 보통 memory access에 대한 instruction은 8cycle 정도 걸리는데 testAndSet instruction은 최소 16 cycle은 걸린다. 다만 이 instruction이 수행되는 동안은 interrupt를 받지않는다. 이를 활용한 방식은 process가 여러개여도 동작한다. 이를 사용하여 동기화를 구현하면 다음과 같다. 12345678910Shared Variable: boolean lock = false;All Process:do { while (testAndSet(&amp;lock)); [critical section] lock = false; [remainder section]} testAndSet 수행결과가 false인 경우만 critical section에 진입할 수 있다. 그리고 instruction이 atomic 하므로 mutual exclusion을 만족한다.다만 위 예제처럼 실제로는 while loop을 통한 busy waiting으로는 잘 구현하지 않는다. testAndSet instruction을 수행할 때마다 memory access가 일어나는데 이를 busy waiting을 하니 Bus가 엄청난 트래픽을 받게되어 성능이 많이 떨어지게 된다. 뒤에 보겠지만 이를 극복하는 방법이 있다. SwapSwap이라는 동기화 instruction도 존재하는데 이 또한 testAndSet의 일종이라고 생각해도 된다. 다음과 같은 swap을 atomic하게 수행되도록 해주는 hardware instruction이다. 12345void swap(boolean *a, boolean *b) { boolean temp = *a; *a = *b; *b = temp;} ARM의 swap instruction 설명을 보자. SWP (Swap) and SWPB (Swap Byte) provide a method for software synchronization that does not require disabling interrupts. This is achieved by performing a special type of memory access, reading a value into a processor register and writing a value to a memory location as an atomic operation. 즉 interrupt disable을 따로 할 필요없이 instruction level에서 atomic 수행을 보장해준다. 이런 testAndSet 이나 swap 같은 동기화 instruction을 사용할 수 있도록 OS나 각 언어별 API에서 제공을 해준다. Synchronization instruction 한계이처럼 동기화 instruction을 사용하면 mutual exclusion은 해결할 수 있으나, bounded waiting 같은 조건은 software 프로그램에서 제공을 해야한다. 사용자 software에서 이런 부담을 지우기 위해 동기화 primitive를 따로 지원하는데 이렇게 해서 나온 것이 세마포어(semaphore)이다. SemaphoreSemaphore는 두개의 원자적 연산을 가지는 변수이다.여기서 말하는 원자적인 연산은 다음과 같다. wait() or P() signal() or V() Semaphore는 그냥 변수이다. critical section에 들어가기 전에 P를 수행하고 critical section에 나오면 V를 수행한다. Semaphore는 P를 통과하면 decrement하고 V를 통과하면 increment 한다.P와 V 연산은 서로 독립적이고, 원자적으로 수행된다. P, V 연산 모두 원자적으로 수행되는 것은 맞지만 P, V 모두 반드시 같은 프로세스에서 진행될 필요는 없다. Semaphore는 2가지로 보통 나누게 된다. Counting Semaphore와 Binary Semaphore이다. Counting SemaphoreSemaphore의 값은 한계가 따로 없으며, 초기 값은 가능한 자원의 수로 정해진다. Binary SemaphoreSemaphore가 가질 수 있는 값은 오직 0과 1이다. Original SemaphoreOriginal Semaphore는 busy waiting을 사용한다.Busy waiting을 이용한 방법은 다음과 같다. 1234567Semaphore as SP(S): while (S &lt;= 0); S = S - 1;V(S): S = S + 1; 위처럼 Busy waiting은 critical section에 진입할 조건이 될 때까지 loop을 돌며 기다린다. 그러므로 CPU cycle을 낭비할 수 있고, 대기중인 프로세스에서 어떤 프로세스가 critical section에 진입할지 알 수 없다. 어떻게 busy waiting을 하지 않을 수 있을까? 기다리고 있는 프로세스들은 sleep을 하도록 만들 수 있겠다. Semaphore with sleep queueBusy waiting 방식의 CPU cycle을 낭비하는 문제를 해결하기 위해 Semaphore의 자료구조에 sleep queue를 추가하여 대기중인 프로세스를 관리할 수 있겠다.다른 프로세스가 이미 critical section에 진입해있으면 진입을 시도하려는 프로세스를 sleep을 하고 sleep queue에 넣는다. 그리고 Semaphore의 값이 양수가 되어 critical section에 진입이 가능하게 된다면 sleep queue에서 대기중인 프로세스를 깨워 실행시킨다.Sleep을 한다는 것에 대해 조금 생각해 볼 필요가 있다. 예전에는 프로세스가 sleep을 하기 위해서는 I/O를 요청하는 경우였다. Time quantum을 전부 다 소진한 프로세스는 다시 ready queue로 들어가게 된다. 이제는 이런 경우가 아닌 새롭게 voluntary(자발적인) sleep의 개념이 생긴 것이다.스케줄링과 조금 엮어서 생각을 해보면, P를 통해 critical section에 진입을 하지 못한 프로세스들은 sleep을 하게되는데 V 호출이 일어나게 되면 sleep 하고 있는 프로세스들에게 critical section에 다시 진입하도록 깨워주어야한다. 실제 구현체에서는 V 호출이 일어나면 현재 semaphore로 인해 sleep 하고있는 모든 프로세스들을 다 깨운다. 그리고 다음 critical section에 대한 진입은 scheduler에게 맡긴다. Mutex vs SemaphoreSemaphore는 서로 다른 프로세스들에서 P와 V를 각각 호출할 수 있다. 즉 process A에서는 P를 호출하고 process B에서는 V를 호출할 수 있다.이러한 이유로 Semaphore에는 여러가지 발생할 수 있는 문제들이 존재하는데 예를들어 P를 호출하지 않았는데 V를 호출한다던가(Accidental Release), P를 호출한 프로세스가 다시 P를 호출하는 재귀적 deadlock(Recursive Deadlock) 등이 있다.다만 Mutex의 경우는 lock을 획득한 주체만이 unlock할 수 있다. 만약 lock을 획득한 프로세스가 아니라 다른 프로세스에서 unlock을 시도하게되면 unlock이 불가능하다. 이것이 Semaphore와의 가장 큰 차이인데 이를 The principle of ownership이라고 한다. Mutex에서는 본인이 lock을 들고있어야 하므로 위에서 설명한 Accidental Release가 발생할 수 없고 Recursive Deadlock도 쉽게 해결할 수 있다. Mutex는 concurrent하게 실행되는 code에 대한 protecting에 초점을 맞춘다면, Semaphore는 한개의 스레드가 다른 스레드에게 signal을 보내는 의미가 강하다고 생각할 수 있다. Deadlock in SemaphoreSemaphore에 Deadlock을 해결하기 위한 간단한 방법이 있다.Semaphore간의 partial order를 정해놓고 그 order에 맞춰서 semaphore를 잡으면 deadlock을 막을 수 있다. 여러개의 프로세스들에서 Semaphore의 P를 잡고 들어가고 V를 호출하는 순서들을 정확히 맞추는 방법이다. 애초에 이런 문제를 해결하기 위해 맨처음 진입시 전체적인 global state를 관리할 수 있는 큰 semaphore 1개를 잡고 할 수 있지않느냐 라고 질문을 할 수 있다. 이러면 deadlock은 발생하지 않는다. 다만 concurrency를 높게 살리기 힘들다. Monitor위에서 살펴보았듯이 Semaphore에는 발생할 수 있는 여러가지 문제점이 있다.(Accidental Release, Recursive Deadlock)그래서 이를 조금 더 high level에서 해결하려는 요구를 하게되었다. High-level 언어에서 procedure를 호출하는 것만으로도 동기화를 해결할 수 있도록 하는 것이다.그래서 Application level에서는 P, V 연산을 호출할 필요없이 지원하는 procedure만 호출하도록 한다. 이를 Monitor라고 한다.예시로는 자바에는 synchronized keyword를 지원한다. synchronized block을 지정하여 동기화되는 영역을 지정할 수 있다. Monitor in Java내부적으로 Entry queue를 만들고 synchronized block에는 한번에 한개의 스레드만 진입할 수 있도록 한다. Reference ARM Synchronization Primitives Development Article Semaphores The Mutex","link":"/2019/07/24/os-synchronization/"},{"title":"운영체제 2편 - 가상화","text":"이 글은 학부 운영체제 수업을 듣고 정리한 글입니다.맥락없이 운영체제에 대한 내용들이 불쑥 등장하니 양해부탁드립니다. 이번에는 Kernel Architecture에 대한 내용입니다.크게 Monolithic kernel, Micro kernel, Hypervisor를 알아봅시다. Monolithic KernelMonolithic Kernel은 우리가 주로 사용하는 운영체제들에서 볼 수 있는 방식이라고 생각하면 편하다. Monolithic Kernel에서는 사용자와 Kernel이 같은 주소공간에 위치한다. 즉 주소공간을 Kernel과 사용자가 나누어서 사용하는 것이다.Monolithic Kernel에서는 kernel 코드가 큰 한덩어리로 구성된다. 흔히 말하는 Monolithic Server Architecture를 생각하면된다. (반대로 흔히 MSA라고 부르는 Micro Server Architecture가 있다) Monolithic Kernel의 장점은 유저의 Application과 커널이 같은 주소공간에 위치하기 때문에 SystemCall 및 커널 서비스 간의 데이터 전달시 오버헤드가 적다. SystemCall 호출시간도 짧은 편이다.다만 단점도 존재하는데 모든 서비스 모듈이 하나의 바이너리로 이루어져 있기 때문에 커널 개발자로서 일부분의 수정이 전체에 영향을 미친다. 또한 커널의 크기가 커질수록 유지보수가 어려워 진다.이에 대한 대항마로 Micro Kernel이 있다. Micro KernelMicro kernel은 kernel을 쪼개기 시작한다. 커널이 한개가 아니고 여러개로 독립된 주소공간을 갖는다.Micro kernel은 kernel 서비스를 각 기능에 따라 모듈화 하여 각각 독립된 주소공간에서 실행한다고 보면 된다.각각의 모듈화한 모듈을 서버라고 부르며 이 서버들은 다른 process로서 존재한다.여러개의 kernel들의 address space가 다 다르기때문에 function call을 할 수 없다.그래서 message를 보내야 한다. Micro kernel 에서는 IPC(서버들간의 통신)라든지, RPC(Remote Procedure Call) 이런 공통기능만 제공한다.File write이 필요하면 “File을 써주세요”라고 message passing을 한다. 그러면 message passing으로 VFS를 호출하고 다시 message passing으로 결과를 반환하는 구조이다. Micro Kernel의 장점은 각 커널 서비스가 따로 구현되어있어 독립적인 개발이 가능하며, 커널 유지보수가 상대적으로 용이하다는 장점이 있다.다만 성능적으로 Monolithic Kernel 보다 낮은 점이 단점이다. 다음은 block I/O에 대해 Monolithic Kernel과 Micro Kernel의 처리를 비교한 그림이다. 가상화이제는 가상화라는 것이 많이 보편화되었다.기본적으로 가상화는 Monolithic vs Micro kernel과는 조금 다른 관점이다.어떤 kernel이든 예전에는 OS와 하드웨어는 1대1로 매핑되어 존재했다.하나의 하드웨어 위에 OS가 여러개 있으면 뭐가 문제인데? 라는 질문을 할 수 있다. 이에 대한 질문으로 가상화가 시작된다.가상화의 시작은 Virtual Machine부터 시작했다. 하나의 하드웨어 위에 여러개의 OS를 올릴 필요가 있었다. 여러개의 OS를 올려야 하니까 OS의 OS가 필요하다. 이름하여 Hypervisor이다.다만 성능이 문제였다. 성능이 매우 좋지않아 사용하지 못하는 수준이였다.하지만 이후에 Xen이라는 논문이 발표되었는데, 논문에서 소개한 방식으로 구현을 하게 되면 machine에 OS를 1개만 올리는 것과 성능이 거의 동일하다는 내용이였다. 약 3% 정도의 성능하락만 있다.이때부터 많은 것이 바뀌었고, Cloud라는 새로운 industry가 급성장하기 시작했다고 한다. 위 그림의 Virtualization Layer가 Hypervisor 이다.옛날에는 OS마다 하드웨어를 분리해서 동작시켜야 했다. 이런 일련의 모든 작업들을 프로비저닝(provisioning)이라고 한다.예전에는 모든 OS에 대해 프로비저닝을 해야 했다.그러나 가상화는 하드웨어를 다 합친다음에 그 위에 여러개의 OS를 돌릴 수 있다.즉 하드웨어를 합쳐서 프로비저닝을 쉽게할 수 있다. 자동차 하나에 CPU가 몇개가 있을까? 자동차에 CPU가 최소 100개가 넘게 들어간다. 만약 이들을 10개로 줄이려는 노력을 한다고 가정해보자.그러러면 어떻게 해야할까? Micro CPU들을 조금 더 강력한 CPU들로 대체해야한다.그런데 그냥 막 대체할 수가 없다. 왜냐하면 기존의 코드수정이 필요할 수 있기 때문이다. 하지만 코드수정하기가 쉽지 않은 상황이라면 어떻게 해야할까?여기서 가상화를 사용하면 쉽게 문제를 해결할 가능성이 높다. 100개의 Application들을 10개의 cpu 위에서 돌리면된다.가상화는 software와 hardware의 결합도를 낮추도록 도와준다. 이것 말고도 isolation 관점에서의 장점도 있다. Guest OS들은 서로 독립되어 있다. Guest OS를 2개 돌렸을때 한개가 죽었을때 다른 한개에 영향을 주지 않는다.즉, Hardware의 abstract를 guest os가 하는게 아니라 하이퍼바이저가 한다.각각의 guest OS는 독립적인 virtual machine으로 작동한다. HypervisorHypervisor는 쉽게말해 Guest OS와 Hardware 사이에 위치하며 virtualize된 컴퓨터 Hardware 자원을 제공하기 위한 관리계층이다.Hypervisor는 hardware의 자원을 분배하는 역할을 한다. 이는 기존의 os가 하던 일이다. Guest OS는 Hypervisor가 제공하는 가상화된 Hardware 자원을 이용한다. 간단하게 Hypervisor의 장단점을 보자. Hypervisor 장점 하나의 Physical Machine에서 여러 종류의 Guest OS가 운용이 가능하다. 실제 컴퓨터가 제공하는 것과 다른 형태의 명령어 집합구조(Instruction Set Architecture)를 제공할 수 있다. 다른 Hardware 환경으로 compile된 Guest OS 및 Application도 구동이 가능하다는 것이다. Hypervisor 단점 Hardware를 직접 사용하는 OS에 비해 성능이 떨어질 수 밖에 없다. 이 문제는 밑에서 설명할 반가상화(Para-virtualization)으로 성능저하 문제를 해결한다. Hypervisor가 device를 emulation 해준다고 볼수있다. Guest OS의 역할은 결국 hardware를 제어하는 것이다.결국 guest OS는 hypervisor위에 있다는 것을 모른다.실제 하드웨어는 intel 칩인데 application은 ARM일 수 있다. 가상화 환경에서 돌리려다 보니까 instruction이 다르다. 그러나 이것도 돌려줄수 있다. Hypervisor가 emulation으로 ARM instruction을 흉내를 낸다. 가상화 자체는 좋은 아이디어지만 성능때문에 처음에는 쓸 수 없다고 생각했다. 그러나 Para virtualization(반가상화)라는 것이 나오면서 생각이 바뀌게 된다.반가상화는 Guest OS를 본인이 Hypervisor 위에서 돌아가는 OS라는 것을 알게한다. 이것이 가상화 성능을 확 끌어올렸다.이를 Hypervisor-aware 라고 한다. OS가 Hypervisor 위에 올라가는 걸 알게하려면 Kernel 코드를 뜯어 고쳐야했고 쉽지않았다.결국 Para-virtualization은 Hypervisor와 협력을 해서 성능을 끌어올린 것이다.반가상화는 가상화를 안한것과 성능이 거의 비슷하도록 발전했다. 하드웨어에서도 virtualization에 대한 큰 발전이 있었는데, Memory Management도 MMU라는 하드웨어가 CPU안에 들어가면서 Memory management가 확 발전하기 시작한 것처럼, intel에서도 가상화 자체를 CPU가 지원하면서 가상화가 확 발전하기 시작했다. 현재 자주 사용하는 경량화된 Hypervisor로서 KVM과 Container가 있다.KVM 이라는 것은 리눅스에 포함된 hypervisor이다. KVM에서는 Xen의 Para-virtualization을 제거하고 intel에서 제공하는 가상화(VT-x) 하드웨어를 사용하여 가속할 수 있다. 더 최근에는 오히려 더 잘알려진 개념인 Container를 많이 쓴다.Container는 OS를 우리가 올리지 말고 경량화해서 사용하자는 개념이다. Container는 Guest OS가 없다.Container는 root file system을 공유하고 라이브러리들을 공유한다. 그러므로 KVM보다 가볍다. Hypervisor에는 Type 1과 Type 2가 있다.지금까지 설명한 것은 Type 1 Hypervisor이고 Bare-metal Hypervisor라고도 한다.Host OS위에 Guest OS가 올라가있는 것을 Type 2 hypervisor라고 한다.성능은 당연히 Type 1이 빠르다. 밑의 그림은 Type 1과 Type 2의 비교이다. 정리 Monolithic Kernel의 단점을 보완하기 위한 Micro Kernel도 있다. Hypervisor의 존재로 한개의 Physical Machine 위에 여러개의 OS를 구동할 수 있었다. 요즘은 intel 가상화의 도움을 받을 수 있는 KVM이나 Container 기반의 기술들을 주로 사용한다. Reference https://www.redhat.com/ko/topics/virtualization/what-is-KVM https://www.techtarget.com/searchitoperations/tip/Whats-the-difference-between-Type-1-vs-Type-2-hypervisor","link":"/2019/07/09/os-virtualization/"},{"title":"파일시스템 2편 - RAID","text":"이 글은 학부 System Programming 수업을 듣고 다른 자료들과 함께 공부한 내용을 정리한 글입니다.이전 포스트의 하드디스크에 이어 이번 포스트에서는 RAID에 대해 알아볼 것입니다. 이번 편은 이전의 운영체제 3편 - 컴퓨터 구조와 I/O 글의 I/O 부분을 이해하고보면 도움이 됩니다. What is RAID?RAID는 Redundant Array of Inexpensive Disks의 약자이다. 즉 여러개의 disk를 사용해서 더 빠르고, 더 크고, 더 신뢰성 있는 disk 시스템을 구축하는 방식이다.외부적으로 보는 관점에서는 RAID는 그냥 disk와 동일하다. 외부에서는 그저 큰 disk처럼 바라볼 뿐이다. disk와 동일하게 block의 묶음을 읽고 쓸수있다. 하지만 RAID 내부적으로는 시스템을 관리하는 프로세서도 존재하고 메모리도 존재하며 무엇보다 여러개의 disk로 구성되어 있다. RAID는 왜 쓰는 것일까?RAID를 사용하면 먼저 성능상으로 이득을 볼 수 있다. 여러개의 disk에 병렬로 I/O를 할 수 있다. 그리고 용량(capacity)측면에서도 이득을 볼 수 있다. 한개의 disk 크기를 넘어서는 크기를 저장할 수 있기 때문이다.그리고 신뢰성(reliability)도 더 높은데 RAID를 구성하는 방식에 따라 다를 수 있지만 데이터를 여러 disk에 중복해서 저장함으로서 single disk failure에 대해 극복할 수 있고 외부에서는 single disk failure가 일어나도 아무 일도 일어나지 않은 것처럼 수행할 수 있다. RAID Internals파일시스템 밑에서 RAID는 그저 매우 크고 신뢰성있고 빠른 disk 처럼 보일뿐이다. single disk처럼 linear array of blocks로 보이고 각 block은 파일시스템에 의해 읽고 쓰여질 수 있다.파일시스템이 RAID에 logical block에 대해 I/O 요청을 하면 RAID는 내부적으로 여러 disk를 가지고 있는데 어떤 disk에 접근해서 physical I/O를 할 지 결정하고, 수행한 후 그 결과를 반환한다. 이 physical I/O를 어떻게 수행할 것인가는 밑에서 볼 RAID level에 따라 다르다.간단하게 RAID가 각 disk의 copy본을 들고있는 mirrored RAID 라고 해보자. 그러면 write을 수행할 때 각 block들을 disk에 2번씩 써주어야 한다. RAID EvaluationRAID를 구성하는 방법은 여러가지가 있다. 구성하는 방법에 따라 특징 그리고 장단점이 존재하는데 이런 특징을 수치로 측정할 수 있으로면 비교하기 쉽다. 그래서 우리는 3가지 측면에서 RAID의 특징을 측정할 것이다. capacity먼저 capacity이다. B개의 block을 저장할 수 있는 N개의 disk가 있다. 클라이언트는 RAID에 얼마나 많은 용량을 저장할 수 있을까?중복없이 저장한다면 N * B가 되겠다. 각 block마다 copy본을 둔다면 N * B / 2가 되겠다. reliability두번째는 reliability 즉 신뢰성이다. RAID가 몇개의 disk failure 까지 허용할 수 있을까에 대해 이야기한다. performance마지막은 performance이다. Performance는 측정하기 조금 힘들 수 있는데 RAID에 요청되는 작업의 종류에 의존하는 경우가 많기 때문이다.RAID performance를 측정할때는 크게 2가지 측면을 고려할 것인데 첫번째는 single-request latency이다. RAID에서 single I/O 요청에 대해 어떤 latency를 가지는지를 측명하면 얼마나 해당 RAID가 병렬성을 가지고 있는지 그대로 이해할 수 있다.두번째는 steady-state throughput 이다. 어떤 규칙적인 요청이 왔을때 그때의 처리량을 의미한다. 예를들어 여러개의 요청이 동시에 왔을때 그때의 RAID 전체의 bandwidth를 측정할 수 있겠다.이들을 조금 더 자세히 측정하기 위해 각 요청에 대한 내용들이 두가지 종류가 있다고 가정한다. sequential과 random이다.sequential workload는 요청들이 큰 단위의 연속된 block을 읽는 요청들로 이루어져 있다고 가정한다. 이런 sequential workload는 큰 파일을 읽어 특정 키워드를 찾고싶을때처럼 자주 오는 요청들이다.random workload는 각 요청은 매우 작은 크기의 block을 읽는 요청들이고 각 요청들은 서로 다른 disk location을 대상으로 한다. 예를들어 처음 4KB에 대해 logical address 10에 접근하고 그다음은 logical address 55,000 그 다음은 logical address 4,500 에 접근하는 방식이다. 이런 random workload는 database에서 빈번하게 일어난다. 위의 sequential, random workload는 disk의 특성으로 인해 각각 다른 performance 특징을 가진다.sequential access에서는 disk는 가장 효율적인 방식으로 동작한다. seek time과 rotational delay에 매우 적은 시간을 사용하므로 대부분의 시간을 data transfer에 활용할 수 있다.다만 random access의 경우는 대부분의 시간을 seek time과 rotational delay에 사용하므로 상대적으로 data transfer에는 작은 시간을 할애할 수밖에 없다.그래서 이런 차이점을 더 명확히 확인하기 위해 sequential workload 에서는 S MB/s 로 data transfer가 가능하다고 하고, random workload 에서는 R MB/s 속도로 data transfer가 가능하다고 하자. 보통은 S가 R보다 훨씬 크다. 이 측정치를 밑에서 RAID level 별로 계산해보며 성능을 비교해볼 것이다. 밑에서는 몇가지 RAID 종류들을 보게될 것이다. RAID Level 0(striping), RAID Level 1(leveling), RAID Levels 4, 5(parity-based redundancy)이다. RAID Level 0: StripingRAID 0은 striping으로 더 잘 알려져 있다. 이 방식은 capacity와 performance 측면에서 높은 결과를 낸다.striping은 말그대로 줄무늬 방식이라고 이해해도 좋다. striping 방식에서는 각 block들을 여러 disk에 걸쳐 줄무늬처럼 배열한다. 다음 그림을 보자. 여기서는 4개의 disk를 사용했다. striping의 기본 아이디어는 block의 array를 라운드로빈 방식으로 disk에 하나씩 할당한다. 이 방식은 large sequential read 요청에 대해 병렬로 처리할 수 있도록 설계한 방식이다.위의 예에서는 1개의 block(4KB) 기준으로 라운드로빈으로 disk에 할당하였는데 다음과 같이 할당할수도 있다. 여기서는 다음 disk로 넘어가기 전에 2개의 block(8KB)를 할당하고 다음 disk로 넘어갔다. 이 단위를 chunk size라고 한다. 여기서는 8KB의 chunk size를 사용하였다. Chunk Sizechunk size는 performance에 영향을 많이 끼친다. 작은 chunk size를 사용한다면 많은 파일들이 여러 disk에 striped 되어 저장될 것이다. 그러므로 파일 read write 시에 병렬성을 증가시킬 수 있을것이다.다만 block에 접근하기 위한 positioning time(seek time + rotational delay)가 증가한다. 왜냐하면 여러 disk에 병렬로 읽거나 쓰게될때 결국 완료시간은 가장 오랜시간이 걸린 positioning time에 의해 결정되기 때문이다. chunk size를 크게잡으면 어떨까?작은 크기의 파일에 대해선은 read, write에 병렬성은 떨어질 것이다. 그러나 positioning time이 줄어들 것이다. 만약 작은 크기의 파일의 크기가 chunk size보다 작아 single disk에 저장된다면 그 single disk 에서의 positioning time이 결정한다. 그러므로 최적의 chunk size를 찾기 위해서는 어떤 요청들을 위주로 처리할지에 대한 지식이 먼저 있으면 결정하기 좋다.대부분은 큰 chunk size(64KB)를 사용하고는 한다. RAID 0 EvaluationRAID 0 에서 capacity, reliability, performance를 측정해보자.먼저 capacity는 간단하다. B개의 block들로 이루어진 N개의 disk가 있다면 N * B의 block을 저장할 수 있다.reliability도 간단한데 striping 방식에서는 reliability가 좋지 않다. 하나의 disk가 fail이 나더라도 바로 data loss로 이어진다. performance는 위에서 본 sequential workload의 S와 random workload의 R을 구해보며 비교해보자.sequential transfer size는 평균적으로 10MB, random transfer size는 평균적으로 10KB라고 가정하자. 이들을 전송하는데 속도가 얼마인지 계산해보자.Disk의 spec은 다음과 같다. Average seek time: 7ms Average rotational delay: 3ms Transfer rate of disk: 50MB/s S는 (Amound of Data) / (Time to access) 이므로 10MB / (7ms + 3ms + 200ms) = 47.62 MB/s 와 같다. Time to access는 seek time + rotational delay와 10MB를 전송하는데 200ms가 걸리므로 이를 합치면 계산할 수 있다.R은 10KB / 10.195ms = 0.981 MB/s이다. 10KB를 전송하는데 0.195ms가 걸린다.이 S와 R은 disk 1개에서 고려한 속도이다. 이처럼 striping의 performace를 보게되면 single-block request에 대해서는 single disk 성능과 동일하다.하지만 sequential, random workload 관점에서 볼때 sequential workload는 하나의 disk가 S의 속도를 낼때 N개의 disk가 있다면 전체 처리량은 S * N이 되겠다.randon workload 에서의 전체 처리량은 R * N이 된다. RAID Level 1: MirroringRAID Level 1은 mirroring으로 잘 알려져있다. Mirrored System에서는 각 block에 대해 copy본을 같이 저장함으로서 disk failure를 극복할 수 있다. 전형적인 mirroring 방식은 다음과 같다. 위의 방식에서는 RAID는 물리적으로 두개의 물리적 copy를 저장한다. disk 0은 disk 1과 동일한 내용을 들고있고 disk 2는 disk 3과 동일한 내용을 들고있다. 데이터들은 이 mirror pair들에 걸쳐 striped 된다.disk들에 copy본들을 어디에 위치시킬지는 여러가지 방법이 있는데 위에서 본 방식은 가장 일반적인 방식으로 이런 방식을 RAID-10이라고 부르기도 한다. stripe of mirror로 RAID1+0 라는 의미이다. disk read를 할때는 복제본 disk 두개중 어디에서 읽어도 상관없다. 다만 write은 두개의 disk에 모두 써주어야 한다. 이 2번의 write은 병렬로 처리할 수 있다.또 위의 경우 복제본을 2개 저장했는데 이를 mirroring level이라고도 한다. 여기서는 mirroring level이 2이다. RAID 1 EvaluationRAID 1 에서 capacity를 먼저보자. capacity 관점에서 RAID 1은 비용이 비싸다. mirroring level 2 에서는 전체 용량의 절반만 저장할 수 있으므로 capacity는 N * B / 2가 되겠다. reliability의 관점에서는 좋다. 아무 disk 1개의 failure를 허용할 수 있다. 사실 정확히는 최대 N/2개의 disk failure 까지 허용가능하다. 위에서 본 예제에서도 만약 운이 좋게도 disk 0과 disk 2가 동시에 fail 했다고 하더라도 data loss가 발생하지 않는다. 운이 좋게 각 copy 본을 저장하는 disk가 중복없이 fail이 발생했기 때문이다. performance 관점을 살펴보자. single read request는 single disk와 성능이 동일하다.다만 single write request는 살짝 더 latency가 늘어날 수 있는데, 각 copy 본을 병렬로 write한다고 하더라도 완료되는 시점은 두개의 disk중 더 오래걸린 시간이기 때문이다. steady-state throughput을 살펴보자. sequential write에서는 각 logical write은 두개의 physical write으로 나누어진다. 그러므로 mirroring 방식에서 전체 bandwidth는 (N / 2) * S가 된다. 최대 bandwidth의 절반밖에 안된다.sequential read도 똑같은데 얼핏 생각하면 각 logical read를 모든 disk에 나누어 처리하면 상황이 더 나아질 것 같지만 disk의 물리적 특성을 생각하면 그렇지 않다. 예를들어 위의 그림에서 block 0 부터 7까지 읽는다고 했을때 block 0은 disk 0에서, block 1은 disk 2에서, block 2는 disk 1에서 block 3은 disk 3에서 읽는다고 해보자. disk 0에서는 block 0을 읽고 그다음 block 4를 읽으면 될 것 같지만 어차피 중간에 block 2가 존재하기 때문에 rotation 하면서 이를 지나가야한다. 그러므로 성능향상이 없다. 그러므로 sequential read 에서의 전체 bandwidth도 N / 2) * S와 같다. Random read는 mirroring 방식에서 best case이다. read를 전체 disk에 분산시킬 수 있으므로 N * R MB/s의 대역폭을 가진다.Random write은 sequential write과 동일하게 두개의 physical write로 나누어지므로 (N / 2) * R MB/s 이다. RAID Level 5: Rotating ParityRAID Level 5에서는 parity 정보를 활용한다.Parity bit은 오류가 생겼는지 검사하는 bit 인데 2가지 종류가 존재한다. 짝수 parity와 홀수 parity이다.개념은 간단한데 짝수 parity에서는 데이터의 각 bit의 값에서 parity bit를 포함한 1의 개수가 짝수가 되도록 하는 것이다. 홀수 parity는 홀수가 되도록 하는것이다. 예를들어 다음과 같다. 짝수 parity라면 C0, C1, C2, C3의 bit에서 1의 개수가 2개이므로 parity bit을 0으로 둔다. 그래야 짝수인 2개로 유지되기 때문이다. parity bit을 활용하면 C0, C1, C2, C3 중 한개가 유실되어도 그 값을 bit 계산으로 알아낼 수 있다. 다시 RAID로 돌아와서 RAID Level 5 방식을 그림으로 보자. 여기서는 parity bit을 disk 별로 돌아가며 설정한다. 즉 위의 block 0, 1, 2, 3의 각 block의 bit를 계산해서 그에 대한 parity bit을 disk 4에 저장한다. RAID 5 Evalutationcapacity 부터 확인해보자. stripe 당 1개의 parity block을 두기때문에 (N - 1) * B의 용량을 가지게 된다.reliability 는 1개의 disk failure를 허용한다. parity bit를 활용해 recovery 가능하다. 다만 2개이상의 disk failure가 나면 복구할 방법이 없다.그렇다면 performance를 계산해보자.single read request는 1개의 disk로 매핑되기 때문에 single disk와 성능이 동일하다.다만 single write request는 다른데 위 예제에서 block 0에 write을 하려면 block 0을 읽고 parity block도 읽어서 parity bit를 다시 계산해야한다. 즉 read 2번, write 2번이 필요하고 read, write 각각 병렬로 처리할 수 있으므로 single disk의 latency의 약 2배정도 걸린다. sequential read는 parity block으로 인해 (N - 1) * S MB/s의 대역폭을 가진다. sequential write도 parity bit을 같이 써주어야 하므로 (N - 1) * S MB/s가 되겠다.random read는 모든 disk를 활용할 수 있다. 다만 random write은 (N / 4) * R MB/s을 가진다.왜냐하면 만약 random write가 위 예제에서 block 1과 block 10에 대해 요청이 왔다면 parity bit 계산을 위해 disk 1과 disk 4를 읽어 block 1을 처리하고, disk 0과 disk 2를 읽어 block 10을 처리한다. 각 요청안에서 block data write과 parity block write은 병렬로 처리할 수 있므으로 총 4개의 I/O 가 필요하다.(disk 1의 read/write + disk 0의 read/write) RAID Level 별로의 evaluation 결과는 다음과 같다. Summaryreliability는 고려하지 않고 performance가 중요한 상황이라면 RAID 0 striping이 좋은 선택이 될 수 있다.반대로 reliability가 중요하고 random I/O 성능이 중요하다면 RAID 1 mirroring이 좋은 선택이다. 다만 비용이 비싸다.capacity와 reliability가 중요하다면 RAID 5 가 좋은 선택이다. 다만 small-write의 성능이 좋지않은면이 있다.만약 sequential I/O가 주된 접근이고 capacity를 최대화 해야하는 상황이라면 이 경우에도 RAID 5가 좋은 선택이다. 여기서 살펴본 RAID 디자인 말고도 다른 여러가지 디자인이 존재한다. 예를들어 RAID 6은 multiple disk failure를 허용한다.RAID는 hardware 자체로 구현되어 제공되기도 하고 software로도 구현되어 제공될 수 있다. References https://www.amazon.com/Operating-Systems-Three-Easy-Pieces-ebook/dp/B00TPZ17O4","link":"/2020/09/05/raid/"},{"title":"시스템 버스(System Bus)란?","text":"System Bus는 간단하게 digital data를 이동시키기 위한 통로이다. Bus에는 3가지 종류가 있다. Control Bus, Address Bus, Data Bus 이다. 이 3가지가 System Bus를 구성한다. 시스템버스는 internal bus로서 프로세서와 내부 internal 하드웨어 장치들과 연결하도록 고안된 버스이다. 시스템버스는 메인보드에 존재한다. Bus의 종류를 하나씩 보자. Control BusControl Bus는 CPU가 다른 internal device들과 통신하는데 사용된다. Control Bus는 이름부터 알 수 있듯이 CPU의 command를 전달하고 device의 status signal을 반환한다. Control Bus 안에는 line 이라는 것이 있는데, control bus마다 line의 개수와 종류가 각각 다르지만 공통으로 가지고 있는 line이 있다. READ line: device가 CPU에게 읽히면 active된다. WRITE line: device가 CPU에게 쓰여지고 있으면 active된다. 메모리에 읽고 쓸때에는 Control Bus의 Read, Write line이 활성화된다.Control Bus는 양방향으로 작동한다. Address BusAddress Bus는 memory에 읽고 쓸때 memory의 physical address를 전달하는데 사용한다. Memory에 read, write를 할때 address bus에는 memory location이 담긴다. read, write 할 메모리 값 그 자체는 밑에서 볼 data bus에 실린다.Address Bus는 Memory Address Register와 연결되어 있으며, 단방향이다.Address bus는 중요한게 bus width(폭)가 시스템이 다룰 수 있는 address를 결정한다. 예를들어, 32bit address bus라면 시스템은 최대 232가지의 주소공간을 다룰 수 있겠다. Byte-addressable이면 가능한 memory space는 4GB가 되겠다.다만 모든 경우에 address bus의 width가 꼭 시스템이 사용하는 address와 동일하게 매칭되는 것은 아니다.CPU 칩에서 pin수는 엄청난 비용이다. 따라서 예전 시스템들에서는 16 bit address bus를 사용하지만 32 bit address space를 사용할 수 있다. 다만 이 경우 16bit로 주소를 나누어 두번에 걸쳐 전송해야한다. Data BusData Bus는 데이터를 전송하는데 사용하는 버스이다. Data Bus는 당연히 읽고 쓸수 있어야 하므로 양방향이다. 32bit, 64bit CPU32bit 운영체제, 64bit 운영체제를 결정하는 것은 무엇일까? 이는 data bus가 결정하지 않는다. 이를 결정하는것은 프로세서의 정수 register 크기이다. Data bus의 폭은 정수 register와 다를수도 있다. 예전 컴퓨터 머신들의 초기설계에는 data bus의 폭과 정수 register의 크기는 같았으나 꼭 그럴 필요는 없다. 실제로 8080 IBM PC는 16bit CPU이나 data bus는 8bit width이다. 그러므로 프로세서의 register에서 RAM으로 전송하려면 8bit씩 두번을 전송해야 했다. 그리고 프로세서의 정수 register와 address bus의 폭도 다를 수 있다. address bus의 폭은 이보다 더 클수도 작을 수도 있다. 실제로 original AMD Operaton은 64bit 시스템이지만 address bus는 memory subsystem을 단순화하기위해 40bit로 설계되었다. 64bit가 주소로 전부 활용되기는 현실적으로 힘들기 때문이다.지금 현재의 대부분의 64bit AMD CPU도 48bit의 address bus를 가지고있다. Modern designs현재의 디자인들은 우리가 잘 알고있는 bit 기반의 paradigm과는 맞지않는 더 복잡한 bus들을 사용한다. Modern CPU들은 매우 복잡하게 설계되어 있으며 이들은 memory 및 다른 CPU들과의 통신을 위해 특수한 bus들을 사용하기도 한다. 더이상의 단일로 존재하는 address bus나 data bus는 없으며 우리가 알고있는 bit-width 기반으로 처리하는 방식이 아닌 다른 signaling 방식으로 작동할 수도 있다. References https://en.wikipedia.org/wiki/Control_bus https://superuser.com/questions/446395/is-it-the-address-bus-size-or-the-data-bus-size-that-determines-8-bit-16-bit","link":"/2019/08/09/system-bus/"}],"tags":[{"name":"java","slug":"java","link":"/tags/java/"},{"name":"thread","slug":"thread","link":"/tags/thread/"},{"name":"컴퓨터구조","slug":"컴퓨터구조","link":"/tags/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0/"},{"name":"OS","slug":"OS","link":"/tags/OS/"},{"name":"covid19","slug":"covid19","link":"/tags/covid19/"},{"name":"coronavirus","slug":"coronavirus","link":"/tags/coronavirus/"},{"name":"nodejs","slug":"nodejs","link":"/tags/nodejs/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"maven","slug":"maven","link":"/tags/maven/"},{"name":"event-loop","slug":"event-loop","link":"/tags/event-loop/"},{"name":"smtp","slug":"smtp","link":"/tags/smtp/"},{"name":"파일시스템","slug":"파일시스템","link":"/tags/%ED%8C%8C%EC%9D%BC%EC%8B%9C%EC%8A%A4%ED%85%9C/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"},{"name":"컴퓨터구조","slug":"컴퓨터구조","link":"/categories/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0/"},{"name":"OS","slug":"OS","link":"/categories/OS/"},{"name":"personal","slug":"personal","link":"/categories/personal/"},{"name":"nodejs","slug":"nodejs","link":"/categories/nodejs/"},{"name":"tool","slug":"tool","link":"/categories/tool/"},{"name":"maven","slug":"maven","link":"/categories/maven/"},{"name":"network","slug":"network","link":"/categories/network/"}],"pages":[]}